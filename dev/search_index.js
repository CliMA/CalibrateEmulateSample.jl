var documenterSearchIndex = {"docs":
[{"location":"examples/sinusoid_example/#Sinusoid-Example","page":"Simple example walkthrough","title":"Sinusoid Example","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository","category":"page"},{"location":"examples/sinusoid_example/#Background","page":"Simple example walkthrough","title":"Background","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"This example demonstrates how to use CalibrateEmulateSample.jl for a simple model that generates noisy observables of a signal. The sinusoid signal is defined by two parameters: its shift along the vertical axis and its amplitude. We make noisy observations of the signal and we can calculate the mean of the signal, which is informative about its shift along the axis, and the range of the signal, which is informative  about the amplitude. Although our sinusoid function is simple and quick to evaluate, we shall pretend it is non-differentiable and expensive to evaluate, as a case study for carrying out uncertainty quantification on  more complex systems. Additionally, we will work in a \"perfect model\" setting for this example, meaning we will generate pseudo-observations for our model and pretend that these are noisy observations of our system.","category":"page"},{"location":"examples/sinusoid_example/#Model","page":"Simple example walkthrough","title":"Model","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We have a model of a sinusoidal signal that is a function of parameters theta=(Av), where A is the amplitude of the signal and v is vertical shift of the signal:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"f(A v) = A sin(phi + t) + v forall t in 02pi","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Here, phi is the random phase of each signal.  The goal is to estimate not just the point estimates of the parameters theta=(Av), but entire probability distributions of them, given some noisy observations. We will use the range and mean of a signal as our observable: ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"G(theta) = big textrangebig(f(theta)big) textmeanbig(f(theta)big) big ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"This highlights the role of choosing a good observable, in particular our choice of G is independent of the random phase shift phi and is in fact deterministic. This allows us to write out an expression for the noisy observation, y_obs:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"y_obs = G(theta) + gamma qquad gamma sim mathcalN(0 Gamma)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"where Gamma is the observational covariance matrix. We will assume the noise to be independent for each observable, giving us a diagonal covariance matrix.","category":"page"},{"location":"examples/sinusoid_example/#Walkthrough-of-code","page":"Simple example walkthrough","title":"Walkthrough of code","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"You can find the full scripts to reproduce this tutorial in examples/Sinusoid/. The code is split into four sections:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Model set up in sinusoid_setup.jl\nCalibrate in calibrate.jl\nEmulate in emulate.jl\nSample in sample.jl","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"You do not need to explicitly run sinusoid_setup.jl as it is called from calibrate.jl. However, this file contains the functions for the model and for generating pseudo-observations.  You will need to run steps 2-4 in order as each one relies on output saved from the previous steps.","category":"page"},{"location":"examples/sinusoid_example/#Set-up","page":"Simple example walkthrough","title":"Set up","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"First, we load the packages we need for setting up the model:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"using LinearAlgebra, Random\nusing Plots\nusing JLD2\nusing Statistics, Distributions\n","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We define a model that generates a sinusoid given parameters theta=(Av)  (amplitude and vertical shift). We will estimate these parameters from data. The model adds a random phase shift upon evaluation.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"# Define x-axis\ndt = 0.01\ntrange = 0:dt:(2 * pi + dt)\n\nfunction model(amplitude, vert_shift)\n    # Set phi, the random phase\n    phi = 2 * pi * rand()\n    return amplitude * sin.(trange .+ phi) .+ vert_shift\nend\n","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We will define a \"true\" amplitude and vertical shift to generate some pseudo-observations.  Let theta=(30 70).","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"amplitude_true = 3.0\nvert_shift_true = 7.0\n# Our input parameters are 2d and our outputs are 2d\ntheta_true = [amplitude_true, vert_shift_true]\ndim_params = 2\n# Generate the \"true\" signal for these parameters\nsignal_true = model(amplitude_true, vert_shift_true)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We will observe properties of the signal that inform us about the amplitude and vertical  position. These properties will be the range (the difference between the maximum and the minimum), which is informative about the amplitude of the sinusoid, and the mean, which is informative  about the vertical shift. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"y1_true = maximum(signal_true) - minimum(signal_true)\ny2_true = mean(signal_true)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"However, our observations are typically not noise-free, so we add some white noise to our  observables. We call this y_obs. The user can choose the observational covariance matrix, Gamma. We will assume the noise is independent (a diagonal covariance matrix Gamma=02 * I). ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"dim_output = 2\nΓ = 0.2 * I\nwhite_noise = MvNormal(zeros(dim_output), Γ)\ny_obs = [y1_true, y2_true] .+ rand(white_noise)\nprintln(\"Observations:\", y_obs)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"This gives y_obs=(615 642). We can plot the true signal in black, the true observables in red and the noisy observables in blue. (Image: signal)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"It will be helpful for us to define a function G(theta), which returns these observables  (the range and the mean) of the sinusoid given a parameter vector. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"function G(theta)\n    amplitude, vert_shift = theta\n    sincurve = model(amplitude, vert_shift)\n    return [maximum(sincurve) - minimum(sincurve), mean(sincurve)]\nend","category":"page"},{"location":"examples/sinusoid_example/#Calibrate","page":"Simple example walkthrough","title":"Calibrate","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We are interested in learning the posterior distribution of theta for the inverse problem y_obs=G(theta)+mathcalN(0Gamma). We first carry out calibration, which aims to solve the inverse problem for point estimates of the optimal values for theta. Specifically, we use an ensemble based calibration method,  such as Ensemble Kalman Inversion, because it provides us with ensembles of G(theta) evaluations that  are focused near to the optimal values for theta. These ensembles provide us with a suitable dataset  for training an emulator to be used in sampling the posterior distribution. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We are using the EnsembleKalmanProcesses.jl package for Ensemble Kalman Inversion (EKI). We start with user-defined prior distributions and sample an ensemble of parameters theta, which we use to evaluate G(theta). Then, we iteratively update the ensemble until our parameters theta are near to the optimal.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"First, we will load the packages we need from CES:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"# CES\nusing CalibrateEmulateSample\nconst EKP = CalibrateEmulateSample.EnsembleKalmanProcesses\nconst PD = EKP.ParameterDistributions","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We define prior distributions on the two parameters. For the amplitude, we define a prior with mean 2 and standard deviation 1. It is additionally constrained to be nonnegative. For the vertical shift we define a Gaussian prior with mean 0 and standard deviation 5.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"prior_u1 = PD.constrained_gaussian(\"amplitude\", 2, 1, 0, Inf)\nprior_u2 = PD.constrained_gaussian(\"vert_shift\", 0, 5, -Inf, Inf)\nprior = PD.combine_distributions([prior_u1, prior_u2])\n# Plot priors\np = plot(prior, fill = :lightgray)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: prior)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We now generate the initial ensemble and set up the EKI.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"N_ensemble = 10\nN_iterations = 5\n\ninitial_ensemble = EKP.construct_initial_ensemble(prior, N_ensemble)\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, y_obs, Γ, EKP.Inversion())","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We are now ready to carry out the inversion. At each iteration, we get the ensemble from the last iteration, apply G(theta) to each ensemble member, and apply the Kalman update to the ensemble.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"for i in 1:N_iterations\n    params_i = EKP.get_ϕ_final(prior, ensemble_kalman_process)\n\n    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, G_ens)\nend","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Finally, we get the ensemble after the last iteration. This provides our estimate of the parameters.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"final_ensemble = EKP.get_ϕ_final(prior, ensemble_kalman_process)\n\n# Check that the ensemble mean is close to the theta_true\nprintln(\"Ensemble mean: \", mean(final_ensemble, dims=2))   # [3.05, 6.37]\nprintln(\"True parameters: \", theta_true)   # [3.0, 7.0]","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Parameter Truth EKI mean\nAmplitude 3.0 3.05\nVertical shift 7.0 6.37","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"The EKI ensemble mean at the final iteration is close to the true parameters, which is good. We can also see how the ensembles evolve at each iteration in the plot below.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: eki)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"The ensembles are initially spread out but move closer to the true parameter values with each iteration, indicating the EKI algorithm is converging towards the minimum. Taking the mean of the ensemble gives a point estimate of the optimal parameters. However, EKI does not give us an estimate of the uncertainty, as the ensemble collapses. To carry out uncertainty quantification, we can sample from the posterior distribution, which requires a \"cheap\" method to evaluate our model, i.e., an emulator.  In the next step of CES, we will build an emulator using the dataset generated in EKI.","category":"page"},{"location":"examples/sinusoid_example/#Emulate","page":"Simple example walkthrough","title":"Emulate","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"In the previous calibrate step, we learned point estimates for the optimal parameters theta, but  for uncertainty quantification, we want to learn posterior distributions on our parameters. We can sample from posterior distributions with Markov chain Monte Carlo (MCMC) methods, but these  typically require many model evaluations. In many scientific problems, model evaluations are highly  costly, making this infeasible. To get around this, we build an emulator of our model,  which allows us to approximate the expensive model almost instantaneously. An emulator can also be  helpful for noisy problems as they provide a smoother approximation, leading to better MCMC  convergence properties.  In this section, we show how the codebase can be used to build emulators of our sinusoid model.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We ran Ensemble Kalman Inversion with an ensemble size of 10 for 5  iterations. This generated a total of 50 input output pairs from our model. We will use these samples to train an emulator. The EKI samples make a suitable  dataset for training an emulator because in the first iteration, the ensemble parameters  are spread out according to the prior, meaning they cover the full support of the parameter space. This is important for building an emulator that can be evaluated anywhere  in this space. In later iterations, the ensemble parameters are focused around the truth.  This means the emulator that will be more accurate around this region.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"First, we load additional packages we need for this section:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"using CalibrateEmulateSample.Emulators\nconst CES = CalibrateEmulateSample","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We will build two types of emulator here for comparison: Gaussian processes and Random  Features. First, set up the data in the correct format. CalibrateEmulateSample.jl uses a paired data container that matches the inputs (in the unconstrained space) to the outputs:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"input_output_pairs = CES.Utilities.get_training_points(ensemble_kalman_process, N_iterations)\nunconstrained_inputs = CES.Utilities.get_inputs(input_output_pairs)\ninputs = Emulators.transform_unconstrained_to_constrained(prior, unconstrained_inputs)\noutputs = CES.Utilities.get_outputs(input_output_pairs)","category":"page"},{"location":"examples/sinusoid_example/#Gaussian-process","page":"Simple example walkthrough","title":"Gaussian process","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We will set up a basic Gaussian process (GP) emulator using the ScikitLearn.jl package or GaussianProcesses.jl.  See the Gaussian process page for more information and options, including choice of package and kernels.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"gppackage = Emulators.GPJL()\ngauss_proc = Emulators.GaussianProcess(gppackage, noise_learn = false)\n\n# Build emulator with data\nemulator_gp = Emulator(gauss_proc, input_output_pairs, normalize_inputs = true,  obs_noise_cov = Γ)\noptimize_hyperparameters!(emulator_gp)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"For this simple example, we already know the observational noise Γ=0.2*I, so we set noise_learn = false.  However, for more complicated problems we may want to learn the noise as an additional hyperparameter.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We will check performance of the GP by testing on unseen data in a moment, but first, we will build a random features emulator for comparison.","category":"page"},{"location":"examples/sinusoid_example/#Random-Features","page":"Simple example walkthrough","title":"Random Features","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"An alternative emulator can be created with random features (RF). Random features can approximate a Gaussian process with improved scaling properties, making them more suitable for higher dimensional problems. We use a Vector Random Features emulator here, chosen because we find it is a reasonable approximation to the Gaussian process emulator above. For new problems, you may need to play around with these parameter choices. More information can be found here.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"# We have two input dimensions and two output dimensions.\ninput_dim = 2\noutput_dim = 2\n# Select number of features\nn_features = 60\nnugget = 1e-9\nkernel_structure = NonseparableKernel(LowRankFactor(2, nugget))\noptimizer_options = Dict(\n    \"n_ensemble\" => 50,\n    \"cov_sample_multiplier\" => 10,\n    \"scheduler\" => EKP.DataMisfitController(on_terminate = \"continue\"),\n    \"n_iteration\" => 50,\n    \"verbose\" => true,\n)\nrandom_features = VectorRandomFeatureInterface(\n    n_features,\n    input_dim,\n    output_dim,\n    kernel_structure = kernel_structure,\n    optimizer_options = optimizer_options,\n)\nemulator_random_features =\n    Emulator(random_features, input_output_pairs, normalize_inputs = true, obs_noise_cov = Γ, decorrelate = false)\noptimize_hyperparameters!(emulator_random_features)","category":"page"},{"location":"examples/sinusoid_example/#Emulator-Validation","page":"Simple example walkthrough","title":"Emulator Validation","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Now we will validate both GP and RF emulators and compare them against the ground truth, G(theta). Note this is only possible in our example because our true model, G(theta), is cheap to evaluate. In more complex systems, we would have limited data to validate emulator performance with. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Here, we will compare emulator performance across a wide range of parameters, but we will pay close attention to  performance near the final ensemble mean theta=(3 6). This is because we need high accuracy in this region in the next step, when we sample the posterior distribution.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"First, we check the ground truth model G(theta) over our parameter space. We can plot how the two outputs (range, mean), vary with the two input parameters (amplitude, vertical shift).","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: groundtruth)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"The first panel shows how the range varies with respect to the two parameters in the true forward map. The contours show the range is mostly dependent on the amplitude, with little variation with respect to the vertical shift. The second panel shows how the mean varies with the respect to the two parameters and is mostly dependent on the vertical shift. This result makes sense for our model setup.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Below, we recreate the same contour plot with the emulators. We will also overlay the training data points  from the EKI, where the colors show the output from G(theta) evaluated at the training points.  The emulator contours should agree with the training data.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"First, for the Gaussian process emulator:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: GP_emulator)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"This looks similar to the output from G(theta). Next, let's check the random features emulator:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: RF_emulator)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Both the GP and RF emulator give similar results to the ground truth G(theta), indicating they are correctly learning the relationships between the parameters and the outputs. We also see the contours agree with the  colors of the training data points. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We should also validate how accurate the emulators are by looking at the absolute difference between emulator predictions and the ground truth. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"The Gaussian process absolute errors are plotted here:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: GP_errors)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"and the random features absolute errors are here:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: RF_errors)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Both these error maps look similar. Importantly, we want the emulator to show the low errors in the region around the true parameter values near theta=(3 6) (i.e, near where the EKI points converge, shown by the scatter points in the previous plot). This the region that we will be sampling in the next step.  We see low errors near here for both outputs and for both emulators. Now we have validated these emulators,  we will proceed the last step of CES: Sampling of the posterior distribution. ","category":"page"},{"location":"examples/sinusoid_example/#Sample","page":"Simple example walkthrough","title":"Sample","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Now that we have a cheap emulator for our model, we can carry out uncertainty quantification to learn the posterier distribution of the parameters, theta. We use Markov chain Monte Carlo (MCMC) to sample the posterior distribtion. In MCMC, we start with a sample from a prior distribution and propose a new sample from a proposal distribution, which is accepted with a probability relating the the ratio of the posterior distribution to the proposal distributions. If accepted, this proposed sample is  added to the chain, or otherwise, the original sample is added to the chain. This is repeated over many iterations and eventually creates a sequence of samples from the posterior distribution. The CES code uses AbstractMCMC.jl, full details can be found here. For this example, we will use a random walk Metropolis-Hastings sampler (RWMHSampling), which assumes that  the proposal distribution is a random walk, with a step-size delta. Usually, we have little knowledge of  what this step size should be, but we can optimize this as shown below.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"First, we will load the additional packages we need:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"using CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We will provide the API with the observations, priors and our cheap emulator from the previous section. In this  example we use the GP emulator. First, we need to find a suitable starting point, ideally one that is near the posterior distribution. We will use the final ensemble mean from EKI as this will increase the chance of acceptance near the start of the chain, and reduce burn-in time.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"init_sample = EKP.get_u_mean_final(ensemble_kalman_process)\nprintln(\"initial parameters: \", init_sample)    # (1.11, 6.37)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Now, we can set up and carry out the MCMC starting from this point. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"mcmc = MCMCWrapper(RWMHSampling(), y_obs, prior, emulator_gp; init_params = init_sample)\n# First let's run a short chain to determine a good step size\nnew_step = optimize_stepsize(mcmc; init_stepsize = 0.1, N = 2000, discard_initial = 0)\n\n# Now begin the actual MCMC\nprintln(\"Begin MCMC - with step size \", new_step)\nchain = MarkovChainMonteCarlo.sample(mcmc, 100_000; stepsize = new_step, discard_initial = 2_000)\n\n# We can print summary statistics of the MCMC chain\ndisplay(chain)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"parameters mean std\namplitude 1.1068 0.0943\nvert_shift 6.3897 0.4601","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"Note that these values are provided in the unconstrained space. The vertical shift seems reasonable, but the amplitude is not. This is because the amplitude is constrained to be positive, but the MCMC is run in the unconstrained space.  We can transform to the real  constrained space and re-calculate these values.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"# Extract posterior samples\nposterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)\n# Back to constrained coordinates\nconstrained_posterior = Emulators.transform_unconstrained_to_constrained(\n    prior, MarkovChainMonteCarlo.get_distribution(posterior)\n)\nprintln(\"Amplitude mean: \", mean(constrained_posterior[\"amplitude\"]), \", std: \", std(constrained_posterior[\"amplitude\"]))\nprintln(\"Vertical shift mean: \", mean(constrained_posterior[\"vert_shift\"]), \", std: \", std(constrained_posterior[\"vert_shift\"]))","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"This gives:","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"parameters mean std\namplitude 3.0382 0.2880\nvert_shift 6.3774 0.4586","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"This is in agreement with the true theta=(30 70) and with the observational covariance matrix we provided Gamma=02 * I (i.e., a standard deviation of approx. 045). CalibrateEmulateSample.jl has built-in plotting recipes to help us visualize the prior and posterior distributions.  Note that these are the marginal distributions.","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"# We can quickly plot priors and posterior using built-in capabilities\np = plot(prior, fill = :lightgray)\nplot!(posterior, fill = :darkblue, alpha = 0.5)\n","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: GP_posterior)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"The MCMC has learned the posterior distribution which is much narrower than the prior.  For multidimensional problems, the posterior is typically multidimensional, and marginal  distribution plots do not show how parameters co-vary. We plot a 2D histogram of theta_1 vs. theta_2 below, with the marginal distributions on each axis. ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: GP_2d_posterior)","category":"page"},{"location":"examples/sinusoid_example/#Sample-with-Random-Features","page":"Simple example walkthrough","title":"Sample with Random Features","text":"","category":"section"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"We can repeat the sampling method using the random features emulator instead of the Gaussian process and we find similar results: ","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"parameters mean std\namplitude 3.3210 0.7216\nvert_shift 6.3986 0.5098","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"(Image: RF_2d_posterior)","category":"page"},{"location":"examples/sinusoid_example/","page":"Simple example walkthrough","title":"Simple example walkthrough","text":"It is reassuring to see that our uncertainty quantification methods are robust to the different emulator choices here. This is because our particular GP and RF emulators showed similar accuracy during validation.  However, this result is highly sensitive to the choices of GP kernel and RF kernel structure. If you find very  different posterior distributions for different emulators, it is likely that the kernel choices need be refined. The kernel choices must be flexible enough to accurately capture the relationships between the inputs and outputs.  We recommend trying a variety of different emulator configurations and carefully considering emulator validation  on samples that the emulator has not been trained on. ","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to CalibrateEmulateSample! We encourage opening issues and pull requests (PRs).","category":"page"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The easiest way to contribute is by using CalibrateEmulateSample, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of CalibrateEmulateSample insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition. Examples could include implementing new statistical emulators, or implementing new data compression tools (beyond normalization, standardization and truncated SVD)","category":"page"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you are unfamiliar with git and version control, the following guides will be helpful:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Atlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"page"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Create your own fork of CalibrateEmulateSample on GitHub and check out your copy:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git clone https://github.com/<your-username>/CalibrateEmulateSample.jl.git\n$ cd CalibrateEmulateSample.jl","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Now you have access to your fork of CalibrateEmulateSample through origin. Create a branch for your feature; this will hold your contribution:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout -b <branchname>","category":"page"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"page"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"julia --project=.dev .dev/climaformat.jl .","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"from the CalibrateEmulateSample.jl directory.","category":"page"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with CalibrateEmulateSample.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To make sure you are up to date with main, you can use the following workflow:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To squash your commits, you can use the following command:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~n","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"where n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~4","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"will open the following file in (typically) vim:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Then in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git push -u origin <name_of_local_branch> --force","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can find more information about squashing here.","category":"page"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Currently a number of checks are run per commit for a given PR.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"JuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Unit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"page"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We ensure that all unit tests across several environments, Documentation builds, and integration tests (managed by Buildkite), pass before merging any PR into main. The integration tests currently run some of our example cases in examples/.","category":"page"},{"location":"installation_instructions/#Installation-Instructions","page":"Installation instructions","title":"Installation Instructions","text":"","category":"section"},{"location":"installation_instructions/#Installing-CalibrateEmulateSample.jl","page":"Installation instructions","title":"Installing CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Currently CalibrateEmulateSample (CES) depends on some external python dependencies ","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: Latest python package versions!\nWe have verified that the configurations work: For Python 3.11 - 3.12: scipy = 1.14.1, scikit-learn = 1.5.1. For Python 3.8 - 3.11: scipy = 1.8.1, scikit-learn = 1.1.1. Please create an issue if you have had success with more up-to-date versions, and we can update this page!","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you have dependencies installed already, then the code can be used by simply entering","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project\n> ]\n> add CalibrateEmulateSample","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"One may instead clone the project into a new local repository (using SSH or https link from github), to easily access the CES codebase (e.g. to run our example suite) .","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you do not have the dependencies installed, we have found it is easiest to install them via Julia's \"Conda.jl\",","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project\n> ]\n> add Conda\n> add CalibrateEmulateSample","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Then install the dependencies by having the project use its own Conda environment variable (set by exporting the ENV variable PYTHON=\"\").","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"This call should build Conda and Pycall. The scikit-learn package (along with scipy) then has to be installed if using a Julia project-specific Conda environment:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scipy=1.14.1\", channel=\"conda-forge\")'\n> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scikit-learn=1.5.1\")'\n","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: Pycall can't find the packages!?\nSometimes Conda.jl builds the python packages, in julia-based python repo but Pycall resorts to a different python path. this throws an error like:    ERROR: InitError: PyError (PyImport_ImportModule\n\nThe Python package sklearn.gaussian_process.kernels could not be imported by pyimport.In this case, simply call julia --project followed byjulia> ENV[\"PYTHON\"]=\"\"\njulia> Pkg.build(\"PyCall\")\njulia> exit()to reset unify the paths.","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"See the PyCall.jl documentation  for more information about how to configure the local Julia / Conda / Python environment. ","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To test that the package is working:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You need to first build the top-level project before building the documentation:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"cd CalibrateEmulateSample.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Then you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project=docs/ -e 'using Pkg; Pkg.instantiate()'\njulia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html. Occasional figures may only be viewable in the online documentation due to the fancy-url package.","category":"page"},{"location":"API/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"Modules = [CalibrateEmulateSample.Utilities]\nOrder   = [:module, :type, :function]","category":"page"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_training_points-Union{Tuple{P}, Tuple{IT}, Tuple{FT}, Tuple{EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, Union{AbstractVector{IT}, IT}}} where {FT, IT, P}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_training_points","text":"get_training_points(\n    ekp::EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P},\n    train_iterations::Union{AbstractVector{IT}, IT} where IT\n) -> EnsembleKalmanProcesses.DataContainers.PairedDataContainer\n\n\nExtract the training points needed to train the Gaussian process regression.\n\nekp - EnsembleKalmanProcess holding the parameters and the data that were produced during the Ensemble Kalman (EK) process.\ntrain_iterations - Number (or indices) EK layers/iterations to train on.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#RandomFeatures","page":"Random Features","title":"RandomFeatures","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/RandomFeatures/#Kernel-and-Covariance-structure","page":"Random Features","title":"Kernel and Covariance structure","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"OneDimFactor\nDiagonalFactor\nCholeskyFactor\nLowRankFactor\nHierarchicalLowRankFactor\nSeparableKernel\nNonseparableKernel\ncalculate_n_hyperparameters\nhyperparameters_from_flat\nbuild_default_prior","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.OneDimFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.OneDimFactor","text":"struct OneDimFactor <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\ncovariance structure for a one-dimensional space\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.DiagonalFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.DiagonalFactor","text":"struct DiagonalFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a diagonal covariance structure\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.CholeskyFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.CholeskyFactor","text":"struct CholeskyFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a general positive-definite covariance structure\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.LowRankFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.LowRankFactor","text":"struct LowRankFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a covariance structure that deviates from the identity with a low-rank perturbation. This perturbation is diagonalized in the low-rank space\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.HierarchicalLowRankFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.HierarchicalLowRankFactor","text":"struct HierarchicalLowRankFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a covariance structure that deviates from the identity with a more general low-rank perturbation\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.SeparableKernel","page":"Random Features","title":"CalibrateEmulateSample.Emulators.SeparableKernel","text":"struct SeparableKernel{CST1<:CalibrateEmulateSample.Emulators.CovarianceStructureType, CST2<:CalibrateEmulateSample.Emulators.CovarianceStructureType} <: CalibrateEmulateSample.Emulators.KernelStructureType\n\nBuilds a separable kernel, i.e. one that accounts for input and output covariance structure separately\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.NonseparableKernel","page":"Random Features","title":"CalibrateEmulateSample.Emulators.NonseparableKernel","text":"struct NonseparableKernel{CST<:CalibrateEmulateSample.Emulators.CovarianceStructureType} <: CalibrateEmulateSample.Emulators.KernelStructureType\n\nBuilds a nonseparable kernel, i.e. one that accounts for a joint input and output covariance structure\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.calculate_n_hyperparameters","page":"Random Features","title":"CalibrateEmulateSample.Emulators.calculate_n_hyperparameters","text":"calculate_n_hyperparameters(\n    d::Int64,\n    odf::CalibrateEmulateSample.Emulators.OneDimFactor\n) -> Int64\n\n\ncalculates the number of hyperparameters generated by the choice of covariance structure\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.hyperparameters_from_flat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.hyperparameters_from_flat","text":"hyperparameters_from_flat(\n    x::AbstractVector,\n    odf::CalibrateEmulateSample.Emulators.OneDimFactor\n)\n\n\nreshapes a list of hyperparameters into a covariance matrix based on the selected structure\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_default_prior","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_default_prior","text":"build_default_prior(\n    name::AbstractString,\n    n_hp::Int64,\n    odf::CalibrateEmulateSample.Emulators.OneDimFactor\n)\n\n\nbuilds a prior distribution for the kernel hyperparameters to initialize optimization.\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#Scalar-interface","page":"Random Features","title":"Scalar interface","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"ScalarRandomFeatureInterface\nScalarRandomFeatureInterface(::Int,::Int)\nbuild_models!(::ScalarRandomFeatureInterface, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict(::ScalarRandomFeatureInterface, ::M) where {M <: AbstractMatrix}","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","page":"Random Features","title":"CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","text":"struct ScalarRandomFeatureInterface{S<:AbstractString, RNG<:Random.AbstractRNG, KST<:CalibrateEmulateSample.Emulators.KernelStructureType} <: CalibrateEmulateSample.Emulators.RandomFeatureInterface\n\nStructure holding the Scalar Random Feature models. \n\nFields\n\nrfms::Vector{RandomFeatures.Methods.RandomFeatureMethod}: vector of RandomFeatureMethods, contains the feature structure, batch-sizes and regularization\nfitted_features::Vector{RandomFeatures.Methods.Fit}: vector of Fits, containing the matrix decomposition and coefficients of RF when fitted to data\nbatch_sizes::Union{Nothing, Dict{S, Int64}} where S<:AbstractString: batch sizes\nn_features::Union{Nothing, Int64}: n_features\ninput_dim::Int64: input dimension\nrng::Random.AbstractRNG: choice of random number generator\nkernel_structure::CalibrateEmulateSample.Emulators.KernelStructureType: Kernel structure type (e.g. Separable or Nonseparable)\nfeature_decomposition::AbstractString: Random Feature decomposition, choose from \"svd\" or \"cholesky\" (default)\noptimizer_options::Dict{S} where S<:AbstractString: dictionary of options for hyperparameter optimizer\noptimizer::Vector: diagnostics from optimizer\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface-Tuple{Int64, Int64}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","text":"ScalarRandomFeatureInterface(\n    n_features::Int64,\n    input_dim::Int64;\n    kernel_structure,\n    batch_sizes,\n    rng,\n    feature_decomposition,\n    optimizer_options\n) -> CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface{String, Random.TaskLocalRNG, CalibrateEmulateSample.Emulators.SeparableKernel{CST1, CalibrateEmulateSample.Emulators.OneDimFactor}} where CST1<:CalibrateEmulateSample.Emulators.CovarianceStructureType\n\n\nConstructs a ScalarRandomFeatureInterface <: MachineLearningTool interface for the RandomFeatures.jl package for multi-input and single- (or decorrelated-)output emulators.\n\nn_features - the number of random features\ninput_dim - the dimension of the input space\nkernel_structure -  - a prescribed form of kernel structure\nbatch_sizes = nothing - Dictionary of batch sizes passed RandomFeatures.jl object (see definition there)\nrng = Random.GLOBAL_RNG - random number generator \nfeature_decomposition = \"cholesky\" - choice of how to store decompositions of random features, cholesky or svd available\noptimizer_options = nothing - Dict of options to pass into EKI optimization of hyperparameters (defaults created in ScalarRandomFeatureInterface constructor):\n\"prior\":  the prior for the hyperparameter optimization \n\"priorinscale\": use this to tune the input prior scale\n\"n_ensemble\":  number of ensemble members\n\"n_iteration\":  number of eki iterations\n\"covsamplemultiplier\": increase for more samples to estimate covariance matrix in optimization (default 10.0, minimum 0.0)  \n\"scheduler\": Learning rate Scheduler (a.k.a. EKP timestepper) Default: DataMisfitController\n\"tikhonov\":  tikhonov regularization parameter if >0\n\"inflation\":  additive inflation ∈ [0,1] with 0 being no inflation\n\"train_fraction\":  e.g. 0.8 (default)  means 80:20 train - test split\n\"nfeaturesopt\":  fix the number of features for optimization (default n_features, as used for prediction)\n\"multithread\": how to multithread. \"ensemble\" (default) threads across ensemble members \"tullio\" threads random feature matrix algebra\n\"accelerator\": use EKP accelerators (default is no acceleration)\n\"verbose\" => false, verbose optimizer statements\n\"cov_correction\" => \"shrinkage\", type of conditioning to improve estimated covariance (Ledoit Wolfe 03), also \"nice\" for (Vishny, Morzfeld et al. 2024)\n\"ncrossvalsets\" => 2, train fraction creates (default 5) train-test data subsets, then use 'ncrossvalsets' of these stacked in the loss function. If set to 0, train=test on the full data provided ignoring \"train_fraction\".\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat};\n    kwargs...\n) -> Union{Nothing, Vector}\n\n\nBuilds the random feature method from hyperparameters. We use cosine activation functions and a Multivariate Normal distribution (from Distributions.jl) with mean M=0, and input covariance U built with the CovarianceStructureType.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#GaussianProcesses.predict-Union{Tuple{M}, Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface, M}} where M<:(AbstractMatrix)","page":"Random Features","title":"GaussianProcesses.predict","text":"predict(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    new_inputs::AbstractMatrix;\n    multithread\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#Vector-Interface","page":"Random Features","title":"Vector Interface","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"VectorRandomFeatureInterface\nVectorRandomFeatureInterface(::Int, ::Int, ::Int)\nbuild_models!(::VectorRandomFeatureInterface, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict(::VectorRandomFeatureInterface, ::M) where {M <: AbstractMatrix}","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","page":"Random Features","title":"CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","text":"struct VectorRandomFeatureInterface{S<:AbstractString, RNG<:Random.AbstractRNG, KST<:CalibrateEmulateSample.Emulators.KernelStructureType} <: CalibrateEmulateSample.Emulators.RandomFeatureInterface\n\nStructure holding the Vector Random Feature models. \n\nFields\n\nrfms::Vector{RandomFeatures.Methods.RandomFeatureMethod}: A vector of RandomFeatureMethods, contains the feature structure, batch-sizes and regularization\nfitted_features::Vector{RandomFeatures.Methods.Fit}: vector of Fits, containing the matrix decomposition and coefficients of RF when fitted to data\nbatch_sizes::Union{Nothing, Dict{S, Int64}} where S<:AbstractString: batch sizes\nn_features::Union{Nothing, Int64}: number of features\ninput_dim::Int64: input dimension\noutput_dim::Int64: output_dimension\nrng::Random.AbstractRNG: rng\nregularization::Vector{Union{LinearAlgebra.Diagonal, LinearAlgebra.UniformScaling, Matrix}}: regularization\nkernel_structure::CalibrateEmulateSample.Emulators.KernelStructureType: Kernel structure type (e.g. Separable or Nonseparable)\nfeature_decomposition::AbstractString: Random Feature decomposition, choose from \"svd\" or \"cholesky\" (default)\noptimizer_options::Dict: dictionary of options for hyperparameter optimizer\noptimizer::Vector: diagnostics from optimizer\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface-Tuple{Int64, Int64, Int64}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","text":"VectorRandomFeatureInterface(\n    n_features::Int64,\n    input_dim::Int64,\n    output_dim::Int64;\n    kernel_structure,\n    batch_sizes,\n    rng,\n    feature_decomposition,\n    optimizer_options\n) -> CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface{String, Random.TaskLocalRNG, CalibrateEmulateSample.Emulators.SeparableKernel{CST1, CST2}} where {CST1<:Union{CalibrateEmulateSample.Emulators.OneDimFactor, CalibrateEmulateSample.Emulators.CholeskyFactor{Float64}, CalibrateEmulateSample.Emulators.DiagonalFactor{Float64}, CalibrateEmulateSample.Emulators.HierarchicalLowRankFactor{Float64}, CalibrateEmulateSample.Emulators.LowRankFactor{Float64}}, CST2<:Union{CalibrateEmulateSample.Emulators.OneDimFactor, CalibrateEmulateSample.Emulators.CholeskyFactor{Float64}, CalibrateEmulateSample.Emulators.DiagonalFactor{Float64}, CalibrateEmulateSample.Emulators.HierarchicalLowRankFactor{Float64}, CalibrateEmulateSample.Emulators.LowRankFactor{Float64}}}\n\n\nConstructs a VectorRandomFeatureInterface <: MachineLearningTool interface for the RandomFeatures.jl package for multi-input and multi-output emulators.\n\nn_features - the number of random features\ninput_dim - the dimension of the input space\noutput_dim - the dimension of the output space\nkernel_structure -  - a prescribed form of kernel structure\nbatch_sizes = nothing - Dictionary of batch sizes passed RandomFeatures.jl object (see definition there)\nrng = Random.GLOBAL_RNG - random number generator \nfeature_decomposition = \"cholesky\" - choice of how to store decompositions of random features, cholesky or svd available\noptimizer_options = nothing - Dict of options to pass into EKI optimization of hyperparameters (defaults created in VectorRandomFeatureInterface constructor):\n\"prior\": the prior for the hyperparameter optimization\n\"priorinscale\"/\"prioroutscale\": use these to tune the input/output prior scale.\n\"n_ensemble\": number of ensemble members\n\"n_iteration\": number of eki iterations\n\"scheduler\": Learning rate Scheduler (a.k.a. EKP timestepper) Default: DataMisfitController\n\"covsamplemultiplier\": increase for more samples to estimate covariance matrix in optimization (default 10.0, minimum 0.0) \n\"tikhonov\": tikhonov regularization parameter if > 0\n\"inflation\": additive inflation ∈ [0,1] with 0 being no inflation\n\"train_fraction\": e.g. 0.8 (default)  means 80:20 train - test split\n\"nfeaturesopt\":  fix the number of features for optimization (default n_features, as used for prediction)\n\"multithread\": how to multithread. \"ensemble\" (default) threads across ensemble members \"tullio\" threads random feature matrix algebra\n\"accelerator\": use EKP accelerators (default is no acceleration)\n\"verbose\" => false, verbose optimizer statements to check convergence, priors and optimal parameters.\n\"cov_correction\" => \"shrinkage\", type of conditioning to improve estimated covariance (Ledoit Wolfe 03), also \"nice\" for (Vishny, Morzfeld et al. 2024)\n\"ncrossvalsets\" => 2, train fraction creates (default 5) train-test data subsets, then use 'ncrossvalsets' of these stacked in the loss function. If set to 0, train=test on the full data provided ignoring \"train_fraction\".\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat};\n    regularization_matrix,\n    kwargs...\n) -> Union{Nothing, Vector{Union{LinearAlgebra.Diagonal, LinearAlgebra.UniformScaling, Matrix}}}\n\n\nBuild Vector Random Feature model for the input-output pairs subject to regularization, and optimizes the hyperparameters with EKP. \n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#GaussianProcesses.predict-Union{Tuple{M}, Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface, M}} where M<:(AbstractMatrix)","page":"Random Features","title":"GaussianProcesses.predict","text":"predict(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    new_inputs::AbstractMatrix\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#Other-utilities","page":"Random Features","title":"Other utilities","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"get_rfms\nget_fitted_features\nget_batch_sizes\nget_n_features\nget_input_dim\nget_output_dim\nEKP.get_rng\nget_kernel_structure\nget_feature_decomposition\nget_optimizer_options\noptimize_hyperparameters!(::ScalarRandomFeatureInterface) \noptimize_hyperparameters!(::VectorRandomFeatureInterface) \nshrinkage_cov","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_rfms","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_rfms","text":"get_rfms(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.RandomFeatureMethod}\n\n\ngets the rfms field\n\n\n\n\n\nget_rfms(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.RandomFeatureMethod}\n\n\nGets the rfms field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_fitted_features","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_fitted_features","text":"get_fitted_features(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.Fit}\n\n\ngets the fitted_features field\n\n\n\n\n\nget_fitted_features(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.Fit}\n\n\nGets the fitted_features field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_batch_sizes","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_batch_sizes","text":"get_batch_sizes(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Union{Nothing, Dict{S, Int64}} where S<:AbstractString\n\n\ngets batch_sizes the field\n\n\n\n\n\nget_batch_sizes(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Union{Nothing, Dict{S, Int64}} where S<:AbstractString\n\n\nGets the batch_sizes field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_n_features","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_n_features","text":"get_n_features(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Union{Nothing, Int64}\n\n\ngets the n_features field\n\n\n\n\n\nget_n_features(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Union{Nothing, Int64}\n\n\nGets the n_features field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_input_dim","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_input_dim","text":"get_input_dim(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Int64\n\n\ngets the input_dim field\n\n\n\n\n\nget_input_dim(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Int64\n\n\nGets the input_dim field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_output_dim","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_output_dim","text":"get_output_dim(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Int64\n\n\nGets the output_dim field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#EnsembleKalmanProcesses.get_rng","page":"Random Features","title":"EnsembleKalmanProcesses.get_rng","text":"get_rng(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Random.AbstractRNG\n\n\ngets the rng field\n\n\n\n\n\nget_rng(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Random.AbstractRNG\n\n\nGets the rng field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_kernel_structure","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_kernel_structure","text":"get_kernel_structure(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> CalibrateEmulateSample.Emulators.KernelStructureType\n\n\nGets the kernel_structure field\n\n\n\n\n\nget_kernel_structure(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> CalibrateEmulateSample.Emulators.KernelStructureType\n\n\nGets the kernel_structure field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_feature_decomposition","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_feature_decomposition","text":"get_feature_decomposition(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> AbstractString\n\n\ngets the feature_decomposition field\n\n\n\n\n\nget_feature_decomposition(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> AbstractString\n\n\nGets the feature_decomposition field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_optimizer_options","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_optimizer_options","text":"get_optimizer_options(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Dict{S} where S<:AbstractString\n\n\ngets the optimizer_options field\n\n\n\n\n\nget_optimizer_options(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Dict\n\n\nGets the optimizer_options field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    args...;\n    kwargs...\n)\n\n\nEmpty method, as optimization takes place within the build_models stage\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    args...;\n    kwargs...\n)\n\n\nEmpty method, as optimization takes place within the build_models stage\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.shrinkage_cov","page":"Random Features","title":"CalibrateEmulateSample.Emulators.shrinkage_cov","text":"shrinkage_cov(sample_mat::AbstractMatrix) -> Any\n\n\nCalculate the empirical covariance, additionally applying a shrinkage operator (here the Ledoit Wolf 2004 shrinkage operation). Known to have better stability properties than Monte-Carlo for low sample sizes\n\n\n\n\n\n","category":"function"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in CalibrateEmulateSample.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.","category":"page"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"Name Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta θ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-example","page":"Lorenz example","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The Lorenz 96 (hereafter L96) example is a toy-problem for the application of the CalibrateEmulateSample.jl optimization and approximate uncertainty quantification methodologies. Here is L96 with additional periodic-in-time forcing, we try to determine parameters (sinusoidal amplitude and stationary component of the forcing) from some output statistics. The standard L96 equations are implemented with an additional forcing term with time dependence. The output statistics which are used for learning are the finite time-averaged variances.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The standard single-scale L96 equations are implemented. The Lorenz 96 system (Lorenz, 1996) is given by ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"fracd x_id t = (x_i+1 - x_i-2) x_i-1 - x_i + F","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"with i indicating the index of the given longitude. The number of longitudes is given by N. The boundary conditions are given by","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"x_-1 = x_N-1  x_0 = x_N  x_N+1 = x_1","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The time scaling is such that the characteristic time is 5 days (Lorenz, 1996).  For very small values of F, the solutions x_i decay to F after the initial transient feature. For moderate values of F, the solutions are periodic, and for larger values of F, the system is chaotic. The solution variance is a function of the forcing magnitude. Variations in the base state as a function of time can be imposed through a time-dependent forcing term F(t).","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"A temporal forcing term is defined","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"F = F_s + A sin(omega t)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"with steady-state forcing F_s, transient forcing amplitude A, and transient forcing frequency omega. The total forcing F must be within the chaotic regime of L96 for all time given the prescribed N.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The L96 dynamics are solved with RK4 integration.","category":"page"},{"location":"examples/lorenz_example/#Structure","page":"Lorenz example","title":"Structure","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The example is structured with two distinct components: 1) L96 dynamical system solver; 2) calibrate-emulate sample code. Each of these are described below.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The forward mapping from input parameters to output statistics of the L96 system is solved using the GModel.jl code, which runs the L96 model across different input parameters theta. The source code for the L96 system solution is within the GModel_common.jl code. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The Calibrate code is located in calibrate.jl which provides the functionality to run the L96 dynamical system (within the GModel.jl code), extract time-averaged statistics from the L96 states, and use the time-average statistics for calibration. While this example description is self-contained, there is an additional description of the use of EnsembleKalmanProcesses.jl for the L96 example that is accessible here.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The Emulate-Sample code is located in emulate_sample.jl which provides the functionality to use the input-output pairs from the Calibrate stage for emulation and sampling (uncertainty quantification). The emulate_sample.jl code relies on outputs from the calibrate.jl code","category":"page"},{"location":"examples/lorenz_example/#Walkthrough-of-the-code","page":"Lorenz example","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"This walkthrough covers calibrate-emulate-sample for the L96 problem defined above. The goal is to learn parameters F_s and A based on the time averaged statistics in a perfect model setting. This document focuses on the emulate-sample (emulate_sample.jl) stages, but discussion of the calibration stage calibrate.jl are made when necessary. This code relies on data generated by first running calibrate.jl. A detailed walkthrough of the calibration stage of CES for the L96 example is available here. ","category":"page"},{"location":"examples/lorenz_example/#Inputs","page":"Lorenz example","title":"Inputs","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"First, we load standard packages","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# Import modules\nusing Distributions  # probability distributions and associated functions\nusing LinearAlgebra\nusing StatsPlots\nusing Plots\nusing Random\nusing JLD2","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then, we load CalibrateEmulateSample.jl packages","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# CES \nusing CalibrateEmulateSample.Emulators\nusing CalibrateEmulateSample.MarkovChainMonteCarlo\nusing CalibrateEmulateSample.Utilities\nusing CalibrateEmulateSample.EnsembleKalmanProcesses\nusing CalibrateEmulateSample.ParameterDistributions\nusing CalibrateEmulateSample.DataContainers","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The first input settings define which input-output pairs to use for training the emulator. The Calibrate stage (run using calibrate.jl) generates parameter-to-data pairs by running the L96 system using an iterative optimization approach (EnsembleKalmanProcess.jl). So we first define which iterations we would like to use data from for our emulator training","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"min_iter = 1\nmax_iter = 5 # number of EKP iterations to use data from is at most this","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The second input settings define the Lorenz dynamics. The emulate_sample.jl code does not actually run the L96 system, it only uses L96 system runs from the calibrate.jl stage to train an emulator and to perform sampling. Therefore, the settings governing the L96 dynamics are fully defined in calibrate.jl and can be modified as necessary. The rest of the input settings in this section are defined in calibrate.jl. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"F_true = 8.0 # Mean F\nA_true = 2.5 # Transient F amplitude\nω_true = 2.0 * π / (360.0 / τc) # Frequency of the transient F (non-dim)\nparams_true = [F_true, A_true]\nparam_names = [\"F\", \"A\"]","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The use of the transient forcing term is with the flag, dynamics. Stationary forcing is dynamics=1 (A=0) and transient forcing is used with dynamics=2 (Aneq0). The system with N longitudes is solved over time horizon t_start to Tfit at fixed time step dt.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"N = 36\ndt = 1/64\nt_start = 100\n# Characteristic time scale\nτc = 5.0 # days, prescribed by the L96 problem\n# This has to be less than 360 and 360 must be divisible by Ts_days\nTs_days = 30.0 # Integration length in days (will be made non-dimensional later)\n# Integration length\nTfit = Ts_days / τc","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The states are integrated over time Ts_days to construct the time averaged statistics for use by the Ensemble Kalman Process calibration. The specification of the statistics to be gathered from the states are provided by stats_type.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We implement (biased) priors as follows","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"prior_means = [F_true + 1.0, A_true + 0.5]\nprior_stds = [2.0, 0.5 * A_true]\n# constrained_gaussian(\"name\", desired_mean, desired_std, lower_bd, upper_bd)\nprior_F = constrained_gaussian(param_names[1], prior_means[1], prior_stds[1], 0, Inf)\nprior_A = constrained_gaussian(param_names[2], prior_means[2], prior_stds[2], 0, Inf)\npriors = combine_distributions([prior_F, prior_A])","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We use the recommended [constrained_gaussian] to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The priors can be plotted directly using plot(priors), as seen below in the example code from calibrate.jl","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# Plot the prior distribution\np = plot(priors, title = \"prior\")\nplot!(p.subplots[1], [F_true], seriestype = \"vline\", w = 1.5, c = :steelblue, ls = :dash, xlabel = \"F\") # vline on top histogram\nplot!(p.subplots[2], [A_true], seriestype = \"vline\", w = 1.5, c = :steelblue, ls = :dash, xlabel = \"A\") # vline on top histogram","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"<img src=\"../../assets/Lorenz-prior.png\" width=\"600\">","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The observational noise can be generated using the L96 system or prescribed, as specified by var_prescribe.  var_prescribe==false The observational noise is constructed by generating independent instantiations of the L96 statistics of interest at the true parameters for different initial conditions. The empirical covariance matrix is constructed.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"var_prescribe==true The observational noise is prescribed as a Gaussian distribution with prescribed mean and variance.","category":"page"},{"location":"examples/lorenz_example/#Calibrate","page":"Lorenz example","title":"Calibrate","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The calibration stage must be run before the emulate-sample stages. The calibration stage is run using calibrate.jl. This code will generate parameter-data pairs that will be used to train the emulator. The parameter-data pairs are visualized below","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"<img src=\"../../assets/Lorenz-training-points.png\" width=\"600\">","category":"page"},{"location":"examples/lorenz_example/#Emulate","page":"Lorenz example","title":"Emulate","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Having run the calibrate.jl code to generate input-output pairs from parameters to data using EnsembleKalmanProcesses.jl, we will now run the Emulate and Sample stages (emulate_sample.jl). First, we need to define which machine learning model we will use for the emulation. We have 8 cases that the user can toggle or customize","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"cases = [\n        \"GP\", # diagonalize, train scalar GP, assume diag inputs\n        \"RF-scalar-diagin\", # diagonalize, train scalar RF, assume diag inputs (most comparable to GP)\n        \"RF-scalar\", # diagonalize, train scalar RF, do not asume diag inputs\n        \"RF-vector-svd-diag\",\n        \"RF-vector-svd-nondiag\",\n        \"RF-vector-nosvd-diag\",\n        \"RF-vector-nosvd-nondiag\",\n        \"RF-vector-svd-nonsep\",\n]","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The first is for GP with GaussianProcesses.jl interface. The next two are for the scalar RF interface, which most closely follows exactly replacing a GP. The rest are examples of vector RF with different types of data processing, (svd = same processing as scalar RF, nosvd = unprocessed) and different RF kernel structures in the output space of increasing complexity/flexibility (diag = Separable diagonal, nondiag = Separable nondiagonal, nonsep = nonseparable nondiagonal).","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The example then loads the relevant training data that was constructed in the calibrate.jl call. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# loading relevant data\nhomedir = pwd()\nprintln(homedir)\nfigure_save_directory = joinpath(homedir, \"output/\")\ndata_save_directory = joinpath(homedir, \"output/\")\ndata_save_file = joinpath(data_save_directory, \"calibrate_results.jld2\")\nekiobj = load(data_save_file)[\"eki\"]\npriors = load(data_save_file)[\"priors\"]\ntruth_sample_mean = load(data_save_file)[\"truth_sample_mean\"]\ntruth_sample = load(data_save_file)[\"truth_sample\"]\ntruth_params_constrained = load(data_save_file)[\"truth_input_constrained\"] #true parameters in constrained space\ntruth_params = transform_constrained_to_unconstrained(priors, truth_params_constrained)\nΓy = ekiobj.obs_noise_cov","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We then set up the structure of the emulator. An example for GP (GP)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"gppackage = Emulators.GPJL()\npred_type = Emulators.YType()\nmlt = GaussianProcess(\n    gppackage;\n    kernel = nothing, # use default squared exponential kernel\n    prediction_type = pred_type,\n    noise_learn = false,\n)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"which calls GaussianProcess.jl. In this L96 example, since we focus on learning F_s and A, we do not need to explicitly learn the noise, so noise_learn = false.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"An example for scalar RF (RF-scalar)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"n_features = 100\nkernel_structure = SeparableKernel(LowRankFactor(2, nugget), OneDimFactor())\nmlt = ScalarRandomFeatureInterface(\n    n_features,\n    n_params,\n    rng = rng,\n    kernel_structure = kernel_structure,\n    optimizer_options = overrides,\n)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Optimizer options for ScalarRandomFeature.jl are provided throough overrides","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"overrides = Dict(\n            \"verbose\" => true,\n            \"scheduler\" => DataMisfitController(terminate_at = 100.0),\n            \"cov_sample_multiplier\" => 1.0,\n            \"n_iteration\" => 20,\n        )\n# we do not want termination, as our priors have relatively little interpretation","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We then build the emulator with the parameters as defined above","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"emulator = Emulator(\n            mlt,\n            input_output_pairs;\n            obs_noise_cov = Γy,\n            normalize_inputs = normalized,\n            standardize_outputs = standardize,\n            standardize_outputs_factors = norm_factor,\n            retained_svd_frac = retained_svd_frac,\n            decorrelate = decorrelate,\n        )","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"For RF and some GP packages, the training occurs during construction of the Emulator, however sometimes one must call an optimize step afterwards","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The emulator is checked for accuracy by evaluating its predictions on the true parameters","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# Check how well the Gaussian Process regression predicts on the\n# true parameters\ny_mean, y_var = Emulators.predict(emulator, reshape(truth_params, :, 1), transform_to_real = true)\ny_mean_test, y_var_test = Emulators.predict(emulator, get_inputs(input_output_pairs_test), transform_to_real = true)\n\nprintln(\"ML prediction on true parameters: \")\nprintln(vec(y_mean))\nprintln(\"true data: \")\nprintln(truth_sample) # what was used as truth\nprintln(\" ML predicted standard deviation\")\nprintln(sqrt.(diag(y_var[1], 0)))\nprintln(\"ML MSE (truth): \")\nprintln(mean((truth_sample - vec(y_mean)) .^ 2))\nprintln(\"ML MSE (next ensemble): \")\nprintln(mean((get_outputs(input_output_pairs_test) - y_mean_test) .^ 2))","category":"page"},{"location":"examples/lorenz_example/#Sample","page":"Lorenz example","title":"Sample","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Now the emulator is constructed and validated, so we next focus on the MCMC sampling. First, we run a short chain (2000 steps) to determine the step size","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# First lets run a short chain to determine a good step size\nmcmc = MCMCWrapper(RWMHSampling(), truth_sample, priors, emulator; init_params = u0)\nnew_step = optimize_stepsize(mcmc; init_stepsize = 0.1, N = 2000, discard_initial = 0)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The step size has been determined, so now we run the full MCMC (100000 steps where the first 2000 are discarded)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# Now begin the actual MCMC\nprintln(\"Begin MCMC - with step size \", new_step)\nchain = MarkovChainMonteCarlo.sample(mcmc, 100_000; stepsize = new_step, discard_initial = 2_000)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"And we finish by extracting the posterior samples","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"And evaluate the results with these printed statements","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"post_mean = mean(posterior)\npost_cov = cov(posterior)\nprintln(\"post_mean\")\nprintln(post_mean)\nprintln(\"post_cov\")\nprintln(post_cov)\nprintln(\"D util\")\nprintln(det(inv(post_cov)))\nprintln(\" \")","category":"page"},{"location":"examples/lorenz_example/#Running-the-Example-and-Postprocessing","page":"Lorenz example","title":"Running the Example and Postprocessing","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"First, the calibrate code must be executed, which will perform the calibration step and, generating input-output pairs of the parameter to data mapping. Then, the emulate-sample code is run, which will load the input-ouput pairs that were generated in the calibration step.","category":"page"},{"location":"examples/lorenz_example/#Calibrate-2","page":"Lorenz example","title":"Calibrate","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The L96 parameter calibration can be run using julia --project calibrate.jl","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The output will provide the estimated parameters in the constrained ϕ-space. The priors are required in the get-method to apply these constraints.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Printed output:","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"# EKI results: Has the ensemble collapsed toward the truth?\nprintln(\"True parameters: \")\nprintln(params_true)\nprintln(\"\\nEKI results:\")\nprintln(get_ϕ_mean_final(priors, ekiobj))","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The parameters and forward model outputs will be saved in parameter_storage.jld2 and data_storage.jld2, respectively. The data will be saved in the directory output. A scatter plot animation of the ensemble convergence to the true parameters is saved in the directory output. These points represent the training points that are used for the emulator.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"<img src=\"../../assets/Lorenz-training-points.png\" width=\"600\">","category":"page"},{"location":"examples/lorenz_example/#Emulate-sample","page":"Lorenz example","title":"Emulate-sample","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The L96 parameter estimation can be run using julia --project emulate_sample.jl","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The output will provide the estimated posterior distribution over the parameters. The emulate-sample code will run for several choices in the machine learning model that is used for the emulation stage, inclding Gaussian Process regression and RF, and using singular value data decorrelation or not. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"The sampling results from two emulators are shown below. We can see that the posterior is relatively insensitive to the choice of the machine learning emulation tool in this L96 example.","category":"page"},{"location":"examples/lorenz_example/#L96-CES-example-case:-GP-regression-emulator-(case\"GP\")","page":"Lorenz example","title":"L96 CES example case: GP regression emulator (case=\"GP\")","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"<img src=\"../../assets/Lorenz-posterior.png\" width=\"600\">","category":"page"},{"location":"examples/lorenz_example/#L96-CES-example-case:-RF-scalar-emulator-(case\"RF-scalar\")","page":"Lorenz example","title":"L96 CES example case: RF scalar emulator (case=\"RF-scalar\")","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"<img src=\"../../assets/Lorenz-posterior-RF.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Regression-of-\\mathbb{R}2-\\to-\\mathbb{R}2-smooth-function","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"info: How do I run this code?\nThe full code is found in the examples/Emulator/ directory of the github repository","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"In this example, we assess directly the performance of our machine learning emulators. The task is to learn the function:","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"Gcolon 02pi^2 to mathbbR^2 G(x_1x_2) = (sin(x_1) + cos(x_2) sin(x_1) - cos(x_2)) ","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"observed at 150 points, subject to additive (and possibly correlated) Gaussian noise N(0Sigma).","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We have several different emulator configurations in this example that the user can play around with. The goal of this example is to predict the function (i.e. posterior mean) and uncertainty (i.e posterior pointwise variance) on a 200times200 grid providing a mean square error between emulated and true function and  with plot_flag = true we also save several plots.","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We will term scalar-output Gaussin process emulator as \"GP\", scalar-output random feature emulator as \"scalar RF\", and vector-output random feature emulator as \"vector RF\" henceforth.","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Walkthrough-of-the-code","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We first import some standard packages","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"using Random\nusing StableRNGs\nusing Distributions\nusing Statistics\nusing LinearAlgebra","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"and relevant CES packages needed to define the emulators, packages and kernel structures","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"using CalibrateEmulateSample.Emulators\n# Contains `Emulator`, `GaussianProcess`, `ScalarRandomFeatureInterface`, `VectorRandomFeatureInterface`\n# `GPJL`, `SKLJL`, `SeparablKernel`, `NonSeparableKernel`, `OneDimFactor`, `LowRankFactor`, `DiagonalFactor`\nusing CalibrateEmulateSample.DataContainers # Contains `PairedDataContainer`","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"To play with the hyperparameter optimization of RF, the optimizer options sometimes require EnsembleKalmanProcesses.jl structures, so we load this too","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"using CalibrateEmulateSample.EnsembleKalmanProcesses # Contains `DataMisfitController`","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We have 9 cases that the user can toggle or customize","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"cases = [\n    \"gp-skljl\",\n    \"gp-gpjl\", # Very slow prediction...\n    \"rf-scalar\",\n    \"rf-svd-diag\",\n    \"rf-svd-nondiag\",\n    \"rf-nosvd-diag\",\n    \"rf-nosvd-nondiag\",\n    \"rf-svd-nonsep\",\n    \"rf-nosvd-nonsep\",\n]","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"The first two are for GP with either ScikitLearn.jl or GaussianProcesses.jl interface. The third is for the scalar RF interface, which most closely follows exactly replacing a GP. The rest are examples of vector RF with different types of data processing, (svd = same processing as scalar RF, nosvd = unprocessed) and different RF kernel structures in the output space of increasing complexity/flexibility (diag = Separable diagonal, nondiag = Separable nondiagonal, nonsep = nonseparable nondiagonal).","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We set up the learning problem specification, defining input and output dimensions, and number of data to train on, and the function g and the perturbed samples y with correlated additive noise","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"n = 150  # number of training points\np = 2   # input dim \nd = 2   # output dim\nX = 2.0 * π * rand(p, n)\n# G(x1, x2)\ng1x = sin.(X[1, :]) .+ cos.(X[2, :])\ng2x = sin.(X[1, :]) .- cos.(X[2, :])\ngx = zeros(2, n)\ngx[1, :] = g1x\ngx[2, :] = g2x\n# Add noise η\nμ = zeros(d)\nΣ = 0.1 * [[0.8, 0.1] [0.1, 0.5]] # d x d\nnoise_samples = rand(MvNormal(μ, Σ), n)\n# y = G(x) + η\nY = gx .+ noise_samples","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We then enter this in a paired data container, which gives a standard of how the data will be read","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"iopairs = PairedDataContainer(X, Y, data_are_columns = true)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We define some common settings for all emulators, e.g. the number of random features to use, and some hyperparameter optimizer options","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# common Gaussian feature setup\npred_type = YType()\n\n# common random feature setup\nn_features = 150\noptimizer_options = Dict(\"n_iteration\" => 10, \"scheduler\" => DataMisfitController(on_terminate = \"continue\"))\nnugget = 1e-12","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We then build the emulators. An example for GP (gp-skljl)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# use scikit learn\ngppackage = SKLJL()\n# build a GP that learns an additional white noise kernel (along with the default RBF kernel)\ngaussian_process = GaussianProcess(gppackage, noise_learn = true)\n# the data processing normalizes input data, and decorrelates output data with information from Σ\nemulator = Emulator(gaussian_process, iopairs, obs_noise_cov = Σ, normalize_inputs = true) ","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"An example for scalar RF (rf-scalar)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# build a scalar RF with a rank-2 kernel in input space (placeholder 1D kernel in output space) and use the optimizer options during training\nsrfi = ScalarRandomFeatureInterface(\n    n_features, \n    p, \n    kernel_structure = SeparableKernel(LowRankFactor(2, nugget), OneDimFactor()), \n    optimizer_options = optimizer_options,\n)\n# the data processing normalizes input data, and decorrelates output data with information from Σ\nemulator = Emulator(srfi, iopairs, obs_noise_cov = Σ, normalize_inputs = true)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"An example for vector RF (rf-nosvd-nonsep)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# build a vector RF with a rank-4 nonseparable kernel and use the optimizer options during training\nvrfi = VectorRandomFeatureInterface(\n    n_features,\n    p,\n    d, # additionally provide the output dimensions size\n    kernel_structure = NonseparableKernel(LowRankFactor(4, nugget)),\n    optimizer_options = optimizer_options,\n)\n# the data processing normalizes input data, and does not decorrelate outputs\nemulator = Emulator(vrfi, iopairs, obs_noise_cov = Σ, normalize_inputs = true, decorrelate = false)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"For RF and some GP packages, the training occurs during construction of the Emulator, however sometimes one must call an optimize step afterwards","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Validation-and-Plots","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Validation and Plots","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We create an evaluation grid for our models, in the right shape:","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"n_pts = 200\nx1 = range(0.0, stop = 2 * π, length = n_pts)\nx2 = range(0.0, stop = 2 * π, length = n_pts)\nX1, X2 = meshgrid(x1, x2)\ninputs = permutedims(hcat(X1[:], X2[:]), (2, 1))","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We predict using the emulators at the new inputs, and transform_to_real inverts the data processing back to physical values","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"em_mean, em_cov = predict(emulator, inputs, transform_to_real = true)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We then plot the predicted mean and pointwise variances, and calculate the errors from the three highlighted cases:","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Gaussian-Process-Emulator-(Sci-kit-learn:-gp-skljl)","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Gaussian Process Emulator (Sci-kit learn: gp-skljl)","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"L^2 error of mean and latent truth:0.0008042391077774167","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"<img src=\"../../../assets/regression2d2d-gp-skljl_y1_predictions.png\" width=\"600\">\n<img src=\"../../../assets/regression2d2d-gp-skljl_y2_predictions.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Random-Feature-Emulator-(rf-scalar)","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Random Feature Emulator (rf-scalar)","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"L^2 error of mean and latent truth:0.0012253119679379056","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"<img src=\"../../../assets/regression2d2d-rf-scalar_y1_predictions.png\" width=\"600\">\n<img src=\"../../../assets/regression2d2d-rf-scalar_y2_predictions.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Random-Feature-Emulator-(vector:-rf-nosvd-nonsep)","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Random Feature Emulator (vector: rf-nosvd-nonsep)","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"L^2 error of mean and latent truth:0.0011094292509180393","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"<img src=\"../../../assets/regression2d2d-rf-nosvd-nonsep_y1_predictions.png\" width=\"600\">\n<img src=\"../../../assets/regression2d2d-rf-nosvd-nonsep_y2_predictions.png\" width=\"600\">","category":"page"},{"location":"GaussianProcessEmulator/#Gaussian-Process-Emulator","page":"Gaussian Process","title":"Gaussian Process Emulator","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"One type of MachineLearningTool we provide for emulation is a Gaussian process. Gaussian processes are a generalization of the Gaussian probability distribution, extended to functions rather than random variables. They can be used for statistical emulation, as they provide both mean and covariances. To build a Gaussian process, we first define a prior over all possible functions, by choosing the covariance function or kernel. The kernel describes how similar two outputs (y_i, y_j) are, given the similarities between their input values (x_i, x_j). Kernels encode the functional form of these relationships and are defined by hyperparameters, which are usually initially unknown to the user. To learn the posterior Gaussian process, we condition on data using Bayes theorem and optimize the hyperparameters of the kernel.  Then, we can make predictions to predict a mean function and covariance for new data points.","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"A useful resource to learn about Gaussian processes is Rasmussen and Williams (2006).","category":"page"},{"location":"GaussianProcessEmulator/#User-Interface","page":"Gaussian Process","title":"User Interface","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"CalibrateEmulateSample.jl allows the Gaussian process emulator to be built using either GaussianProcesses.jl  or ScikitLearn.jl. Different packages may be optimized for different settings, we recommend users give both a try, and checkout the individual package documentation to make a choice for their problem setting. ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"To use GaussianProcesses.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.GPJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"To use ScikitLearn.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.SKLJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Initialize a basic Gaussian Process with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(gppackage)","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"This initializes the prior Gaussian process.  We train the Gaussian process by feeding the gauss_proc alongside the data into the Emulator struct and optimizing the hyperparameters, described here.","category":"page"},{"location":"GaussianProcessEmulator/#Prediction-Type","page":"Gaussian Process","title":"Prediction Type","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can specify the type of prediction when initializing the Gaussian Process emulator. The default type of prediction is to predict data, YType().  You can create a latent function type prediction with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage,\n    prediction_type = FType())\n","category":"page"},{"location":"GaussianProcessEmulator/#Kernels","page":"Gaussian Process","title":"Kernels","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"The Gaussian process above assumes the default kernel: the Squared Exponential kernel, also called  the Radial Basis Function (RBF).  A different type of kernel can be specified when the Gaussian process is initialized.  Read more about kernel options here.","category":"page"},{"location":"GaussianProcessEmulator/#GPJL","page":"Gaussian Process","title":"GPJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"For the GaussianProcess.jl package, there are a range of kernels to choose from.  For example, ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nmy_kernel = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You do not need to provide useful hyperparameter values when you define the kernel, as these are learned in  optimize_hyperparameters!(emulator).","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine kernels together through linear operations, for example,","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nkernel_1 = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\nkernel_2 = GaussianProcesses.Lin(0.)               # Create a linear kernel with lengthscale=0\nmy_kernel = kernel_1 + kernel_2                    # Create a new additive kernel\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/#SKLJL","page":"Gaussian Process","title":"SKLJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Alternatively if you are using the ScikitLearn.jl package, you can find the list of kernels here.  These need this preamble:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using PyCall\nusing ScikitLearn\nconst pykernels = PyNULL()\nfunction __init__()\n    copy!(pykernels, pyimport(\"sklearn.gaussian_process.kernels\"))\nend","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Then they are accessible, for example, as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"my_kernel = pykernels.RBF(length_scale = 1)\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine multiple ScikitLearn kernels via linear operations in the same way as above.","category":"page"},{"location":"GaussianProcessEmulator/#Learning-additional-white-noise","page":"Gaussian Process","title":"Learning additional white noise","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Often it is useful to learn the discrepancy between the Gaussian process prediction and the data, by learning additional white noise. Though one often knows, and provides, the discrepancy between the true model and data with an observational noise covariance; the additional white kernel can help account for approximation error from the selected Gaussian process kernel and the true model. This is added with the Boolean keyword noise_learn when initializing the Gaussian process. The default is true. ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = true )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"When noise_learn is true, an additional white noise kernel is added to the kernel. This white noise is present across all parameter values, including the training data.  The scale parameters of the white noise kernel are learned in optimize_hyperparameters!(emulator). ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You may not need to learn the noise if you already have a good estimate of the noise from your training data, and if the Gaussian process kernel is well specified.  When noise_learn is false, a small additional regularization is added for stability. The default value is 1e-3 but this can be chosen through the optional argument alg_reg_noise:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = false,\n    alg_reg_noise = 1e-3 )","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Integrating-Lorenz-63-with-an-emulated-integrator","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"info: How do I run this code?\nThe full code is found in the examples/Emulator/ directory of the github repository","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"In this example, we assess directly the performance of our machine learning emulators. The task is to learn the forward Euler integrator of a Lorenz 63 system. The model parameters are set to their classical settings (sigma rho beta) = (1028frac83) to exhibit chaotic behavior. The discrete system is given as:","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"beginaligned\nx(t_n+1) = x(t_n) + Delta t(y(t_n) - x(t_n))\ny(t_n+1) = y(t_n) + Delta t(x(t_n)(28 - z(t_n)) - y(t_n))\nz(t_n+1) = z(t_n) + Delta t(x(t_n)y(t_n) - frac83z(t_n))\nendaligned","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"where t_n = nDelta t. The data consists of pairs  (x(t_k)y(t_k)z(t_k)) (x(t_k+1)y(t_k+1)z(t_k+1)+eta_k for 600 values of k, with each output subjected to independent, additive Gaussian noise eta_ksim N(0Gamma_y).","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We have several different emulator configurations in this example that the user can play around with. The goal of the emulator is that the posterior mean will approximate this discrete update map, or integrator, for any point on the Lorenz attractor from the sparse noisy data. To validate this, we recursively apply the trained emulator to the state, plotting the evolution of the trajectory and marginal statistics of the states over short and long timescales. We include a repeats option (n_repeats) to run the randomized training for multiple trials and illustrate robustness of marginal statistics by plotting long time marginal cdfs of the state. ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We will term scalar-output Gaussin process emulator as \"GP\", and scalar- or vector-output random feature emulator as \"RF\".","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Walkthrough-of-the-code","page":"Integrating Lorenz 63 with an emulated integrator","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We first import some standard packages","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using Random, Distributions, LinearAlgebra # utilities\nusing CairoMakie, ColorSchemes # for plots\nusing JLD2 # for saved data","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For the true integrator of the Lorenz system we import","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using OrdinaryDiffEq ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For relevant CES packages needed to define the emulators, packages and kernel structures","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using CalibrateEmulateSample.Emulators\n# Contains `Emulator`, `GaussianProcess`, `ScalarRandomFeatureInterface`, `VectorRandomFeatureInterface`\n# `GPJL`, `SeparablKernel`, `NonSeparableKernel`, `OneDimFactor`, `LowRankFactor`, `DiagonalFactor`\nusing CalibrateEmulateSample.DataContainers # Contains `PairedDataContainer`","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"and if one wants to play with optimizer options for the random feature emulators we import","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using CalibrateEmulateSample.EnsembleKalmanProcesses # Contains `DataMisfitController`","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We generate the truth data using OrdinaryDiffEq: the time series sol is used for training data, sol_test is used for plotting short time trajectories, and sol_hist for plotting histograms of the state over long times:","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"# Run L63 from 0 -> tmax\nu0 = [1.0; 0.0; 0.0]\ntmax = 20\ndt = 0.01\ntspan = (0.0, tmax)\nprob = ODEProblem(lorenz, u0, tspan)\nsol = solve(prob, Euler(), dt = dt)\n\n# Run L63 from end for test trajectory data\ntmax_test = 100\ntspan_test = (0.0, tmax_test)\nu0_test = sol.u[end]\nprob_test = ODEProblem(lorenz, u0_test, tspan_test)\nsol_test = solve(prob_test, Euler(), dt = dt)\n\n# Run L63 from end for histogram matching data\ntmax_hist = 1000\ntspan_hist = (0.0, tmax_hist)\nu0_hist = sol_test.u[end]\nprob_hist = ODEProblem(lorenz, u0_hist, tspan_hist)\nsol_hist = solve(prob_hist, Euler(), dt = dt)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We generate the training data from sol within [tburn, tmax]. The user has the option of how many training points to take n_train_pts and whether these are selected randomly or sequentially (sample_rand). The true outputs are perturbed by noise of variance 1e-4 and pairs are stored in the compatible data format PairedDataContainer","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"tburn = 1 # NB works better with no spin-up!\nburnin = Int(floor(tburn / dt))\nn_train_pts = 600 \nsample_rand = true\nif sample_rand\n   ind = Int.(shuffle!(rng, Vector(burnin:(tmax / dt - 1)))[1:n_train_pts])\nelse\n   ind = burnin:(n_train_pts + burnin)\nend\nn_tp = length(ind)\ninput = zeros(3, n_tp)\noutput = zeros(3, n_tp)\nΓy = 1e-4 * I(3)\nnoise = rand(rng, MvNormal(zeros(3), Γy), n_tp)\nfor i in 1:n_tp\n    input[:, i] = sol.u[ind[i]]\n    output[:, i] = sol.u[ind[i] + 1] + noise[:, i]\nend\niopairs = PairedDataContainer(input, output)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We have several cases the user can play with,","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"cases = [\"GP\", \"RF-scalar\", \"RF-scalar-diagin\", \"RF-svd-nonsep\", \"RF-nosvd-nonsep\", \"RF-nosvd-sep\"]","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"Then, looping over the repeats, we first define some common hyperparamter optimization options for the \"RF-\" cases. In this case, the options are used primarily for diagnostics and acceleration (not required in general to solve this problem) ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"rf_optimizer_overrides = Dict(\n    \"verbose\" => true, # output diagnostics from the hyperparameter optimizer\n    \"scheduler\" => DataMisfitController(terminate_at = 1e4), # timestepping method for the optimizer\n    \"cov_sample_multiplier\" => 0.5, # 0.5*default number of samples to estimate covariances in optimizer\n    \"n_features_opt\" => 200, # number of features during hyperparameter optimization\n    \"n_iteration\" => 20, # iterations of the optimizer solver\n)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"Then we build the machine learning tools. Here we highlight scalar-output Gaussian process (GP), where we use the default squared-exponential kernel, and learn a lengthscale hyperparameter in each input dimension. To handle multiple outputs, we will use a decorrelation in the output space, and so will actually train three of these models.","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"gppackage = Emulators.GPJL() # use GaussianProcesses.jl\npred_type = Emulators.YType() # predicted variances are for data not latent function\nmlt = GaussianProcess(\n    gppackage;\n    prediction_type = pred_type,\n    noise_learn = false, # do not additionally learn a white kernel\n)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"we also highlight the Vector Random Feature with nonseparable kernel (RF-nosvd-nonsep), this can natively handle multiple outputs without decorrelation of the output space. This kernel is a rank-3 representation with small nugget term.","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"nugget = 1e-12\nkernel_structure = NonseparableKernel(LowRankFactor(3, nugget))\nn_features = 500 # number of features for prediction\nmlt = VectorRandomFeatureInterface(\n    n_features,\n    3, # input dimension\n    3, # output dimension\n    rng = rng, # pin random number generator\n    kernel_structure = kernel_structure,\n    optimizer_options = rf_optimizer_overrides, \n)           ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"With machine learning tools specified, we build the emulator object","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"# decorrelate = true for `GP`\n# decorrelate = false for `RF-nosvd-nonsep`\nemulator = Emulator(mlt, iopairs; obs_noise_cov = Γy, decorrelate = decorrelate) \noptimize_hyperparameters!(emulator) # some GP packages require this additional call ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Plots","page":"Integrating Lorenz 63 with an emulated integrator","title":"Plots","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We predict the trained emulator mean, over the short-timescale validation trajectory","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"u_test_tmp = zeros(3, length(xspan_test))\nu_test_tmp[:, 1] = sol_test.u[1] # initialize at the final-time solution of the training period\n\nfor i in 1:(length(xspan_test) - 1)\n    rf_mean, _ = predict(emulator, u_test_tmp[:, i:i], transform_to_real = true) # 3x1 matrix\n    u_test_tmp[:, i + 1] = rf_mean\nend","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"The other trajectories are similar. We then produce the following plots. In all figures, the results from evolving the state with the true integrator is orange, and with the emulated integrators are blue.","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Gaussian-Process-Emulator-(Sci-kit-learn:-GP)","page":"Integrating Lorenz 63 with an emulated integrator","title":"Gaussian Process Emulator (Sci-kit learn: GP)","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For one example fit","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"<img src=\"../../../assets/GP_l63_test.png\" width=\"600\">\n<img src=\"../../../assets/GP_l63_attr.png\" width=\"300\"><img src=\"../../../assets/GP_l63_pdf.png\" width=\"300\">","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Random-Feature-Emulator-(RF-nosvd-nonsep)","page":"Integrating Lorenz 63 with an emulated integrator","title":"Random Feature Emulator (RF-nosvd-nonsep)","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For one example fit","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"<img src=\"../../../assets/RF-nosvd-nonsep_l63_test.png\" width=\"600\">\n<img src=\"../../../assets/RF-nosvd-nonsep_l63_attr.png\" width=\"300\"><img src=\"../../../assets/RF-nosvd-nonsep_l63_pdf.png\" width=\"300\">","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"and here are CDFs over 20 randomized trials of the random feature hyperparameter optimization","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"<img src=\"../../../assets/RF-nosvd-nonsep_l63_cdfs.png\" width=\"600\">","category":"page"},{"location":"API/GaussianProcess/#GaussianProcess","page":"Gaussian Process","title":"GaussianProcess","text":"","category":"section"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"GaussianProcessesPackage\nPredictionType\nGaussianProcess\nGaussianProcess(\n    ::GPPkg;\n    ::Union{K, KPy, AK, Nothing},\n    ::Any,\n    ::FT,\n    ::PredictionType,\n) where {GPPkg <: GaussianProcessesPackage, K <: GaussianProcesses.Kernel, KPy <: PyObject, AK <:AbstractGPs.Kernel, FT <: AbstractFloat}\nbuild_models!(::GaussianProcess{GPJL}, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\noptimize_hyperparameters!(::GaussianProcess{GPJL})\npredict(::GaussianProcess{GPJL},  ::AbstractMatrix{FT}) where {FT <: AbstractFloat}","category":"page"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcessesPackage","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcessesPackage","text":"abstract type GaussianProcessesPackage\n\nType to dispatch which GP package to use:\n\nGPJL for GaussianProcesses.jl, [julia - gradient-free only]\nSKLJL for the ScikitLearn GaussianProcessRegressor, [python - gradient-free]\nAGPJL for AbstractGPs.jl, [julia - ForwardDiff compatible]\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.PredictionType","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.PredictionType","text":"abstract type PredictionType\n\nPredict type for GPJL in GaussianProcesses.jl:\n\nYType\nFType latent function.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcess","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcess","text":"struct GaussianProcess{GPPackage, FT} <: CalibrateEmulateSample.Emulators.MachineLearningTool\n\nStructure holding training input and the fitted Gaussian process regression models.\n\nFields\n\nmodels::Vector{Union{Nothing, PyCall.PyObject, AbstractGPs.PosteriorGP, GaussianProcesses.GPE}}: The Gaussian Process (GP) Regression model(s) that are fitted to the given input-data pairs.\nkernel::Union{Nothing, var\"#s18\", var\"#s19\", var\"#s20\"} where {var\"#s18\"<:GaussianProcesses.Kernel, var\"#s19\"<:PyCall.PyObject, var\"#s20\"<:KernelFunctions.Kernel}: Kernel object.\nnoise_learn::Bool: Learn the noise with the White Noise kernel explicitly?\nalg_reg_noise::Any: Additional observational or regularization noise in used in GP algorithms\nprediction_type::CalibrateEmulateSample.Emulators.PredictionType: Prediction type (y to predict the data, f to predict the latent function).\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcess-Union{Tuple{FT}, Tuple{AK}, Tuple{KPy}, Tuple{K}, Tuple{GPPkg}} where {GPPkg<:CalibrateEmulateSample.Emulators.GaussianProcessesPackage, K<:GaussianProcesses.Kernel, KPy<:PyCall.PyObject, AK<:KernelFunctions.Kernel, FT<:AbstractFloat}","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcess","text":"GaussianProcess(\n    package::AbstractFloat;\n    kernel,\n    noise_learn,\n    alg_reg_noise,\n    prediction_type\n)\n\n\npackage - GaussianProcessPackage object.\nkernel - GaussianProcesses kernel object. Default is a Squared Exponential kernel.\nnoise_learn - Boolean to additionally learn white noise in decorrelated space. Default is true.\nalg_reg_noise - Float to fix the (small) regularization parameter of algorithms when noise_learn = true\nprediction_type - PredictionType object. Default predicts data, not latent function (FType()).\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat};\n    kwargs...\n)\n\n\nMethod to build Gaussian process models based on the package.\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}}","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    args...;\n    kwargs...\n)\n\n\nOptimize Gaussian process hyperparameters using in-build package method.\n\nWarning: if one uses GPJL() and wishes to modify positional arguments. The first positional argument must be the Optim method (default LBGFS()).\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#GaussianProcesses.predict-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, AbstractMatrix{FT}}} where FT<:AbstractFloat","page":"Gaussian Process","title":"GaussianProcesses.predict","text":"predict(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2}\n) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\n","category":"method"},{"location":"examples/emulators/global_sens_analysis/#Global-Sensitiviy-Analysis-(GSA)-test-functions","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"info: How do I run this code?\nThe full code is found in the examples/Emulator/ directory of the github repository","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"In this example, we assess directly the performance of our machine learning emulators. The task is to learn a function for use in a global sensitivity analysis. In particular, we have two cases:","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"The Ishigami function","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"f(x a b) = (1 + bx_3^4)sin(x_1) + a sin(x_2) forall xin -pipi^3","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"with a=7 b=01.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"The Sobol G-function","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"f(x a) = prod_i=1^d frac4x_i - 2+a_i1+x_i","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"with the user-chosen input dimension d, and coefficients a_i = fraci-12 geq 0 for i=1dotsd, where small i (and thus small a_i) imply larger first-order effects, and interactions are primarily present between these variables.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"In this example, global sensitivity analysis refers to calculation of two Sobol indices. The first index collects proportions V_i (a.k.a firstorder) of the variance of f attributable to the input x_i, and the second index collects proportions TV_i (a.k.a totalorder) of the residual variance having removed contributions attributable to inputs x_j forall jneq i. The Ishigami function has an analytic formula for these Sobol indices, it is also known that one can obtain numerical approximation through quasi-Monto-Carlo methods by evaluating the Ishigami function on a special quasi-random Sobol sequence.","category":"page"},{"location":"examples/emulators/global_sens_analysis/#Ishigami","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Ishigami","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"To emulate the Ishigami function, the data consists of 300 pairs xf(x)+eta where eta sim N(0Sigma) is additive noise, and the x are sampled from the Sobol sequence. The emulators are validated by evaluating the posterior mean function on the full 16000 points of the Sobol sequence and the Sobol indices are estimated. We rerun the experiment for many repeats of the random feature hyperparameter optimization and present the statistics of these indices, as well as plotting a realization of the emulator.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"We use the package GlobalSensitivityAnalysis.jl for many of the GSA tools.","category":"page"},{"location":"examples/emulators/global_sens_analysis/#Walkthrough-of-the-code","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"We first load some standard packages","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"using Distributions\nusing DataStructures # for `OrderedDict`\nusing Random\nusing LinearAlgebra\nusing CairoMakie, ColorSchemes ","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"and the packages for providing the Ishigami function, Sobol sequence, and evaluation of the indices","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"using GlobalSensitivityAnalysis # for `SobolData`\nconst GSA = GlobalSensitivityAnalysis","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"then the CES packages for the emulators","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"using CalibrateEmulateSample.Emulators # for `SKLJL`, `GaussianProcess`, `SeparableKernel`, `LowRankFactor`, `OneDimFactor`, `ScalarRandomFeatureInterface`, `Emulator`\nusing CalibrateEmulateSample.DataContainers # for `PairedDataContainer`\nusing CalibrateEmulateSample.EnsembleKalmanProcesses # for `DataMisfitController`","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"We set up the sampling procedure, evaluate the true ishigami function on these points, and calculate the sobol indices","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"n_data_gen = 2000 \n\ndata = SobolData(\n    params = OrderedDict(:x1 => Uniform(-π, π), :x2 => Uniform(-π, π), :x3 => Uniform(-π, π)),\n    N = n_data_gen,\n)\n\n# To perform global analysis,\n# one must generate samples using Sobol sequence (i.e. creates more than N points)\nsamples = GSA.sample(data)\nn_data = size(samples, 1) # [n_samples x 3]\n# run model (example)\ny = GSA.ishigami(samples)\n# perform Sobol Analysis\nresult = analyze(data, y)","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"Next we create the noisy training data from the sequence samples","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"n_train_pts = 300\nind = shuffle!(rng, Vector(1:n_data))[1:n_train_pts]\n# now subsample the samples data\nn_tp = length(ind)\ninput = zeros(3, n_tp)\noutput = zeros(1, n_tp)\nΓ = 1e-2\nnoise = rand(rng, Normal(0, Γ), n_tp)\nfor i in 1:n_tp\n    input[:, i] = samples[ind[i], :]\n    output[i] = y[ind[i]] + noise[i]\nend\niopairs = PairedDataContainer(input, output)","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"We have a few cases for the user to investigate","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"cases = [\n    \"Prior\", # Scalar random feature emulator with no hyperparameter learning\n    \"GP\", # Trained Sci-kit learn Gaussian process emulator\n    \"RF-scalar\", # Trained scalar random feature emulator\n]","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"Each case sets up a different machine learning configuration in the Emulator object.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"For the random feature case, RF-scalar, we use a rank-3 kernel in the input space, and 500 features for prediction, though for efficiency we use only 200 when learning the hyperparameters. The relevant code snippets are","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"nugget = Float64(1e-12)\noverrides = Dict(\n    \"scheduler\" => DataMisfitController(terminate_at = 1e4),\n    \"n_features_opt\" => 200,\n)\n\nkernel_structure = SeparableKernel(LowRankFactor(3, nugget), OneDimFactor())\nn_features = 500\nmlt = ScalarRandomFeatureInterface(\n    n_features,\n    3,\n    rng = rng,\n    kernel_structure = kernel_structure,\n    optimizer_options = overrides,\n)","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"For the gaussian process case GP we use the sci-kit learn package, a default squared exponential kernel with lengthscale learnt in each input dimensions. We do not learn an additional white kernel for the noise.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"gppackage = Emulators.SKLJL()\npred_type = Emulators.YType()\nmlt = GaussianProcess(\n    gppackage;\n    prediction_type = pred_type,\n    noise_learn = false,\n)","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"We finish by building the emulator object","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"emulator = Emulator(mlt, iopairs; obs_noise_cov = Γ * I, decorrelate = decorrelate)\noptimize_hyperparameters!(emulator) # (only needed for some Emulator packages)","category":"page"},{"location":"examples/emulators/global_sens_analysis/#Results-and-plots","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Results and plots","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"We validate the emulator by evaluating it on the entire Sobol sequence, and calculating the Sobol indices (presenting mean and std if using multiple repeats.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"# predict on all Sobol points with emulator (example)    \ny_pred, y_var = predict(emulator, samples', transform_to_real = true)\n\n# obtain emulated Sobol indices\nresult_pred = analyze(data, y_pred')","category":"page"},{"location":"examples/emulators/global_sens_analysis/#Gaussian-Process-Emulator-(sci-kit-learn-GP)","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Gaussian Process Emulator (sci-kit learn GP)","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"Here is the plot for one emulation","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"<img src=\"../../../assets/ishigami_slices_GP.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"and the outputted table of Sobol indices","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"True Sobol Indices\n******************\n    firstorder: [0.31390519114781146, 0.44241114479004084, 0.0]\n    totalorder: [0.5575888552099592, 0.44241114479004084, 0.24368366406214775]\n \nSampled truth Sobol Indices (# points 16000)\n***************************\n    firstorder: [0.31261591941512257, 0.441887746224135, -0.005810964687365922]\n    totalorder: [0.5623611180844434, 0.44201284296404386, 0.24465876318633062]\n \nSampled Emulated Sobol Indices (# obs 300, noise var 0.01)\n***************************************************************\n    firstorder: [0.3094638183079643, 0.4518400892052567, -0.007351344957260407]\n    totalorder: [0.5502469909342245, 0.4587734278791574, 0.23542404141319245]","category":"page"},{"location":"examples/emulators/global_sens_analysis/#Random-feature-emulator-(Separable-Low-rank-kernel-RF-scalar)","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Random feature emulator (Separable Low-rank kernel RF-scalar)","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"Here is the plot for one emulation","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"<img src=\"../../../assets/ishigami_slices_RF-scalar.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"Table for 20 repeats of the algorithm","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"True Sobol Indices\n******************\n    firstorder: [0.31390519114781146, 0.44241114479004084, 0.0]\n    totalorder: [0.5575888552099592, 0.44241114479004084, 0.24368366406214775]\n \nSampled truth Sobol Indices (# points 16000)\n***************************\n    firstorder: [0.31261591941512257, 0.441887746224135, -0.005810964687365922]\n    totalorder: [0.5623611180844434, 0.44201284296404386, 0.24465876318633062]\n \nSampled Emulated Sobol Indices (# obs 300, noise var 0.01)\n***************************************************************\n(mean) firstorder: [0.33605548545490044, 0.41116050093679196, -0.0012213648484969539]\n(std)  firstorder: [0.05909336956162543, 0.11484966121124164, 0.012908533302492602]\n(mean) totalorder: [0.5670345355855254, 0.4716028261179354, 0.24108222433317]\n(std)  totalorder: [0.10619345801872732, 0.1041023777237331, 0.07200225781785778]\n","category":"page"},{"location":"examples/emulators/global_sens_analysis/#Sobol-G-function-results","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Sobol G-function results","text":"","category":"section"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"To emulate the Sobol function, a similar code script is used to set up the Ishigami emulation. The primary change is that the input dimension is now a user parameter n_dimension that can be adjusted, and some reasonable defaults are set within the script. As an output, plots are produced of the Sobol function values, and slices through the function.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"For example, we repeat the scalar random feature emulation task 30 times over different training realizations.","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"For three input dimensions, one obtains the following plot of the analytic indices (X-true), qMC-approximated with true function (X-approx), and the 95% confidence interval of the qMC-approximated with emulator (X-emulate).","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"<img src=\"../../../assets/GFunction_sens_RF-scalar_3.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"One also obtains the slices through the emulated G-function, with red being the training points and blue being the prediction","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"<img src=\"../../../assets/GFunction_slices_RF-scalar_3.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"For ten input dimensions one obtains similar plots","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"<img src=\"../../../assets/GFunction_sens_RF-scalar_10.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"Here we plot only slices through the three most sensitive dimensions","category":"page"},{"location":"examples/emulators/global_sens_analysis/","page":"Global Sensitiviy Analysis (GSA) test functions","title":"Global Sensitiviy Analysis (GSA) test functions","text":"<img src=\"../../../assets/GFunction_slices_RF-scalar_10.png\" width=\"600\">","category":"page"},{"location":"sample/#The-Sample-stage","page":"Sample","title":"The Sample stage","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"The \"sample\" part of CES refers to exact sampling from the emulated posterior, in our current framework this is achieved with a Markov chain Monte Carlo algorithm (MCMC). Within this paradigm, we want to provide the flexibility to use multiple sampling algorithms; the approach we take is to use the general-purpose AbstractMCMC.jl API, provided by the Turing.jl probabilistic programming framework.","category":"page"},{"location":"sample/#User-interface","page":"Sample","title":"User interface","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"We briefly outline an instance of how one sets up and uses MCMC within the CES package. The user first loads the MCMC module, and provides one of the Protocols (i.e. how one wishes to generate sampling proposals)","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"using CalibrateEmulateSample.MarkovChainMonteCarlo\nprotocol = RWMHSampling() # Random-Walk algorithm\n# protocol = pCNMHSampling() # preconditioned-Crank-Nicholson algorithm","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"Then one builds the MCMC by providing the standard Bayesian ingredients (prior and data) from the calibrate stage, alongside the trained statistical emulator from the emulate stage:","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"mcmc = MCMCWrapper(\n    protocol,\n    truth_sample, \n    prior,\n    emulator;\n    init_params=mean_u_final,\n    burnin=10_000,\n)","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"The keyword arguments init_params give a starting step of the chain (often taken to be the mean of the final iteration of calibrate stage), and a burnin gives a number of initial steps to be discarded when drawing statistics from the sampling method.","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"For good efficiency, one often needs to run MCMC with a problem-dependent step size. We provide a simple utility to help choose this. Here the optimizer runs short chains (of length N), and adjusts the step-size until the MCMC acceptance rate falls within an acceptable range, returning this step size.","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"new_step = optimize_stepsize(\nmcmc;\ninit_stepsize = 1,\nN = 2000\n)","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"To generate 10^5 samples with a given step size (and optional random number generator rng), one calls","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"chain = sample(rng, mcmc, 100_000; stepsize = new_step)\ndisplay(chain) # gives diagnostics","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"The return argument is stored in an MCMCChains.Chains object. To convert this back into a ParameterDistribution type (which contains e.g. the transformation maps) one can call","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"posterior = get_posterior(mcmc, chain)\nconstrained_posterior = transform_unconstrained_to_constrained(prior, get_distribution(posterior))","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"One can quickly plot the marginals of the prior and posterior distribution with","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"using Plots\nplot(prior)\nplot!(posterior)","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"or extract statistics of the (unconstrained) distribution with","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"mean_posterior = mean(posterior)\ncov_posterior = cov(posterior)","category":"page"},{"location":"sample/#AbstractMCMC-sampling-API","page":"Sample","title":"Further details on the implementation","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"This page provides a summary of AbstractMCMC which augments the existing documentation ([1], [2]) and highlights how it's used by the CES package in MarkovChainMonteCarlo. It's not meant to be a complete description of the AbstractMCMC package.","category":"page"},{"location":"sample/#Use-in-MarkovChainMonteCarlo","page":"Sample","title":"Use in MarkovChainMonteCarlo","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"At present, Turing has limited support for derivative-free optimization, so we only use this abstract API and not Turing itself. We also use two related dependencies, AdvancedMH and MCMCChains. ","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"Julia's philosophy is to use small, composable packages rather than monoliths, but this can make it difficult to remember where methods are defined! Below we describe the relevant parts of ","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"The AbstractMCMC API,\nExtended to the case of Metropolis-Hastings (MH) sampling by AdvancedMH,\nFurther extended for the needs of CES in Markov chain Monte Carlo.","category":"page"},{"location":"sample/#Sampler","page":"Sample","title":"Sampler","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"A Sampler is AbstractMCMC's term for an implementation of a MCMC sampling algorithm, along with all its configuration parameters. All samplers are a subtype of AbstractMCMC's AbstractSampler. ","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"Currently CES only implements the Metropolis-Hastings (MH) algorithm. Because it's so straightforward, much of AbstractMCMC isn't needed. We implement two variants of MH with two different Samplers: RWMetropolisHastings and pCNMetropolisHastings, both of which are subtypes of AdvancedMH.MHSampler. The constructor for both Samplers is MetropolisHastingsSampler; the different Samplers are specified by passing a MCMCProtocol object to this constructor.","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"The MHSampler has only one field, proposal, the distribution used to generate new MH proposals via additive stochastic perturbations to the current parameter values. This is done by AdvancedMH.propose(), which gets called for each MCMC step(). The difference between Samplers comes from how the proposal is generated:","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"RWMHSampling does vanilla random-walk proposal generation with a constant, user-specified step size (this differs from the AdvancedMH implementation, which doesn't provide for a step size.)\npCNMHSampling for preconditioned Crank-Nicholson proposals. Vanilla random walk sampling doesn't have a well-defined limit for high-dimensional parameter spaces; pCN replaces the random walk with an Ornstein–Uhlenbeck [AR(1)] process so that the Metropolis acceptance probability remains non-zero in this limit. See Beskos et. al. (2008) and Cotter et. al. (2013).","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"Generated proposals are then either accepted or rejected according to the same MH criterion (in step(), below.)","category":"page"},{"location":"sample/#Models","page":"Sample","title":"Models","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"In Turing, the Model is the distribution one performs inference on, which may involve observed and hidden variables and parameters. For CES, we simply want to sample from the posterior, so our Model distribution is simply the emulated likelihood (see Emulators) together with the prior. This is constructed by EmulatorPosteriorModel.","category":"page"},{"location":"sample/#Sampling-with-the-MCMC-Wrapper-object","page":"Sample","title":"Sampling with the MCMC Wrapper object","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"At a high level, a Sampler and Model is all that's needed to do MCMC sampling. This is done by the sample method provided by AbstractMCMC (extending the method from BaseStats). ","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"To be more user-friendly, in CES we wrap the Sampler, Model and other necessary configuration into a MCMCWrapper object. The constructor for this object ensures that all its components are created consistently, and performs necessary bookkeeping, such as converting coordinates to the decorrelated basis. We extend sample with methods to use this object (that simply unpack its fields and call the appropriate method from AbstractMCMC.)","category":"page"},{"location":"sample/#Chain","page":"Sample","title":"Chain","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"The MCMCChain package provides the Chains container to store the results of the MCMC sampling; the package provides methods to for quick diagnostics and plot utilities of the the Chains objects. For example,","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"using MCMCChains\nusing StatsPlots\n\n# ... from our MCMC example above ...\n# chain = sample(rng, mcmc, 100_000; stepsize = new_step)\n\ndisplay(chain) # diagnostics\nplot(chain) # plots samples over iteration and PDFs for each parameter","category":"page"},{"location":"sample/#Internals:-Transitions","page":"Sample","title":"Internals: Transitions","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"Implementing MCMC involves defining states and transitions of a Markov process (whose stationary distribution is what we seek to sample from). AbstractMCMC's terminology is a bit confusing for the MH case; states of the chain are described by Transition objects, which contain the current sample (and other information like its log-probability). ","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"AdvancedMH defines an AbstractTransition base class for use with its methods; we implement our own child class, MCMCState, in order to record statistics on the MH acceptance ratio.","category":"page"},{"location":"sample/#Internals:-Markov-steps","page":"Sample","title":"Internals: Markov steps","text":"","category":"section"},{"location":"sample/","page":"Sample","title":"Sample","text":"Markov transitions of the chain are defined by overloading AbstractMCMC's step method, which takes the Sampler and current Transition and implements the Sampler's logic to returns an updated Transition representing the chain's new state (actually, a pair of Transitions, for cases where the Sampler doesn't obey detailed balance; this isn't relevant for us). ","category":"page"},{"location":"sample/","page":"Sample","title":"Sample","text":"For example, in Metropolis-Hastings sampling this is where we draw a proposal sample and accept or reject it according to the MH criterion. AdvancedMH implements this here; we re-implement this method because 1) we need to record whether a proposal was accepted or rejected, and 2) our calls to propose() are stepsize-dependent.","category":"page"},{"location":"API/MarkovChainMonteCarlo/#MarkovChainMonteCarlo","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Top-level-class-and-methods","page":"MarkovChainMonteCarlo","title":"Top-level class and methods","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCWrapper\nMCMCWrapper(mcmc_alg::MCMCProtocol, obs_sample::AbstractVector{FT}, prior::ParameterDistribution, em::Emulator;init_params::AbstractVector{FT}, burnin::IT, kwargs...) where {FT<:AbstractFloat, IT<:Integer}\nsample\nget_posterior\noptimize_stepsize","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"struct MCMCWrapper\n\nTop-level class holding all configuration information needed for MCMC sampling: the prior,  emulated likelihood and sampling algorithm (DensityModel and Sampler, respectively, in  AbstractMCMC's terminology).\n\nFields\n\nprior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: ParameterDistribution object describing the prior distribution on parameter values.\nlog_posterior_map::AbstractMCMC.AbstractModel: AdvancedMH.DensityModel object, used to evaluate the posterior density being sampled from.\nmh_proposal_sampler::AbstractMCMC.AbstractSampler: Object describing a MCMC sampling algorithm and its settings.\nsample_kwargs::NamedTuple: NamedTuple of other arguments to be passed to AbstractMCMC.sample().\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper-Union{Tuple{IT}, Tuple{FT}, Tuple{CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol, AbstractVector{FT}, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, CalibrateEmulateSample.Emulators.Emulator}} where {FT<:AbstractFloat, IT<:Integer}","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"MCMCWrapper(\n    mcmc_alg::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol,\n    obs_sample::AbstractArray{FT<:AbstractFloat, 1},\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    emulator::CalibrateEmulateSample.Emulators.Emulator;\n    init_params,\n    burnin,\n    kwargs...\n)\n\n\nConstructor for MCMCWrapper which performs the same standardization (SVD  decorrelation) that was applied in the Emulator. It creates and wraps an instance of  EmulatorPosteriorModel, for sampling from the Emulator, and  MetropolisHastingsSampler, for generating the MC proposals.\n\nmcmc_alg: MCMCProtocol describing the MCMC sampling algorithm to use. Currently implemented algorithms are:\nRWMHSampling: Metropolis-Hastings sampling from a vanilla random walk with fixed stepsize.\npCNMHSampling: Metropolis-Hastings sampling using the preconditioned  Crank-Nicholson algorithm, which has a well-behaved small-stepsize limit.\nBarkerSampling: Metropolis-Hastings sampling using the Barker proposal, which has a robustness to choosing step-size parameters.\nobs_sample: A single sample from the observations. Can, e.g., be picked from an  Observation struct using get_obs_sample.\nprior: ParameterDistribution  object containing the parameters' prior distributions.\nemulator: Emulator to sample from.\nstepsize: MCMC step size, applied as a scaling to the prior covariance.\ninit_params: Starting parameter values for MCMC sampling.\nburnin: Initial number of MCMC steps to discard from output (pre-convergence).\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#StatsBase.sample","page":"MarkovChainMonteCarlo","title":"StatsBase.sample","text":"sample([rng,] mcmc::MCMCWrapper, args...; kwargs...)\n\nExtends the sample methods of AbstractMCMC (which extends StatsBase) to sample from the  emulated posterior, using the MCMC sampling algorithm and Emulator configured in  MCMCWrapper. Returns a MCMCChains.Chains  object containing the samples. \n\nSupported methods are:\n\nsample([rng, ]mcmc, N; kwargs...)\nReturn a MCMCChains.Chains object containing N samples from the emulated posterior.\nsample([rng, ]mcmc, isdone; kwargs...)\nSample from the model with the Markov chain Monte Carlo sampler until a convergence  criterion isdone returns true, and return the samples. The function isdone has the  signature\n    isdone(rng, model, sampler, samples, state, iteration; kwargs...)\nwhere state and iteration are the current state and iteration of the sampler,  respectively. It should return true when sampling should end, and false otherwise.\nsample([rng, ]mcmc, parallel_type, N, nchains; kwargs...)\nSample nchains Monte Carlo Markov chains in parallel according to parallel_type, which may be MCMCThreads() or MCMCDistributed() for thread and parallel sampling,  respectively.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","text":"get_posterior(\n    mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper,\n    chain::MCMCChains.Chains\n) -> EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n\n\nReturns a ParameterDistribution object corresponding to the empirical distribution of the  samples in chain.\n\nnote: Note\nThis method does not currently support combining samples from multiple Chains.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","text":"optimize_stepsize(\n    rng::Random.AbstractRNG,\n    mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper;\n    init_stepsize,\n    N,\n    max_iter,\n    target_acc,\n    sample_kwargs...\n) -> Float64\n\n\nUses a heuristic to return a stepsize for the mh_proposal_sampler element of  MCMCWrapper which yields fast convergence of the Markov chain.\n\nThe criterion used is that Metropolis-Hastings proposals should be accepted between 15% and  35% of the time.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"See AbstractMCMC sampling API for background on our use of Turing.jl's  AbstractMCMC API for  MCMC sampling.","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Sampler-algorithms","page":"MarkovChainMonteCarlo","title":"Sampler algorithms","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCProtocol\nAutodiffProtocol\nForwardDiffProtocol\nReverseDiffProtocol\nRWMHSampling\npCNMHSampling\nBarkerSampling\nMetropolisHastingsSampler","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","text":"abstract type MCMCProtocol\n\nType used to dispatch different methods of the MetropolisHastingsSampler  constructor, corresponding to different sampling algorithms.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol","text":"abstract type AutodiffProtocol\n\nType used to dispatch different autodifferentiation methods where different emulators have a different compatability with autodiff packages \n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.ForwardDiffProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.ForwardDiffProtocol","text":"abstract type ForwardDiffProtocol <: CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol\n\nType to construct samplers for emulators compatible with ForwardDiff.jl autodifferentiation\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.ReverseDiffProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.ReverseDiffProtocol","text":"abstract type ReverseDiffProtocol <: CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol\n\nType to construct samplers for emulators compatible with ReverseDiff.jl autodifferentiation\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","text":"struct RWMHSampling{T<:CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol} <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters as as vanilla random walk, based on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","text":"struct pCNMHSampling{T<:CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol} <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters according to the preconditioned Crank-Nicholson (pCN) algorithm, which is  usable for MCMC in the stepsize → 0 limit, unlike the vanilla random walk. Steps are based  on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.BarkerSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.BarkerSampling","text":"struct BarkerSampling{T<:CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol} <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for new parameters according to the Barker proposal.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","text":"MetropolisHastingsSampler(\n    _::CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling{T<:CalibrateEmulateSample.MarkovChainMonteCarlo.AutodiffProtocol},\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n) -> CalibrateEmulateSample.MarkovChainMonteCarlo.RWMetropolisHastings{AdvancedMH.RandomWalkProposal{false, _A}} where _A\n\n\nConstructor for all Sampler objects, with one method for each supported MCMC algorithm.\n\nwarning: Warning\nBoth currently implemented Samplers use a Gaussian approximation to the prior:  specifically, new Metropolis-Hastings proposals are a scaled combination of the old  parameter values and a random shift distributed as a Gaussian with the same covariance as the prior. This suffices for our current use case, in which our priors are Gaussian (possibly in a transformed space) and we assume enough fidelity in the Emulator that  inference isn't prior-dominated.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Emulated-posterior-(Model)","page":"MarkovChainMonteCarlo","title":"Emulated posterior (Model)","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"EmulatorPosteriorModel","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","text":"EmulatorPosteriorModel(\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    obs_sample::AbstractArray{FT<:AbstractFloat, 1}\n) -> AdvancedMH.DensityModel{F} where F<:(CalibrateEmulateSample.MarkovChainMonteCarlo.var\"#9#10\"{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution{PDType, CType, ST}, CalibrateEmulateSample.Emulators.Emulator{FT}, <:AbstractVector{FT1}} where {PDType<:EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType, CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType, ST<:AbstractString, FT<:AbstractFloat, FT1<:AbstractFloat})\n\n\nFactory which constructs AdvancedMH.DensityModel objects given a prior on the model  parameters (prior) and an Emulator of the log-likelihood of the data given  parameters. Together these yield the log posterior density we're attempting to sample from  with the MCMC, which is the role of the DensityModel class in the AbstractMCMC interface.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-MCMC-State","page":"MarkovChainMonteCarlo","title":"Internals - MCMC State","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCState\naccept_ratio","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","text":"struct MCMCState{T, L<:Real} <: AdvancedMH.AbstractTransition\n\nExtends the AdvancedMH.Transition (which encodes the current state of the MC during sampling) with a boolean flag to record whether this state is new (arising from accepting a Metropolis-Hastings proposal) or old (from rejecting a proposal).\n\nFields\n\nparams::Any: Sampled value of the parameters at the current state of the MCMC chain.\nlog_density::Real: Log probability of params, as computed by the model using the prior.\naccepted::Bool: Whether this state resulted from accepting a new MH proposal.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","text":"accept_ratio(chain::MCMCChains.Chains) -> Any\n\n\nFraction of MC proposals in chain which were accepted (according to Metropolis-Hastings.)\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-Other","page":"MarkovChainMonteCarlo","title":"Internals - Other","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"to_decorrelated","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","text":"to_decorrelated(\n    data::AbstractArray{FT<:AbstractFloat, 2},\n    em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}\n) -> Any\n\n\nTransform samples from the original (correlated) coordinate system to the SVD-decorrelated coordinate system used by Emulator. Used in the constructor for MCMCWrapper.\n\n\n\n\n\n","category":"function"},{"location":"random_feature_emulator/#Random-Feature-Emulator","page":"Random Features","title":"Random Feature Emulator","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Have a go with Gaussian processes first\nWe recommend that users first try GaussianProcess for their problems. As random features are a more recent tool, the training procedures and interfaces are still experimental and in development. ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Random features provide a flexible framework to approximates a Gaussian process. Using random sampling of features, the method is a low-rank approximation leading to advantageous scaling properties (with the number of training points, input, and output dimensions). In the infinite sample limit, there are often (known) explicit Gaussian process kernels that the random feature representation converges to.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We provide two types of MachineLearningTool for random feature emulation, the ScalarRandomFeatureInterface and the VectorRandomFeatureInterface.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The ScalarRandomFeatureInterface closely mimics the role of a GaussianProcess package, by training a scalar-output function distribution. It can be applied to multidimensional output problems (as with GaussianProcess) by relying on data processing tools, such as performed when the decorrelate=true keyword argument is provided to the Emulator.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The VectorRandomFeatureInterface, when applied to multidimensional problems, directly trains a function distribution between multi-dimensional spaces. This approach is not restricted to the data processing of the scalar method (though this can still be helpful). It can be cheaper to evaluate, but on the other hand the training can be more challenging/computationally expensive.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Building a random feature interface is similar to building a Gaussian process: one defines a kernel to encode similarities between outputs (y_iy_j) based on inputs (x_ix_j). Additionally, one must specify the number of random feature samples to be taken to build the emulator.","category":"page"},{"location":"random_feature_emulator/#Recommended-configuration","page":"Random Features","title":"Recommended configuration","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Below is listed a recommended configuration that is flexible and requires learning relatively few parameters. Users can increase r to balance flexibility against having more kernel hyperparameters to learn.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\n# given input_dim, output_dim, and a PairedDataContainer\n\n# define number of features for prediction\nn_features = 400 # number of features for prediction \n\n# define kernel \nnugget = 1e8*eps() # small nugget term\nr = 1 # start with smallest rank \nlr_perturbation = LowRankFactor(r, nugget) \nnonsep_lrp_kernel = NonseparableKernel(lr_perturbation) \n\n# configure optimizer\noptimizer_options = Dict(\n    \"verbose\" => true, # print diagnostics for optimizer\n    \"n_features_opt\" => 100, # use less features during hyperparameter optimization/kernel learning\n    \"cov_sample_multiplier\" => 1.0, # use to reduce/increase number of samples in initial cov estimation stage\n)\n\nmachine_learning_tool = VectorRandomFeatureInterface(\n    n_features,\n    input_dim,\n    output_dim,\n    kernel_structure = nonsep_lrp_kernel,\n    optimizer_options = optimizer_options\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Users can change the kernel complexity with r, and the number of features for prediciton with n_features and optimization with n_features_opt. ","category":"page"},{"location":"random_feature_emulator/#User-Interface","page":"Random Features","title":"User Interface","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"CalibrateEmulateSample.jl allows the random feature emulator to be built using the external package RandomFeatures.jl. In the notation of this package's documentation, our interface allows for families of RandomFourierFeature objects to be constructed with different Gaussian distributions of the \"xi\" a.k.a weight distribution, and with a learnable \"sigma\", a.k.a scaling parameter.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Relating features and kernels\nThe parallels of random features and gaussian processes can be quite strong. For example:The restriction to RandomFourierFeature objects is a restriction to the approximation of shift-invariant kernels (i.e. K(xy) = K(x-y))\nThe restriction of the weight (\"xi\") distribution to Gaussians is a restriction of approximating squared-exponential kernels. Other distributions (e.g. student-t) leads to other kernels (e.g. Matern)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The interfaces are defined minimally with","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"srfi = ScalarRandomFeatureInterface(n_features, input_dim; ...)\nvrfi = VectorRandomFeatureInterface(n_features, input_dim, output_dim; ...)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"This will build an interface around a random feature family based on n_features features and mapping between spaces of dimenstion input_dim to 1 (scalar), or output_dim (vector).","category":"page"},{"location":"random_feature_emulator/#The-kernel_structure-keyword-for-flexibility","page":"Random Features","title":"The kernel_structure keyword - for flexibility","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To adjust the expressivity of the random feature family one can define the keyword argument kernel_structure. The more expressive the kernel, the more hyperparameters are learnt in the optimization.  ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We have two types,","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"separable_kernel = SeparableKernel(input_cov_structure, output_cov_structure)\nnonseparable_kernel = NonseparableKernel(cov_structure)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"where the cov_structure implies some imposed user structure on the covariance structure. The basic covariance structures are given by ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"oned_cov_structure = OneDimFactor() # the problem dimension is 1\ndiagonal_structure = DiagonalFactor() # impose diagonal structure (e.g. ARD kernel)\ncholesky_structure = CholeskyFactor() # general positive definite matrix\nlr_perturbation = LowRankFactor(r) # assume structure is a rank-r perturbation from identity","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"All covariance structures (except OneDimFactor) have their final positional argument being a \"nugget\" term adding +epsilon I to the covariance structure. Set to 1 by default.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The current default kernels are as follows:","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"scalar_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(input_dim)))), OneDimFactor())\nvector_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(output_dim)))), LowRankFactor(Int(ceil(sqrt(output_dim)))))","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Relating covariance structure and training\nThe parallels between random feature and Gaussian process also extends to the hyperparameter learning. For example,A ScalarRandomFeatureInterface with a DiagonalFactor input covariance structure approximates a Gaussian process with automatic relevance determination (ARD) kernel, where one learns a lengthscale in each dimension of the input space","category":"page"},{"location":"random_feature_emulator/#The-optimizer_options-keyword-for-performance","page":"Random Features","title":"The optimizer_options keyword - for performance","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Passed as a dictionary, this keyword allows the user to configure many options from their defaults in the hyperparameter optimization. The optimizer itself relies on the EnsembleKalmanProcesses package.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We recommend users experiment with a subset of these flags. At first enable","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\"verbose\" => true)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If the covariance sampling takes too long, run with multithreading (e.g. julia --project -t n_threads script.jl). Sampling is embarassingly parallel so this acheives near linear scaling,","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If sampling still takes too long, try setting","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\n    \"cov_sample_multiplier\" => csm,\n    \"train_fraction\" => tf,\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Decreasing csm (default 10.0) towards 0.0 directly reduces the number of samples to estimate a covariance matrix in the optimizer, by using a shrinkage estimator to improve matrix conditioning. Guide: more samples implies less shrinkage for good conditioning and less approximation error. The amount of shrinkage is returned to user as a value between 0 (no shrinkage) and 1 (shrink to diagonal matrix), it is suggested that users choose csm to keep the shrinkage amount below 0.2.\nIncreasing tf towards 1 changes the train-validate split, reducing samples but increasing cost-per-sample and reducing the available validation data (default 0.8, suggested range (0.5,0.95)).","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If optimizer convergence stagnates or is too slow, or if it terminates before producing good results, try:","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\n    \"n_ensemble\" => n_e, \n    \"n_iteration\" => n_i,\n    \"localization\" => loc,\n    \"scheduler\" => sch,\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We suggest looking at the EnsembleKalmanProcesses documentation for more details; but to summarize","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Reducing optimizer samples n_e and iterations n_i reduces computation time but may limit convergence progress, see here.\nIf n_e becomes less than the number of hyperparameters, the updates may fail and a localizer must be specified in loc, see here. \nIf the algorithm terminates at T=1 and resulting emulators looks unacceptable one can change or add arguments in sch e.g. DataMisfitController(\"on_terminate\"=continue), see here","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Note\nWidely robust defaults here are a work in progress","category":"page"},{"location":"random_feature_emulator/#Key-methods","page":"Random Features","title":"Key methods","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To interact with the kernel/covariance structures we have standard get_* methods along with some useful functions","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"cov_structure_from_string(string,dim) creates a basic covariance structure from a predefined string: onedim, diagonal, cholesky, lowrank etc. and a dimension\ncalculate_n_hyperparameters(in_dim, out_dim, kernel_structure) calculates the number of hyperparameters created by using the given kernel structure (can be applied to the covariance structure individually too)\nbuild_default_priors(in_dim, out_dim, kernel_structure) creates a ParameterDistribution for the hyperparameters based on the kernel structure. This serves as the initialization of the training procedure.","category":"page"},{"location":"random_feature_emulator/#Example-families-and-their-hyperparameters","page":"Random Features","title":"Example families and their hyperparameters","text":"","category":"section"},{"location":"random_feature_emulator/#Scalar:-\\mathbb{R}5-\\to-\\mathbb{R}-at-defaults","page":"Random Features","title":"Scalar: mathbbR^5 to mathbbR at defaults","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 5\n# build the default scalar kernel directly (here it will be a rank-3 perturbation from the identity)\nscalar_default_kernel = SeparableKernel(\n    cov_structure_from_string(\"lowrank\", input_dim),\n    cov_structure_from_string(\"onedim\", 1)\n) \n\ncalculate_n_hyperparameters(input_dim, scalar_default_kernel) \n# answer = 19, 18 for the covariance structure, and one scaling parameter\n\nbuild_default_prior(input_dim, scalar_default_kernel)\n# builds a 3-entry distribution\n# 3-dim positive distribution 'input_lowrank_diagonal'\n# 15-dim unbounded distribution 'input_lowrank_U'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/#Vector,-separable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-at-defaults","page":"Random Features","title":"Vector, separable: mathbbR^25 to mathbbR^50 at defaults","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Or take a diagonalized 8-dimensional input, and assume full 6-dimensional output","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\n# build the default vector kernel directly (here it will be a rank-5 input and rank-8 output)\nvector_default_kernel = SeparableKernel(\n    cov_structure_from_string(\"lowrank\", input_dim),\n    cov_structure_from_string(\"lowrank\", output_dim)\n)\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_default_kernel) \n# answer = 539; 130 for input, 408 for the output, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_default_kernel)\n# builds a 5-entry distribution\n# 5-dim positive distribution 'input_lowrank_diagonal'\n# 125-dim unbounded distribution 'input_lowrank_U'\n# 8-dim positive distribution 'output_lowrank_diagonal'\n# 400-dim unbounded distribution 'output_lowrank_U'\n# 1-dim postive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/#Vector,-nonseparable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}","page":"Random Features","title":"Vector, nonseparable: mathbbR^25 to mathbbR^50","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The following represents the most general kernel case.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Note\nUse low-rank/diagonls representations where possible to control the number of hyperparameters.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\neps = 1e-8\nrank=5\n# build a full-rank nonseparable vector kernel\nvector_lowrank_kernel = NonseparableKernel(LowRankFactor(rank, eps))\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_lowrank_kernel)\n# answer = 6256; 6255 for the joint input-output space, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_lowrank_kernel)\n# builds a 2-entry distribution\n# 5-dim positive distribution 'full_lowrank_diagonal'\n# 6250-dim unbounded distribution 'full_lowrank_U'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"warning: Warning\nNaive representations lead to very large numbers of hyperparameters.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\neps = 1e-8\n# build a full-rank nonseparable vector kernel\nvector_general_kernel = NonseparableKernel(CholeskyFactor(eps))\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_general_kernel)\n# answer = 781876; 781875 for the joint input-output space, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_general_kernel)\n# builds a 2-entry distribution\n# 781875-dim unbounded distribution 'full_cholesky'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"See the API for more details.","category":"page"},{"location":"calibrate/#The-Calibrate-stage","page":"Calibrate","title":"The Calibrate stage","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration of the computer model entails finding an optimal parameter theta^* that maximizes the posterior probability","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"rho(thetavert y Gamma_y) = dfrace^-mathcalL(theta y)Z(yvertGamma_y)rho_mathrmprior(theta) qquad mathcalL(theta y) = langle mathcalG(theta) - y    Gamma_y^-1 left ( mathcalG(theta) - y right ) rangle","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"where mathcalL is the loss or negative log-likelihood, Z(yvertGamma) is a normalizing constant, y represents the data, Gamma_y is the noise covariance matrix and rho_mathrmprior(theta) is the prior density. Calibration is performed using ensemble Kalman processes, which generate input-output pairs theta mathcalG(theta) in high density from the prior initial guess to the found optimal parameter theta^*. These input-output pairs are then used as the data to train an emulator of the forward model mathcalG.","category":"page"},{"location":"calibrate/#Ensemble-Kalman-Processes","page":"Calibrate","title":"Ensemble Kalman Processes","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration can be performed using different ensemble Kalman processes: ensemble Kalman inversion (Iglesias et al, 2013), ensemble Kalman sampler (Garbuno-Inigo et al, 2020), and unscented Kalman inversion (Huang et al, 2022). All algorithms are implemented in EnsembleKalmanProcesses.jl. Documentation of each algorithm is available in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"calibrate/#Typical-construction-of-the-EnsembleKalmanProcess","page":"Calibrate","title":"Typical construction of the EnsembleKalmanProcess","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Documentation on how to construct an EnsembleKalmanProcess from the computer model and the data can be found in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"calibrate/#Julia-free-forward-model","page":"Calibrate","title":"Julia-free forward model","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"One draw of our approach is that it does not require the forward map to be written in Julia. To aid construction of such a workflow, EnsembleKalmanProcesses.jl provides a documented example of a BASH workflow for the sinusoid problem, with source code here. The forward map interacts with the calibration tools (EKP) only though TOML file reading an writing, and thus can be written in any language; for example, to be used with slurm HPC scripts, with source code here.","category":"page"},{"location":"examples/Cloudy_example/#Cloudy-example","page":"Cloudy example","title":"Learning the initial parameters of a droplet mass distribution in Cloudy","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"warn: version control for Cloudy\nDue to rapid developments in Cloudy, this example will not work with the latest version. It is known to work pinned to specific commit b4fa7e3, please add Cloudy to the example Project using command add Cloudy#b4fa7e3 in Pkg to avoid errors.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"This example is based on Cloudy, a microphysics model that simulates how cloud droplets collide and coalesce into larger drops. Collision-coalescence is a crucial process for the formation of rain. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"Cloudy is initialized with a mass distribution of the cloud droplets; this distribution is then stepped forward in time, with more and more droplets colliding and combining into bigger drops according to the droplet-droplet interactions specified by a collision-coalescence kernel. The evolution of the droplet distribution is completely determined by the shape of the initial distribution and the form of the kernel.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We will show how Calibrate-Emulate-Sample (CES) can be used to learn the parameters of the initial cloud droplet mass distribution from observations of the moments of that mass distribution at a later time. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"Cloudy is used here in a \"perfect model\" (aka \"known truth\") setting, which means that the \"observations\" are generated by Cloudy itself, by running it with the true parameter values–-in more realistic applications, one would use actual measurements of cloud properties.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The following schematic gives an overview of the example:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: cloudy_schematic)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The input to the CES algorithm consists of data y, the observational covariance Γ, and prior parameter distributions. The data, a vector of moments of the droplet mass distribution, are obtained by running Cloudy with the parameters set to their true values. The covariance is obtained from model output. The calibration stage is performed by an ensemble Kalman inversion (EKI), in which Cloudy has to be run once per iteration and for each ensemble member. The resulting input-output pairs theta_i mathcalG(theta_i)_i are used to train an emulator model. This emulator widetildemathcalG(theta) is cheap to evaluate; it replaces the original parameter-to-data map in the Markov chain Monte Carlo (MCMC) sampling, which produces (approximate) samples of the posterior parameter distribution. These samples are the final output of the CES algorithm. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"This paper describes Cloudy in much more detail and shows results of experiments using CES to learn model parameters.","category":"page"},{"location":"examples/Cloudy_example/#Running-the-example","page":"Cloudy example","title":"Running the example","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"Cloudy_calibrate.jl performs the calibration using ensemble Kalman inversion; Cloudy_emulate_sample.jl fits an emulator and uses it to sample the posterior distributions of the parameters. Once Cloudy is installed, the example can be run from the julia REPL:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"include(\"Cloudy_calibrate.jl\")\ninclude(\"Cloudy_emulate_sample.jl\")","category":"page"},{"location":"examples/Cloudy_example/#Walkthrough-of-the-code:-Cloudy_calibrate.jl","page":"Cloudy example","title":"Walkthrough of the code: Cloudy_calibrate.jl","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"This file performs the calibration stage of CES.","category":"page"},{"location":"examples/Cloudy_example/#Import-packagages-and-modules","page":"Cloudy example","title":"Import packagages and modules","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"First we load standard packages,","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"using Distributions\nusing StatsBase\nusing LinearAlgebra\nusing StatsPlots\nusing Plots\nusing Plots.PlotMeasures\nusing Random\nusing JLD2","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"the Cloudy modules,","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"using Cloudy\nusing Cloudy.ParticleDistributions\nusing Cloudy.KernelTensors\n\n# Import the module that runs Cloudy\ninclude(joinpath(@__DIR__, \"DynamicalModel.jl\"))","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"and finally the EKP packages.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nusing EnsembleKalmanProcesses.DataContainers\nusing EnsembleKalmanProcesses.PlotRecipes","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The module DynamicalModel.jl is the forward solver; it provides a function that runs Cloudy with a given instance of the parameter vector we want to learn.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"include(\"DynamicalModel.jl\")","category":"page"},{"location":"examples/Cloudy_example/#Define-the-true-parameters","page":"Cloudy example","title":"Define the true parameters","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We define the true parameters–-they are known here because this is a known truth example. Knowing the true parameters will allow us to assess how well Calibrate-Emulate-Sample has managed to solve the inverse problem.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We will assume that the true particle mass distribution is a Gamma distribution, which at time t = 0 has parameters phi_0 = N_0 0 k_0 theta_0. We will then try to learn these parameters from observations y = M_0(t_end) M_1(t_end) M_2(t_end) of the zeroth-, first-, and second-order moments of the distribution at time t_end  0 (where t_end = 1.0 in this example). The true parameters phi_0 texttrue are defined as follows:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"param_names = [\"N0\", \"θ\", \"k\"]\nn_params = length(param_names)\nN0_true = 300.0  # number of particles (scaling factor for Gamma distribution)\nθ_true = 1.5597  # scale parameter of Gamma distribution\nk_true = 0.0817  # shape parameter of Gamma distribution\nϕ_true = [N0_true, θ_true, k_true] # true parameters in constrained space\ndist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)","category":"page"},{"location":"examples/Cloudy_example/#Define-priors-for-the-parameters","page":"Cloudy example","title":"Define priors for the parameters","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"As we are working with Bayesian methods, we treat the parameters we want to learn as random variables whose prior distributions we specify here. The prior distributions will behave like an \"initial guess\" for the likely region of parameter space where we expect the solution to be located. We use constrained_gaussian to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity (and numerical stability). ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# We choose to use normal distributions to represent the prior distributions of\n# the parameters in the transformed (unconstrained) space.\nprior_N0 = constrained_gaussian(param_names[1], 400, 300, 0.4 * N0_true, Inf)\nprior_θ = constrained_gaussian(param_names[2], 1.0, 5.0, 1e-1, Inf)\nprior_k = constrained_gaussian(param_names[3], 0.2, 1.0, 1e-4, Inf)\npriors = combine_distributions([prior_N0, prior_θ, prior_k])","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The plot recipe for ParameterDistribution types allows for quick visualization of the priors:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# Plot the priors\np = plot(priors, constrained=false)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: priors)","category":"page"},{"location":"examples/Cloudy_example/#Generate-(synthetic)-observations","page":"Cloudy example","title":"Generate (synthetic) observations","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We generate synthetic observations by running Cloudy 100 times with the true parameters (i.e., with the true initial Gamma distribution of droplet masses) and then adding noise to simulate measurement error.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"dyn_model_settings_true = DynamicalModel.ModelSettings(\n    kernel, dist_true, moments, tspan)\n\nG_t = DynamicalModel.run_dyn_model(ϕ_true, dyn_model_settings_true)\nn_samples = 100\ny_t = zeros(length(G_t), n_samples)\n# In a perfect model setting, the \"observational noise\" represents the \n# internal model variability. Since Cloudy is a purely deterministic model, \n# there is no straightforward way of coming up with a covariance structure \n# for this internal model variability. We decide to use a diagonal \n# covariance, with entries (variances) largely proportional to their \n# corresponding data values, G_t\nΓy = convert(Array, Diagonal([100.0, 5.0, 30.0]))\nμ = zeros(length(G_t))\n\n# Add noise\nfor i in 1:n_samples\n    y_t[:, i] = G_t .+ rand(MvNormal(μ, Γy))\nend\n\ntruth = Observation(\n    Dict(\n        \"samples\" => vec(mean(y_t, dims = 2)),\n        \"covariances\" => Γy,\n        \"names\" => data_names,\n    )\n)","category":"page"},{"location":"examples/Cloudy_example/#Perform-ensemble-Kalman-inversion","page":"Cloudy example","title":"Perform ensemble Kalman inversion","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We sample the initial ensemble from the prior and create the EnsembleKalmanProcess object as an ensemble Kalman inversion (EKI) algorithm using the Inversion() keyword. We also use the DataMisfitController() learning rate scheduler. The number of ensemble members must be larger than the dimension of the parameter space to ensure a full rank ensemble covariance.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"N_ens = 50 # number of ensemble members\nN_iter = 8 # number of EKI iterations\n# initial parameters: n_params x N_ens\ninitial_params = construct_initial_ensemble(rng, priors, N_ens)\nekiobj = EnsembleKalmanProcess(\n    initial_params,\n    truth,\n    Inversion(),\n    scheduler=DataMisfitController()\n)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We perform the inversion loop. Remember that within calls to get_ϕ_final the EKP transformations are applied, thus the ensemble that is returned will be the gamma distribution parameters that can be used directly to run the forward model, rather than their corresponding values in the unconstrained space where the EKI takes place. Each ensemble member is stored as a column and therefore for uses such as plotting one needs to reshape to the desired dimension.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# Initialize a ParticleDistribution with dummy parameters. The parameters \n# will then be set within `run_dyn_model`\ndummy = ones(n_params)\ndist_type = ParticleDistributions.GammaPrimitiveParticleDistribution(dummy...)\nmodel_settings = DynamicalModel.ModelSettings(kernel, dist_type, moments, tspan)\n# EKI iterations\nfor n in 1:N_iter\n    # Return transformed parameters in physical/constrained space\n    ϕ_n = get_ϕ_final(priors, ekiobj)\n    # Evaluate forward map\n    G_n = [DynamicalModel.run_dyn_model(ϕ_n[:, i], model_settings) for i in 1:N_ens]\n    G_ens = hcat(G_n...)  # reformat\n    EnsembleKalmanProcesses.update_ensemble!(ekiobj, G_ens)\nend","category":"page"},{"location":"examples/Cloudy_example/#Visualize-and-store-the-results-of-the-calibration","page":"Cloudy example","title":"Visualize and store the results of the calibration","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration, which is printed to standard output. An output directory is created, where a file cloudy_calibrate_results.jld2 is stored, which contains all parameters and model output from the ensemble Kalman iterations (both as DataContainers.DataContainer objects), the mean and one sample of the synthetic observations, as well as the true prameters and their priors. In addition, an animation is produced that shows the evolution of the ensemble of particles over subsequent iterations of the optimization, both in the computational (unconstrained) and physical (constrained) spaces.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: eki_iterations_animation)","category":"page"},{"location":"examples/Cloudy_example/#Walkthrough-of-the-code:-Cloudy_emulate_sample.jl","page":"Cloudy example","title":"Walkthrough of the code: Cloudy_emulate_sample.jl","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"This file performs the emulation and sampling stages of the CES algorithm.","category":"page"},{"location":"examples/Cloudy_example/#Import-packages-and-modules","page":"Cloudy example","title":"Import packages and modules","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"First, we import some standard packages","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"using Distributions\nusing StatsBase\nusing GaussianProcesses\nusing LinearAlgebra\nusing Random\nusing JLD2\nENV[\"GKSwstype\"] = \"100\"\nusing CairoMakie, PairPlots","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"as well as the relevant CES packages needed to construct the emulators and perform the Markov chain Monte Carlo (MCMC) sampling. We also need some functionality of EnsembleKalmanProcesses.jl.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"using CalibrateEmulateSample.Emulators\nusing CalibrateEmulateSample.MarkovChainMonteCarlo\nusing CalibrateEmulateSample.Utilities\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nusing EnsembleKalmanProcesses.DataContainers","category":"page"},{"location":"examples/Cloudy_example/#Load-the-calibration-results","page":"Cloudy example","title":"Load the calibration results","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We will train the emulator on the input-output pairs we obtained during the calibration. They are stored within the EnsembleKalmanProcess object (ekiobj), which is loaded here together with the other information that was saved in the calibration step.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"ekiobj = load(data_save_file)[\"eki\"]\npriors = load(data_save_file)[\"priors\"]\ntruth_sample_mean = load(data_save_file)[\"truth_sample_mean\"]\ntruth_sample = load(data_save_file)[\"truth_sample\"]\n# True parameters:\n# - ϕ: in constrained space\n# - θ: in unconstrained space\nϕ_true = load(data_save_file)[\"truth_input_constrained\"]\nθ_true = transform_constrained_to_unconstrained(priors, ϕ_true)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The user can choose one or more of the following three emulators: a Gaussian Process (GP) emulator with GaussianProcesses.jl interface (gp-gpjl), a scalar Random Feature (RF) interface (rf-scalar), and vector RF with a nonseparable, nondiagonal kernel structure in the output space (rf-nosvd-nonsep). See  here for a complete overview of the available emulators. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"cases = [\n    \"rf-scalar\",\n    \"gp-gpjl\",  # Veeeery slow predictions\n    \"rf-nosvd-nonsep\"\n]","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We first define some settings for the two emulators, e.g., the prediction type for the GP emulator, or the number of features and hyperparameter optimizer options for the RF emulator. The docs for GPs and RFs explain the different options in more detail and provide some useful heuristics for how to customize the settings depending on the problem at hand.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# These settings are the same for all Gaussian Process cases\npred_type = YType() # we want to predict data\n\n# These settings are the same for all Random Feature cases\nn_features = 400 \nnugget = 1e-8\noptimizer_options = Dict(\n    \"verbose\" => true,\n    \"scheduler\" => DataMisfitController(terminate_at = 100.0),\n    \"cov_sample_multiplier\" => 1.0,\n    \"n_iteration\" => 20,\n)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"Emulation is performed through the construction of an Emulator object from the following components:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"a wrapper for the machine learning tool (mlt) to be used as emulator\nthe input-output pairs on which the emulator will be trained\noptional arguments specifying data processing and dimensionality reduction functionality ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"For gp-gpjl, this looks as follows:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"gppackage = GPJL()\n# Kernel is the sum of a squared exponential (SE), Matérn 5/2, and white noise\ngp_kernel = SE(1.0, 1.0) + Mat52Ard(zeros(3), 0.0) + Noise(log(2.0))\n\n# Wrapper for GP\nmlt = GaussianProcess(\n    gppackage;\n    kernel = gp_kernel,\n    prediction_type = pred_type,\n    noise_learn = false,\n)\n\ndecorrelate = true\nstandardize_outputs = true","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"And similarly for rf-scalar","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"kernel_rank = 3\nkernel_structure = SeparableKernel(\n    LowRankFactor(kernel_rank, nugget),\n    OneDimFactor()\n)\n\nmlt = ScalarRandomFeatureInterface(\n    n_features,\n    n_params,\n    kernel_structure = kernel_structure,\n    optimizer_options = optimizer_options,\n)\n\ndecorrelate = true\nstandardize_outputs = true","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"and for rf-nosvd-nonsep:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"kernel_rank = 4\nmlt = VectorRandomFeatureInterface(\n    n_features,\n    n_params,\n    n_outputs,\n    kernel_structure = NonseparableKernel(LowRankFactor(kernel_rank, nugget)),\n    optimizer_options = optimizer_options\n)\n\n# Vector RF does not require decorrelation of outputs\ndecorrelate = false\nstandardize_outputs = false","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We construct the emulator using the input-output pairs obtained in the calibration stage (note that we're not using all available input-output pairs–-using all of them may not give the best results, especially if the EKI parameter converges rapidly and then \"stays in the same place\" during the remaining iterations). For the gp-gpjl and rf-scalar cases, we want the output data to be decorrelated with information from Γy, but for the vector RF case decorrelation is not required.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"input_output_pairs = get_training_points(ekiobj,\n                                          length(get_u(ekiobj))-2)\n\n# Use the medians of the outputs as standardizing factors\nnorm_factors = get_standardizing_factors(\n    get_outputs(input_output_pairs)\n)\n\n# The data processing normalizes input data, and decorrelates\n# output data with information from Γy, if required\n# Note: The `standardize_outputs_factors` are only used under the\n# condition that `standardize_outputs` is true.\nemulator = Emulator(\n    mlt,\n    input_output_pairs,\n    decorrelate = decorrelate,\n    obs_noise_cov = Γy,\n    standardize_outputs = true,\n    standardize_outputs_factors = vcat(norm_factors...),\n)","category":"page"},{"location":"examples/Cloudy_example/#Train-the-emulator","page":"Cloudy example","title":"Train the emulator","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The emulator is trained when we combine the machine learning tool and the data into the Emulator above. We must also optimize the hyperparameters:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"To test the predictive skill of the emulator, we can e.g. compare its prediction on the true parameters to the true data. (One could also hold out a subset of the input-output pairs from the training and evaluate the emulator's predictions on them).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# Check how well the emulator predicts on the true parameters\ny_mean, y_var = Emulators.predict(\n    emulator,\n    reshape(θ_true, :, 1);\n    transform_to_real = true\n)\n\nprintln(\"Emulator ($(case)) prediction on true parameters: \")\nprintln(vec(y_mean))\nprintln(\"true data: \")\nprintln(truth_sample) # what was used as truth","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The emulator predicts both a mean value and a covariance.","category":"page"},{"location":"examples/Cloudy_example/#Sample-the-posterior-distributions-of-the-parameters","page":"Cloudy example","title":"Sample the posterior distributions of the parameters","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The last step is to plug the emulator into an MCMC algorithm, which is then used to produce samples from the posterior distribution of the parameters. Essentially, the emulator acts as a stand-in for the original forward model (which in most cases of interest is computationally expensive to run) during the MCMC sampling process.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We use the mean across all parameter ensembles from the EKI as the initial parameters for the MCMC. Before running the actual MCMC chain, we determine a good step size by running chains of length N = 2000:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# initial values\nu0 = vec(mean(get_inputs(input_output_pairs), dims = 2))\nprintln(\"initial parameters: \", u0)\n\n# First let's run a short chain to determine a good step size\nyt_sample = truth_sample\nmcmc = MCMCWrapper(\n    RWMHSampling(),\n    truth_sample,\n    priors,\n    emulator;\n    init_params = u0\n)\n\nnew_step = optimize_stepsize(\n    mcmc;\n    init_stepsize = 0.1,\n    N = 2000,\n    discard_initial = 0\n)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We choose a sample size of 100,000 for the actual MCMC, discarding the first 1,000 samples as burn-in:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# Now begin the actual MCMC\nprintln(\"Begin MCMC - with step size \", new_step)\nchain = MarkovChainMonteCarlo.sample(\n    mcmc,\n    100_000;\n    stepsize = new_step,\n    discard_initial = 1_000\n)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"After running the MCMC, we can extract the posterior samples as follows:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The samples of the posterior distributions represent the ultimate output of the CES process. By constructing histograms of these samples and comparing them with the known true parameter values, we can evaluate the results' accuracy. Ideally, the peak of the posterior distribution should be located near the true values, indicating a high-quality estimation. Additionally, visualizing the prior distributions alongside the posteriors shows the distributional change effected by the Bayesian learning process.","category":"page"},{"location":"examples/Cloudy_example/#Results","page":"Cloudy example","title":"Results","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We first produce pair plots (also known as corner plots or scatter plot matrices) to visualize the posterior parameter distributions as a grid of histograms. Recall that the task was to solve the inverse problem of finding the parameters N_0 0, k_0, and theta_0, which define a gamma distribution of droplet masses in Cloudy at time t = 0.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"# Make pair plots of the posterior distributions in the unconstrained\n# and in the constrained space (this uses `PairPlots.jl`)\nfigpath_unconstr = joinpath(output_directory,\n                            \"joint_posterior_unconstr.png\")\nfigpath_constr = joinpath(output_directory,\n                          \"joint_posterior_constr.png\")\nlabels = get_name(posterior)\n\ndata_unconstr = (; [(Symbol(labels[i]),\n                     posterior_samples_unconstr[i, :]) for i in 1:length(labels)]...)\ndata_constr = (; [(Symbol(labels[i]),\n                   posterior_samples_constr[i, :]) for i in 1:length(labels)]...)\n\np_unconstr = pairplot(data_unconstr => (PairPlots.Scatter(),))\np_constr = pairplot(data_constr => (PairPlots.Scatter(),))","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"For the GP emulator, the results (shown in the constrained/physical space) look as follows:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: pairplot_posterior_gpjl)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"And we can plot the same for the scalar RF emulator...","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: pairplot_posterior_rf-scalar)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"...and for the vector RF emulator:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: pairplot_posterior_rf-vec)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"In addition, we plot the marginals of the posterior distributions–-we are showing them here for the GP emulator case:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"for idx in 1:n_params\n\n    # Find the range of the posterior samples\n    xmin = minimum(posterior_samples_constr[idx, :])\n    xmax = maximum(posterior_samples_constr[idx, :])\n\n    # Create a figure and axis for plotting\n    fig = Figure(; size = (800, 600))\n    ax = Axis(fig[1, 1])\n\n    # Histogram for posterior samples\n    hist!(ax, posterior_samples_constr[idx, :], bins = 100,\n          color = :darkorange, label = \"posterior\")\n\n    # Plotting the prior distribution\n    hist!(ax, prior_samples_constr[idx, :], bins = 10000,\n          color = :slategray)\n\n    # Adding a vertical line for the true value\n    vlines!(ax, [ϕ_true[idx]], color = :indigo, linewidth = 2.6,\n            label = \"true \" * param_names[idx])\n\n    xlims!(ax, xmin, xmax)\n    ylims!(ax, 0, Inf)\n\n    # Setting title and labels\n    ax.title = param_names[idx]\n    ax.xlabel = \"Value\"\n    ax.ylabel = \"Density\"","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"This is what the marginal distributions of the three parameters look like, for the case of the GP emulator, and in the constrained/physical space:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: posterior_N0_gpjl)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: posterior_theta_gpjl)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"(Image: posterior_k_gpjl)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"Here, the posterior distributions are shown as orange histograms, the prior distribution are shown as grey histograms (though with the exception of the parmaeter k, they are barely visible), and the true parameter values are marked as vertical purple lines.","category":"page"},{"location":"examples/Cloudy_example/#Appendix:-What-Does-Cloudy-Do?","page":"Cloudy example","title":"Appendix: What Does Cloudy Do?","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"For the purpose of Bayesian parameter learning, the forward model can be treated as a black box that processes input parameters to yield specific outputs. However, for those who wish to learn more about the inner workings of Cloudy, we refer to his paper and offer a brief outline below:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The mathematical starting point of Cloudy is the stochastic collection equation (SCE; sometimes also called Smoluchowski equation after Marian Smoluchowski), which describes the time rate of change of f = f(m t), the mass distribution function of liquid water droplets, due to the process of collision and coalescence. The distribution function f depends on droplet mass m and time t and is defined such that f(m) text dm denotes the number of droplets with masses in the interval m m + dm per unit volume. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The stochastic collection equation is an integro-differential equation that can be written as ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"    fracpartial f(m t)partial t = frac12 int_m=0^infty f(m t) f(m-m t)  mathcalC(m m-m)textdm - f(m t) int_m=0^infty f(m t)mathcalC(m m) textdm ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"where mathcalC(m m) is the collision-coalescence kernel, which  encapsulates the physics of droplet-droplet interactions – it describes the rate at which two drops of masses m and m come into contact and coalesce into a drop of mass m + m. The first term on the right-hand side of the SCE describes the rate of increase of the number of drops having a mass m due to collision and coalescence of drops of masses m and m-m (where the factor frac12 avoids double counting), while the second term describes the rate of reduction of drops of mass m due to collision and coalescence of drops having a mass m with other drops. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"We can rewrite the SCE in terms of the moments M_k of f, which are the prognostic variables in Cloudy. They are defined by","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"    M_k = int_0^infty m^k f(m t) textdm","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The time rate of change of the k-th moment of f is obtained by multiplying the SCE by m^k and integrating over the entire range of droplet masses (from m=0 to infty), which yields","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"    fracpartial M_k(t)partial t = frac12int_0^infty left((m+m)^k - m^k - m^kright) mathcalC(m m)f(m t)f(m t)  textdm textdm  (1)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"In this example, the kernel is set to be constant – mathcalC(m m) = B = textconst – and the cloud droplet mass distribution is assumed to be a textGamma(k_t theta_t) distribution, scaled by a factor N_0t which denotes the droplet number concentration:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"f(m t) = fracN_0tGamma(k_t)theta_t^k m^k_t-1 exp(-mtheta_t)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"The parameter vector phi_t= N_0t k_t theta_t changes over time (as indicated by the subscript t), as the shape of the distribution evolves. In fact, there is a priori no reason to assume that the distribution would retain its Gamma shape over time, but this is a common assumption that is made in order to solve the closure problem (without this assumption, one would have to keep track of infinitely many moments of the mass distribution in order to uniquely identify the distribution f at each time step, which is obviously not practicable).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy example","title":"Cloudy example","text":"For Gamma mass distribution functions, specifying the first three moments (M_0, M_1, and M_2) is sufficient to uniquely determine the parameter vector phi_t, hence Cloudy solves equation (1) for k = 0 1 2. This mapping of the parameters of the initial cloud droplet mass distribution to the (zeroth-, first-, and second-order) moments of the distribution at a specified end time is done by DynamicalModel.jl.","category":"page"},{"location":"API/Emulators/#Emulators","page":"General Interface","title":"Emulators","text":"","category":"section"},{"location":"API/Emulators/","page":"General Interface","title":"General Interface","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/Emulators/","page":"General Interface","title":"General Interface","text":"Emulator\noptimize_hyperparameters!(::Emulator)\nEmulator(::MachineLearningTool, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict\nnormalize\nstandardize\nreverse_standardize\nsvd_transform\nsvd_reverse_transform_mean_cov","category":"page"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.Emulator","page":"General Interface","title":"CalibrateEmulateSample.Emulators.Emulator","text":"struct Emulator{FT<:AbstractFloat}\n\nStructure used to represent a general emulator, independently of the algorithm used.\n\nFields\n\nmachine_learning_tool::CalibrateEmulateSample.Emulators.MachineLearningTool: Machine learning tool, defined as a struct of type MachineLearningTool.\ntraining_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT} where FT<:AbstractFloat: Normalized, standardized, transformed pairs given the Booleans normalize_inputs, standardize_outputs, retained_svd_frac.\ninput_mean::AbstractVector{FT} where FT<:AbstractFloat: Mean of input; length input_dim.\nnormalize_inputs::Bool: If normalizing: whether to fit models on normalized inputs ((inputs - input_mean) * sqrt_inv_input_cov).\nnormalization::Union{Nothing, LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat: (Linear) normalization transformation; size input_dim × input_dim.\nstandardize_outputs::Bool: Whether to fit models on normalized outputs: outputs / standardize_outputs_factor.\nstandardize_outputs_factors::Union{Nothing, AbstractVector{FT}} where FT<:AbstractFloat: If standardizing: Standardization factors (characteristic values of the problem).\ndecomposition::Union{Nothing, LinearAlgebra.SVD}: The singular value decomposition of obs_noise_cov, such that obs_noise_cov = decomposition.U * Diagonal(decomposition.S) * decomposition.Vt. NB: the SVD may be reduced in dimensions.\nretained_svd_frac::AbstractFloat: Fraction of singular values kept in decomposition. A value of 1 implies full SVD spectrum information.\n\n\n\n\n\n","category":"type"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.Emulator}","page":"General Interface","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    args...;\n    kwargs...\n) -> Any\n\n\nOptimizes the hyperparameters in the machine learning tool.\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.Emulator-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.MachineLearningTool, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"General Interface","title":"CalibrateEmulateSample.Emulators.Emulator","text":"Emulator(\n    machine_learning_tool::CalibrateEmulateSample.Emulators.MachineLearningTool,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat};\n    obs_noise_cov,\n    normalize_inputs,\n    standardize_outputs,\n    standardize_outputs_factors,\n    decorrelate,\n    retained_svd_frac,\n    mlt_kwargs...\n) -> CalibrateEmulateSample.Emulators.Emulator{FT} where Float64<:FT<:AbstractFloat\n\n\nPositional Arguments\n\nmachine_learning_tool ::MachineLearningTool,\ninput_output_pairs ::PairedDataContainer\n\nKeyword Arguments \n\nobs_noise_cov: A matrix/uniform scaling to provide the observational noise covariance of the data - used for data processing (default nothing),\nnormalize_inputs: Normalize the inputs to be unit Gaussian, in the smallest full-rank space of the data (default true),\nstandardize_outputs: Standardize outputs with by dividing by a vector of provided factors (default false),\nstandardize_outputs_factors: If standardizing, the provided dim_output-length vector of factors,\ndecorrelate: Apply (truncated) SVD to the outputs. Predictions are returned in the decorrelated space, (default true)\nretained_svd_frac: The cumulative sum of singular values retained after output SVD truncation (default 1.0 - no truncation)\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#GaussianProcesses.predict","page":"General Interface","title":"GaussianProcesses.predict","text":"predict(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2}\n) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\npredict(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    new_inputs::AbstractMatrix;\n    multithread\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\npredict(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    new_inputs::AbstractMatrix\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\npredict(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    new_inputs::AbstractMatrix;\n    transform_to_real,\n    vector_rf_unstandardize,\n    mlt_kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction using the emulator on new inputs (each new inputs given as data columns). Default is to predict in the decorrelated space.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.normalize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.normalize","text":"normalize(\n    emulator::CalibrateEmulateSample.Emulators.Emulator,\n    inputs::AbstractVecOrMat\n) -> Any\n\n\nNormalize the input data, with a normalizing function.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.standardize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.standardize","text":"standardize(\n    outputs::AbstractVecOrMat,\n    output_covs::Union{AbstractVector{<:AbstractMatrix}, AbstractVector{<:LinearAlgebra.UniformScaling}},\n    factors::AbstractVector\n) -> Tuple{Any, Union{AbstractVector{<:AbstractMatrix}, AbstractVector{<:LinearAlgebra.UniformScaling}}}\n\n\nStandardize with a vector of factors (size equal to output dimension).\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.reverse_standardize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.reverse_standardize","text":"reverse_standardize(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    outputs::AbstractVecOrMat,\n    output_covs::AbstractVecOrMat\n) -> Tuple{Any, Any}\n\n\nReverse a previous standardization with the stored vector of factors (size equal to output  dimension). output_cov is a Vector of covariance matrices, such as is returned by svd_reverse_transform_mean_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_transform","page":"General Interface","title":"CalibrateEmulateSample.Emulators.svd_transform","text":"svd_transform(\n    data::AbstractArray{FT<:AbstractFloat, 2},\n    obs_noise_cov::Union{Nothing, AbstractArray{FT<:AbstractFloat, 2}};\n    retained_svd_frac\n) -> Tuple{Any, Any}\n\n\nApply a singular value decomposition (SVD) to the data\n\ndata - GP training data/targets; size output_dim × N_samples\nobs_noise_cov - covariance of observational noise\ntruncate_svd - Project onto this fraction of the largest principal components. Defaults to 1.0 (no truncation).\n\nReturns the transformed data and the decomposition, which is a matrix  factorization of type LinearAlgebra.SVD. \n\nNote: If F::SVD is the factorization object, U, S, V and Vt can be obtained via  F.U, F.S, F.V and F.Vt, such that A = U * Diagonal(S) * Vt. The singular values  in S are sorted in descending order.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","page":"General Interface","title":"CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","text":"svd_reverse_transform_mean_cov(\n    μ::AbstractArray{FT<:AbstractFloat, 2},\n    σ2::AbstractVector,\n    decomposition::LinearAlgebra.SVD\n) -> Tuple{Any, Any}\n\n\nTransform the mean and covariance back to the original (correlated) coordinate system\n\nμ - predicted mean; size output_dim × N_predicted_points.\nσ2 - predicted variance; size output_dim × N_predicted_points.      - predicted covariance; size N_predicted_points array of size output_dim × output_dim.\ndecomposition - SVD decomposition of obs_noise_cov.\n\nReturns the transformed mean (size output_dim × N_predicted_points) and variance.  Note that transforming the variance back to the original coordinate system results in non-zero off-diagonal elements, so instead of just returning the  elements on the main diagonal (i.e., the variances), we return the full  covariance at each point, as a vector of length N_predicted_points, where  each element is a matrix of size output_dim × output_dim.\n\n\n\n\n\n","category":"function"},{"location":"emulate/#The-Emulate-stage","page":"Emulator","title":"The Emulate stage","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Emulation is performed through the construction of an Emulator object, which has two components","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"A wrapper for any statistical emulator,\nData-processing and dimensionality reduction functionality.","category":"page"},{"location":"emulate/#Typical-construction-from-Lorenz_example.jl","page":"Emulator","title":"Typical construction from Lorenz_example.jl","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"First, obtain data in a PairedDataContainer, for example, get this from an EnsembleKalmanProcess ekpobj generated during the Calibrate stage, or see the constructor here","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"using CalibrateEmulateSample.Utilities\ninput_output_pairs = Utilities.get_training_points(ekpobj, 5) # use first 5 iterations as data","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Wrapping a predefined machine learning tool, e.g. a Gaussian process gauss_proc, the Emulator can then be built:","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"emulator = Emulator(\n    gauss_proc, \n    input_output_pairs; # optional arguments after this\n    obs_noise_cov = Γy,\n    normalize_inputs = true,\n    standardize_outputs = true,\n    standardize_outputs_factors = factor_vector,\n    retained_svd_frac = 0.95,\n)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"The optional arguments above relate to the data processing.","category":"page"},{"location":"emulate/#Emulator-Training","page":"Emulator","title":"Emulator Training","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"The emulator is trained when we combine the machine learning tool and the data into the Emulator above.  For any machine learning tool, hyperparameters are optimized.","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"For some machine learning packages however, this may be completed during construction automatically, and for others this will not. If automatic construction took place, the optimize_hyperparameters! line does not perform any new task, so may be safely called. In the Lorenz example, this line learns the hyperparameters of the Gaussian process, which depend on the choice of kernel, and the choice of GP package. Predictions at new inputs can then be made using","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"y, cov = Emulator.predict(emulator, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"This returns both a mean value and a covariance.","category":"page"},{"location":"emulate/#Data-processing","page":"Emulator","title":"Data processing","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Some effects of the following are outlined in a practical setting in the results and appendices of Howland, Dunbar, Schneider, (2022).","category":"page"},{"location":"emulate/#Diagonalization-and-output-dimension-reduction","page":"Emulator","title":"Diagonalization and output dimension reduction","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"obs_noise_cov = Γy (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"We always use singular value decomposition to diagonalize the output space, requiring output covariance Γy. Why? If we need to train a mathbbR^10 to mathbbR^100 emulator, diagonalization allows us to instead train 100 mathbbR^10 to mathbbR^1 emulators (far cheaper).","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"retained_svd_frac = 0.95 (default 1.0)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Performance is increased further by throwing away less informative output dimensions, if 95% of the information (i.e., variance) is in the first 40 diagonalized output dimensions then setting retained_svd_frac=0.95 will train only 40 emulators.","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"note: Note\nDiagonalization is an approximation. It is however a good approximation when the observational covariance varies slowly in the parameter space.","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"warn: Warn\nSevere approximation errors can occur if obs_noise_cov is not provided.","category":"page"},{"location":"emulate/#Normalization-and-standardization","page":"Emulator","title":"Normalization and standardization","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"normalize_inputs = true (default: true)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"We normalize the input data in a standard way by centering, and scaling with the empirical covariance","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"standardize_outputs = true (default: false)\nstandardize_outputs_factors = factor_vector (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"To help with poor conditioning of the covariance matrix, users can also standardize each output dimension with by a multiplicative factor given by the elements of factor_vector.","category":"page"},{"location":"emulate/#modular-interface","page":"Emulator","title":"Modular interface","text":"","category":"section"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Developers may contribute new tools by performing the following","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Create MyMLToolName.jl, and include \"MyMLToolName.jl\" in Emulators.jl\nCreate a struct MyMLTool <: MachineLearningTool, containing any arguments or optimizer options \nCreate the following three methods to build, train, and predict with your tool (use GaussianProcess.jl as a guide)","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"build_models!(mlt::MyMLTool, iopairs::PairedDataContainer) -> Nothing\noptimize_hyperparameters!(mlt::MyMLTool, args...; kwargs...) -> Nothing\nfunction predict(mlt::MyMLTool, new_inputs::Matrix; kwargs...) -> Matrix, Union{Matrix, Array{,3}","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"note: on dimensions of the predict inputs and outputs\nThe predict method takes as input, an input_dim-by-N_new matrix. It return both a predicted mean and a predicted (co)variance at new inputs. (i) for scalar-output methods relying on diagonalization, return output_dim-by-N_new matrices for mean and variance, (ii) For vector-output methods, return output_dim-by-N_new for mean and output_dim-by-output_dim-by-N_new for covariances.","category":"page"},{"location":"emulate/","page":"Emulator","title":"Emulator","text":"Please get in touch with our development team when contributing new statistical emulators, to help us ensure the smoothest interface with any new tools.","category":"page"},{"location":"#CalibrateEmulateSample.jl","page":"Home","title":"CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl solves parameter estimation problems using accelerated (and approximate) Bayesian inversion.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The framework can be applied currently to learn:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the joint distribution for a moderate numbers of parameters (<40),\nit is not inherently restricted to unimodal distributions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It can be used with computer models that:","category":"page"},{"location":"","page":"Home","title":"Home","text":"can be noisy or chaotic,\nare non-differentiable,\ncan only be treated as black-box (interfaced only with parameter files).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The computer model is supplied by the user, as a parameter-to-data map mathcalG(theta) mathbbR^p rightarrow mathbbR^d. For example, mathcalG could be a map from any given parameter configuration theta to a collection of statistics of a dynamical system trajectory. mathcalG is referred to as the forward model in the Bayesian inverse problem setting.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The data produced by the forward model are compared to observations y, which are assumed to be corrupted by additive noise eta, such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"y = mathcalG(theta) + eta","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the noise eta is drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y).","category":"page"},{"location":"#The-inverse-problem","page":"Home","title":"The inverse problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Given an observation y, the computer model mathcalG, the observational noise Gamma_y, and some broad prior information on theta, we return the joint distribution of a data-informed distribution for \"theta given y\".","category":"page"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, CalibrateEmulateSample.jl breaks this problem into a sequence of three steps: calibration, emulation, and sampling. A comprehensive treatment of the calibrate-emulate-sample approach to Bayesian inverse problems can be found in Cleary et al. (2020).","category":"page"},{"location":"#The-three-steps-of-the-algorithm:-see-our-walkthrough-of-the-[Sinusoid-Example](@ref)","page":"Home","title":"The three steps of the algorithm: see our walkthrough of the Sinusoid Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Learn the vertical shift and amplitude of the signal given the noisy observation","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"assets/sinusoid_true_vs_observed_signal.png\" width=\"400\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"The calibrate step of the algorithm consists of an application of Ensemble Kalman Processes, which generates input-output pairs theta mathcalG(theta) in high density around an optimal parameter theta^*. Here, theta are amplitude and vertical shift pairs, and mathcalG(theta) are the resulting signal mean and range. This theta^* will be near a mode of the posterior distribution (Note: This is the only time we interface with the forward model mathcalG).","category":"page"},{"location":"","page":"Home","title":"Home","text":"calibrate with EKP to generate data pairs...","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"assets/sinusoid_eki_pairs.png\" width=\"400\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"The emulate step takes these pairs theta mathcalG(theta) and trains a statistical surrogate model (e.g., a Gaussian process), emulating the forward map mathcalG.","category":"page"},{"location":"","page":"Home","title":"Home","text":"emulate the map statistically from EKP pairs... ","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"assets/sinusoid_GP_emulator_contours.png\" width=\"400\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"The sample step uses this surrogate in place of mathcalG in a sampling method (Markov chain Monte Carlo) to sample the posterior distribution of theta.","category":"page"},{"location":"","page":"Home","title":"Home","text":"sample the emulated map with MCMC...","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"assets/sinusoid_MCMC_hist_GP.png\" width=\"400\">","category":"page"},{"location":"#Code-Components","page":"Home","title":"Code Components","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl contains the following modules:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nCalibrateEmulateSample.jl A wrapper for the pipeline\nEmulator.jl Modular template for the emulators\nGaussianProcess.jl A Gaussian process emulator\nScalar/VectorRandomFeatureInterface.jl A Scalar/Vector-output Random Feature emulator\nMarkovChainMonteCarlo.jl Modular template for Markov Chain Monte Carlo samplers\nUtilities.jl Helper functions","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl is being developed by the Climate Modeling Alliance.","category":"page"},{"location":"examples/edmf_example/#Extended-Eddy-Diffusivity-Mass-Flux-(EDMF)-Scheme","page":"Turbulence example","title":"Extended Eddy-Diffusivity Mass-Flux (EDMF) Scheme","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository","category":"page"},{"location":"examples/edmf_example/#Background","page":"Turbulence example","title":"Background","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The extended EDMF scheme is a unified model of turbulence and convection. More information about the model can be found here. This example builds an emulator of the extended EDMF scheme from input-output pairs obtained during a calibration process, and runs emulator-based MCMC to obtain an estimate of the joint parameter distribution.","category":"page"},{"location":"examples/edmf_example/#What-is-being-solved-here","page":"Turbulence example","title":"What is being solved here","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"This example reads calibration data containing input-output pairs obtained during calibration of the EDMF scheme. The calibration is performed using ensemble Kalman inversion, an ensemble-based algorithm that updates the location of the input parameters from the prior to the posterior, thus ensuring an optimal placement of the data used to train the emulator. In this example, the input is formed by either two or five EDMF parameters, and the output is the time-averaged liquid water path (LWP) at 40 locations in the eastern Pacific Ocean. The calibration data also contains the prior distribution of EDMF parameters and the variance of the observed variables (LWP in this case), which is used as a proxy for the magnitude of observational noise.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"More information about EDMF calibration can be found here. The calibration data is used to train the emulator.","category":"page"},{"location":"examples/edmf_example/#Running-the-examples","page":"Turbulence example","title":"Running the examples","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"We have two example scenario data (output from a (C)alibration run) that must be simply unzipped before calibration:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"ent-det-calibration.zip # two-parameter calibration\nent-det-tked-tkee-stab-calibration.zip # five-parameter calibration","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To perform uncertainty quantification use the file uq_for_EDMF.jl. Set the experiment name, and date (for outputs), e.g.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"exp_name = \"ent-det-tked-tkee-stab-calibration\" \ndate_of_run = Date(year, month, day)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"and call,","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"> julia --project uq_for_EDMF.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Info\nThese runs take currently take ~1-3 hours to complete with Gaussian process emulator. Random feature training currently requires significant multithreading for performance and takes a similar amount of time.","category":"page"},{"location":"examples/edmf_example/#Solution-and-output","page":"Turbulence example","title":"Solution and output","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The solution is the posterior distribution, stored in the file posterior.jld2.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior is visualized by using plot_posterior.jl, which produces corner-type scatter plots of posterior distribution, which show pairwise correlations. Again, set the exp_name and date_of_run values, then call","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"julia --project plot_posterior.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"For example, using Random features for case exp_name = \"ent-det-calibration\" one obtains","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"<img src=\"../../assets/edmf_nonsep_posterior_2d.png\" width=\"300\">","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"and exp_name = \"ent-det-tked-tkee-stab-calibration\" or one obtains","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"<img src=\"../../assets/edmf_nonsep_posterior_5d.png\" width=\"600\">","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior samples can also be investigated directly. They are stored as a ParameterDistribution-type Samples object. One can load this and extract an array of parameters with:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"# input:\n# path to posterior.jld2: posterior_filepath (string)\n\nusing CalibrateEmulateSample.ParameterDistribution\nposterior = load(posterior_filepath)[\"posterior\"]\nposterior_samples = vcat([get_distribution(posterior)[name] for name in get_name(posterior)]...) # samples are columns","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To transform these samples into physical parameter space use the following:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"transformed_posterior_samples =\nmapslices(x -> transform_unconstrained_to_constrained(posterior, x), posterior_samples, dims = 1)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Computational vs Physical space\nThe computational theta-space are the parameters on which the algorithms act. Statistics (e.g. mean/covariance) are most meaningful when taken in this space. The physical phi-space is a (nonlinear) transformation of the computational space to apply parameter constraints. To pass parameter values back into the forward model, one must transform them. Full details and examples can be found here","category":"page"}]
}
