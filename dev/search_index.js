var documenterSearchIndex = {"docs":
[{"location":"API/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"Modules = [CalibrateEmulateSample.Utilities]\nOrder   = [:module, :type, :function]","category":"page"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_obs_sample-Union{Tuple{IT}, Tuple{Random.AbstractRNG, EnsembleKalmanProcesses.Observations.Observation}} where IT<:Int64","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_obs_sample","text":"get_obs_sample(rng::Random.AbstractRNG, obs::EnsembleKalmanProcesses.Observations.Observation; rng_seed) -> Any\n\n\nReturn a random sample from the observations, for use in the MCMC.\n\nrng - optional RNG object used to pick random sample; defaults to Random.GLOBAL_RNG.\nobs - Observation struct with the observations (extract will pick one         of the sample observations to train).\nrng_seed - optional kwarg; if provided, used to re-seed rng before sampling.\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_training_points-Union{Tuple{P}, Tuple{IT}, Tuple{FT}, Tuple{EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, IT}} where {FT, IT, P}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_training_points","text":"get_training_points(ekp::EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, N_train_iterations) -> EnsembleKalmanProcesses.DataContainers.PairedDataContainer\n\n\nExtract the training points needed to train the Gaussian process regression.\n\nekp - EnsembleKalmanProcess holding the parameters and the data that were produced during the Ensemble Kalman (EK) process.\nN_train_iterations - Number of EK layers/iterations to train on.\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#MarkovChainMonteCarlo","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"Modules = [CalibrateEmulateSample.MarkovChainMonteCarlo]\nOrder   = [:module, :type, :function]","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMC","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMC","text":"mutable struct MCMC{FT<:AbstractFloat, IT<:Int64}\n\nStructure to organize MCMC parameters and data.\n\nFields\n\nobs_sample::AbstractVector{FT} where FT<:AbstractFloat\nA single sample from the observations. Can e.g. be picked from an Obs struct using get_obs_sample.\nobs_noise_cov::Union{AbstractMatrix{FT}, LinearAlgebra.UniformScaling{FT}} where FT<:AbstractFloat\nCovariance of the observational noise.\nprior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\nArray of length N_parameters with the parameters' prior distributions.\nstep::AbstractFloat\nMCMC step size.\nburnin::Int64\nNumber of MCMC steps that are considered burnin.\nparam::AbstractVector{FT} where FT<:AbstractFloat\nThe current parameters.\nposterior::AbstractMatrix{FT} where FT<:AbstractFloat\nArray of accepted MCMC parameter samples (param_dim × n_samples). The histogram of these samples gives an approximation of the posterior distribution of the parameters.\nlog_posterior::Union{Nothing, FT} where FT<:AbstractFloat\nThe current value of the logarithm of the posterior (= log_likelihood + log_prior of the current parameters).\niter::Int64\nIteration/step of the MCMC.\naccept::Int64\nNumber of accepted proposals.\nalgtype::String\nMCMC algorithm to use. Currently implemented: 'rmw' (random walk Metropolis), 'pCN' (preconditioned Crank-Nicholson).\nrng::Random.AbstractRNG\nRandom number generator object (algorithm + seed) used for sampling and noise, for reproducibility.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMC-Union{Tuple{IT}, Tuple{FT}, Tuple{AbstractVector{FT}, Union{AbstractMatrix{FT}, LinearAlgebra.UniformScaling{FT}}, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, FT, AbstractVector{FT}, IT, String, IT}} where {FT<:AbstractFloat, IT<:Int64}","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMC","text":"MCMC(obs_sample::AbstractArray{FT<:AbstractFloat, 1}, obs_noise_cov::Union{AbstractArray{FT<:AbstractFloat, 2}, LinearAlgebra.UniformScaling{FT<:AbstractFloat}}, prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, step::AbstractFloat, param_init::AbstractArray{FT<:AbstractFloat, 1}, max_iter::Int64, algtype::String, burnin::Int64; svdflag, standardize, norm_factor, truncate_svd, rng) -> CalibrateEmulateSample.MarkovChainMonteCarlo.MCMC{_A, Int64} where _A<:AbstractFloat\n\n\nConstructor for MCMC.\n\nmax_iter - The number of MCMC steps to perform (e.g., 100_000).\n\n\n\n\n\n","category":"method"},{"location":"examples/lorenz_example/#Lorenz-96-example","page":"Lorenz example","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We provide the following template for how the tools may be applied.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"For small examples typically have 2 files.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"GModel.jl Contains the forward map. The inputs should be the so-called free parameters we are interested in learning, and the output should be the measured data\nThe example script which contains the inverse problem setup and solve","category":"page"},{"location":"examples/lorenz_example/#The-structure-of-the-example-script","page":"Lorenz example","title":"The structure of the example script","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"First we create the data and the setting for the model","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Set up the forward model.\nConstruct/load the truth data. Store this data conveniently in the Observations.Observation object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we set up the inverse problem","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Define the prior distributions. Use the ParameterDistribution object\nDecide on which process tool you would like to use (we recommend you begin with Invesion()). Then initialize this with the relevant constructor\ninitialize the EnsembleKalmanProcess object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we solve the inverse problem, in a loop perform the following for as many iterations as required:","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Obtain the current parameter ensemble\nTransform them from the unbounded computational space to the physical space\ncall the forward map on the ensemble of parameters, producing an ensemble of measured data\ncall the update_ensemble! function to generate a new parameter ensemble based on the new data","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"One can then obtain the solution, dependent on the process type.","category":"page"},{"location":"API/GaussianProcess/#GaussianProcess","page":"Gaussian Process","title":"GaussianProcess","text":"","category":"section"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"GaussianProcessesPackage\nPredictionType\nGaussianProcess\nbuild_models!\noptimize_hyperparameters!(::GaussianProcess{GPJL})","category":"page"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcessesPackage","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcessesPackage","text":"abstract type GaussianProcessesPackage\n\nType to dispatch which GP package to use:\n\nGPJL for GaussianProcesses.jl,\nSKLJL for the ScikitLearn GaussianProcessRegressor.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.PredictionType","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.PredictionType","text":"abstract type PredictionType\n\nPredict type for GPJL in GaussianProcesses.jl:\n\nYType\nFType latent function.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcess","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcess","text":"struct GaussianProcess{GPPackage} <: CalibrateEmulateSample.Emulators.MachineLearningTool\n\nStructure holding training input and the fitted Gaussian process regression models.\n\nFields\n\nmodels::Vector{Union{Nothing, PyCall.PyObject, GaussianProcesses.GPE}}\nThe Gaussian Process (GP) Regression model(s) that are fitted to the given input-data pairs.\nkernel::Union{Nothing, var\"#s11\", var\"#s12\"} where {var\"#s11\"<:GaussianProcesses.Kernel, var\"#s12\"<:PyCall.PyObject}\nKernel object.\nnoise_learn::Bool\nLearn the noise with the White Noise kernel explicitly?\nprediction_type::CalibrateEmulateSample.Emulators.PredictionType\nPrediction type (y to predict the data, f to predict the latent function).\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.build_models!","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat})\n\n\nMethod to build Gaussian process models based on the package.\n\n\n\n\n\n","category":"function"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}}","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL})\n\n\nOptimize Gaussian process hyperparameters using in-build package method.\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#Emulators","page":"General Emulator","title":"Emulators","text":"","category":"section"},{"location":"API/Emulators/","page":"General Emulator","title":"General Emulator","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/Emulators/","page":"General Emulator","title":"General Emulator","text":"Emulator\noptimize_hyperparameters!(::Emulator)\npredict\nnormalize\nstandardize\nreverse_standardize\nsvd_transform\nsvd_reverse_transform_mean_cov","category":"page"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.Emulator","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.Emulator","text":"struct Emulator{FT<:AbstractFloat}\n\nStructure used to represent a general emulator, independently of the algorithm used.\n\nFields\n\nmachine_learning_tool::CalibrateEmulateSample.Emulators.MachineLearningTool\nMachine learning tool, defined as a struct of type MachineLearningTool.\ntraining_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT} where FT<:AbstractFloat\nNormalized, standardized, transformed pairs given the Boolean's normalize_inputs, standardize_outputs, truncate_svd.\ninput_mean::AbstractVector{FT} where FT<:AbstractFloat\nMean of input; length input_dim.\nnormalize_inputs::Bool\nSquare root of the inverse of the input covariance matrix; size input_dim × input_dim.\nsqrt_inv_input_cov::Union{Nothing, AbstractMatrix{FT}, LinearAlgebra.UniformScaling{FT}} where FT<:AbstractFloat\nWhether to fit models on normalized outputs: outputs / standardize_outputs_factor.\nstandardize_outputs::Bool\nIf normalizing: whether to fit models on normalized inputs ((inputs - input_mean) * sqrt_inv_input_cov).\nstandardize_outputs_factors::Union{Nothing, AbstractVector{FT}} where FT<:AbstractFloat\nIf standardizing: Standardization factors (characteristic values of the problem).\ndecomposition::Union{Nothing, LinearAlgebra.SVD}\nThe singular value decomposition of obs_noise_cov, such that obs_noise_cov = decomposition.U * Diagonal(decomposition.S) * decomposition.Vt. NB: the SVD may be reduced in dimensions.\n\n\n\n\n\n","category":"type"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.Emulator}","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat})\n\n\nOptimizes the hyperparameters in the machine learning tool.\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.predict","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.predict","text":"predict(gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, new_inputs::AbstractArray{FT<:AbstractFloat, 2}) -> Tuple{Matrix{Float64}, Matrix{Float64}}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\npredict(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, new_inputs::AbstractArray{FT<:AbstractFloat, 2}; transform_to_real) -> Tuple{Any, Any}\n\n\nMakes a prediction using the emulator on new inputs (each new inputs given as data columns). Default is to predict in the decorrelated space.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.normalize","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.normalize","text":"normalize(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, inputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}) -> Any\n\n\nNormalize the input data, with a normalizing function.\n\n\n\n\n\nnormalize(inputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}, input_mean::AbstractArray{FT<:AbstractFloat, 1}, sqrt_inv_input_cov::Union{AbstractArray{FT<:AbstractFloat, 2}, LinearAlgebra.UniformScaling{FT<:AbstractFloat}}) -> Any\n\n\nNormalize with the empirical Gaussian distribution of points.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.standardize","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.standardize","text":"standardize(outputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}, output_covs::Array{var\"#s18\", 1} where var\"#s18\"<:Union{AbstractArray{FT<:AbstractFloat, 2}, LinearAlgebra.UniformScaling{FT<:AbstractFloat}}, factors::AbstractArray{FT<:AbstractFloat, 1}) -> Tuple{Any, Vector{var\"#s20\"} where {FT<:AbstractFloat, var\"#s20\"<:Union{AbstractMatrix{FT}, LinearAlgebra.UniformScaling{FT}}}}\n\n\nStandardize with a vector of factors (size equal to output dimension).\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.reverse_standardize","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.reverse_standardize","text":"reverse_standardize(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, outputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}, output_covs::Union{AbstractArray{FT<:AbstractFloat, 2}, Array{var\"#s19\", 1} where var\"#s19\"<:AbstractArray{FT<:AbstractFloat, 2}}) -> Tuple{Any, Any}\n\n\nReverse a previous standardization with the stored vector of factors (size equal to output  dimension). output_cov is a Vector of covariance matrices, such as is returned by svd_reverse_transform_mean_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_transform","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.svd_transform","text":"svd_transform(data::AbstractArray{FT<:AbstractFloat, 2}, obs_noise_cov::Union{Nothing, AbstractArray{FT<:AbstractFloat, 2}}; truncate_svd) -> Tuple{Any, Any}\n\n\nApply a singular value decomposition (SVD) to the data\n\ndata - GP training data/targets; size output_dim × N_samples\nobs_noise_cov - covariance of observational noise\ntruncate_svd - Project onto this fraction of the largest principal components. Defaults to 1.0 (no truncation).\n\nReturns the transformed data and the decomposition, which is a matrix  factorization of type LinearAlgebra.SVD. \n\nNote: If F::SVD is the factorization object, U, S, V and Vt can be obtained via  F.U, F.S, F.V and F.Vt, such that A = U * Diagonal(S) * Vt. The singular values  in S are sorted in descending order.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","text":"svd_reverse_transform_mean_cov(μ::AbstractArray{FT<:AbstractFloat, 2}, σ2::AbstractArray{FT<:AbstractFloat, 2}, decomposition::LinearAlgebra.SVD) -> Tuple{Any, Any}\n\n\nTransform the mean and covariance back to the original (correlated) coordinate system\n\nμ - predicted mean; size output_dim × N_predicted_points.\nσ2 - predicted variance; size output_dim × N_predicted_points.\ndecomposition - SVD decomposition of obs_noise_cov.\n\nReturns the transformed mean (size output_dim × N_predicted_points) and variance.  Note that transforming the variance back to the original coordinate system results in non-zero off-diagonal elements, so instead of just returning the  elements on the main diagonal (i.e., the variances), we return the full  covariance at each point, as a vector of length N_predicted_points, where  each element is a matrix of size output_dim × output_dim.\n\n\n\n\n\n","category":"function"},{"location":"#CalibrateEmulateSample.jl","page":"Home","title":"CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl solves parameter estimation problems using (approximate) Bayesian inversion. It is designed for problems that require running a computer model that is expensive to evaluate, but can also be used for simple models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The computer model is supplied by the user – it is a forward model, i.e., it takes certain parameters and produces data that can then be compared with the actual observations. We can think of that model as a parameter-to-data map G(u) mathbbR^p rightarrow mathbbR^d. For example, G could be a global climate model or a model that predicts the motion of a robot arm. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The data produced by the forward model are compared to observations y, which are assumed to be corrupted by additive noise eta, such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginequation\ny = G(u) + eta\nendequation","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the noise eta is drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given knowledge of the  observations y, the forward model G(u) mathbbR^p rightarrow mathbbR^d, and some information about the noise level such as its size or distribution (but not its value), the inverse problem we want to solve is to find the unknown parameters u.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, CalibrateEmulateSample.jl breaks this problem into a sequence of three steps: calibration, emulation, and sampling. A comprehensive treatment of the calibrate-emulate-sample approach to Bayesian inverse problems can be found in Cleary et al., 2020.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In a one-sentence summary, the calibrate step of the algorithm consists of an Ensemble Kalman inversion that is used to find good training points for a Gaussian process regression, which in turn is used as a surrogate (emulator) of the original forward model G in the subsequent Markov chain Monte Carlo sampling of the posterior distributions of the unknown parameters.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl contains the following modules:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nEnsembleKalmanProcesses.jl Calibrate – Ensemble Kalman inversion\nGaussianProcessEmulator.jl Emulate – Gaussian process regression\nMarkovChainMonteCarlo.jl Sample – Markov chain Monte Carlo\nObservations.jl Structure to hold observations\nUtilities.jl Helper functions","category":"page"},{"location":"","page":"Home","title":"Home","text":"The best way to get started is to have a look at the examples!","category":"page"},{"location":"installation_instructions/#Installation-Instructions","page":"Installation instructions","title":"Installation Instructions","text":"","category":"section"},{"location":"installation_instructions/#Installing-CalibrateEmulateSample.jl","page":"Installation instructions","title":"Installing CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Currently CalibrateEmulateSample (CES) depends on some external python dependencies  including scikit-learn wrapped by ScikitLearn.jl, which requires a couple extra  installation steps:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"First clone the project into a new local repository","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> git clone git@github.com:Clima/CalibrateEmulateSample.jl\n> cd CalibrateEmulateSample.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Install and build the project dependencies. Given that CES depends on python packages  it is easiest to set the project to use its own  Conda environment variable (set by exporting the ENV variable PYTHON=\"\").","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Pkg; Pkg.instantiate()","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The scikit-learn package then has to be installed if using a Julia project-specific  Conda environment:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scikit-learn\")'\n","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"See the PyCall.jl documentation  for more information about how to configure the local Julia / Conda / Python environment.","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To test that the package is working:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You need to first build the top-level project before building the documentation:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"cd CalibrateEmulateSample.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Then you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project=docs/ -e 'using Pkg; Pkg.instantiate()'\njulia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html.","category":"page"}]
}
