# Reference the in-tree version of CalibrateEmulateSample on Julias load path
include(joinpath(@__DIR__, "../..", "ci", "linkfig.jl"))
include(joinpath(@__DIR__, "DynamicalModel.jl")) # Import the module that runs Cloudy

# This example requires Cloudy to be installed (it's best to install the master
# branch), which can be done by:
#] add Cloudy#master
using Cloudy
using Cloudy.ParticleDistributions
using Cloudy.KernelTensors

# Import the module that runs Cloudy
#include("DynamicalModel.jl")
#using .DynamicalModel

# Import modules
using Distributions  # probability distributions and associated functions
using StatsBase
using GaussianProcesses
using LinearAlgebra
using StatsPlots
using Plots
using Plots.PlotMeasures # is this needed?
using Random
using JLD2 # is this needed?

# Import Calibrate-Emulate-Sample modules
# For the calibration step we use the EnsembleKalmanProcesses package, and
# for the sampling we use Markov chain Monte Carlo methods. We'll 
# run this example twice, using first a Gaussian process emulator, and 
# then a Random Feature emulator.
using EnsembleKalmanProcesses
using EnsembleKalmanProcesses.Observations
using EnsembleKalmanProcesses.ParameterDistributions
using EnsembleKalmanProcesses.DataContainers
using CalibrateEmulateSample.Emulators
using CalibrateEmulateSample.MarkovChainMonteCarlo
using CalibrateEmulateSample.Utilities

# This example requires Cloudy to be installed.
using Cloudy
const PDistributions = Cloudy.ParticleDistributions

################################################################################
#                                                                              #
#                      Cloudy Calibrate-Emulate-Sample Example                 #
#                                                                              #
#                                                                              #
#     This example uses Cloudy, a microphysics model that simulates the        #
#     coalescence of cloud droplets into bigger drops, to demonstrate how      #
#     the full Calibrate-Emulate-Sample pipeline can be used for Bayesian      #
#     learning and uncertainty quantification of parameters, given some        #
#     observations.                                                            #
#                                                                              #
#     Specifically, this examples shows how to learn parameters of the         #
#     initial cloud droplet mass distribution, given observations of some      #
#     moments of that mass distribution at a later time, after some of the     #
#     droplets have collided and become bigger drops.                          #
#                                                                              #
#     In this example, Cloudy is used in a "perfect model" (aka "known         #
#     truth") setting, which means that the "observations" are generated by    #
#     Cloudy itself, by running it with the true parameter values. In more     #
#     realistic applications, the observations will come from some external    #
#     measurement system.                                                      #
#                                                                              #
#     The purpose is to show how to do parameter learning using                #
#     Calibrate-Emulate-Sample in a simple (and highly artificial) setting.    #
#                                                                              #
#     For more information on Cloudy, see                                      #
#              https://github.com/CliMA/Cloudy.jl.git                          #
#                                                                              #
################################################################################


rng_seed = 41
Random.seed!(rng_seed)
rng = Random.seed!(Random.GLOBAL_RNG, rng_seed)

homedir = pwd()
figure_save_directory = homedir * "/output/"
data_save_directory = homedir * "/output/"
if ~isdir(figure_save_directory)
    mkdir(figure_save_directory)
end
if ~isdir(data_save_directory)
    mkdir(data_save_directory)
end

###
###  Define the (true) parameters and their priors
###

# Define the parameters that we want to learn
# We assume that the true particle mass distribution is a Gamma 
# distribution with parameters N0_true, θ_true, k_true
param_names = ["N0", "θ", "k"]
n_params = length(param_names)
N0_true = 300.0  # number of particles (scaling factor for Gamma distribution)
θ_true = 1.5597  # scale parameter of Gamma distribution
k_true = 0.0817  # shape parameter of Gamma distribution
ϕ_true = [N0_true, θ_true, k_true] # true parameters in constrained space
dist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)


###
###  Define priors for the parameters we want to learn
###

# We choose to use normal distributions to represent the prior distributions of
# the parameters in the transformed (unconstrained) space. i.e log coordinates
prior_N0 = constrained_gaussian(par_names[1], 400, 300, 0.4 * N0_true, Inf)
prior_θ = constrained_gaussian(par_names[2], 1.0, 5.0, 1e-1, Inf)
prior_k = constrained_gaussian(par_names[3], 0.2, 1.0, 1e-4, Inf)
priors = combine_distributions([prior_N0, prior_θ, prior_k])

###
###  Define the data from which we want to learn the parameters
###

data_names = ["M0", "M1", "M2"]
moments = [0.0, 1.0, 2.0]
n_moments = length(moments)


###
###  Model settings
###

# Collision-coalescence kernel to be used in Cloudy
tspan = (0.0, 1.0)
print("tspan ", tspan)
coalescence_coeff = 1 / 3.14 / 4 / 100
kernel_func = x -> coalescence_coeff
kernel = Cloudy.KernelTensors.CoalescenceTensor(kernel_func, 0, 300.0)


###
###  Generate (artificial) truth samples
###

dyn_model_settings_true = DynamicalModel.ModelSettings(kernel, dist_true, moments, tspan)

G_t = DynamicalModel.run_dyn_model(ϕ_true, dyn_model_settings_true)
n_samples = 100
y_t = zeros(length(G_t), n_samples)
# In a perfect model setting, the "observational noise" represents the 
# internal model variability. Since Cloudy is a purely deterministic model, 
# there is no straightforward way of coming up with a covariance structure 
# for this internal model variability. We decide to use a diagonal 
# covariance, with entries (variances) largely proportional to their 
# corresponding data values, G_t
Γy = convert(Array, Diagonal([100.0, 5.0, 30.0]))
μ = zeros(length(G_t))

# Add noise
for i in 1:n_samples
    y_t[:, i] = G_t .+ rand(MvNormal(μ, Γy))
end

truth = Observations.Observation(y_t, Γy, data_names)
truth_sample = truth.mean # TODO: should this be yt[:, end]


###
###  Calibrate: Ensemble Kalman Inversion
###

N_ens = 50 # number of ensemble members
N_iter = 8 # number of EKI iterations
# initial parameters: n_params x N_ens
initial_params = construct_initial_ensemble(rng, priors, N_ens)
ekiobj = EnsembleKalmanProcess(
    initial_params,
    truth_sample,
    truth.obs_noise_cov,
    Inversion(),
)

# Initialize a ParticleDistribution with dummy parameters. The parameters 
# will then be set within `run_dyn_model`
dummy = ones(n_params)
dist_type = ParticleDistributions.GammaPrimitiveParticleDistribution(dummy...)
model_settings = DynamicalModel.ModelSettings(kernel, dist_type, moments, tspan)
# EKI iterations
for n in 1:N_iter
    # Return transformed parameters in physical/constrained space
    ϕ_n = get_ϕ_final(priors, ekiobj)
    # Evaluate forward map
    G_n = [DynamicalModel.run_dyn_model(ϕ_n[:, i], model_settings) for i in 1:N_ens]
    G_ens = hcat(G_n...)  # reformat
    EnsembleKalmanProcesses.update_ensemble!(ekiobj, G_ens)
end

# EKI results: Has the ensemble collapsed toward the truth?
θ_true = transform_constrained_to_unconstrained(priors, ϕ_true)
println("True parameters (unconstrained): ")
println(θ_true)

println("\nEKI results:")
println(get_u_mean_final(ekiobj))

u_stored = get_u(ekiobj, return_array = false)
g_stored = get_g(ekiobj, return_array = false)
#@save data_save_directory * "parameter_storage_eki.jld2" u_stored
#@save data_save_directory * "data_storage_eki.jld2" g_stored
save(
    joinpath(data_save_directory, "cloudy_calibrate_results.jld2"),
    "inputs",
    u_stored,
    "outputs",
    g_stored,
    "priors",
    priors,
    "eki",
    ekiobj,
    "truth_sample",
    truth_sample,
    "truth_sample_mean",
    truth.mean,
    "truth_input_constrained",
    ϕ_true, #constrained here, as these are in a physically constrained space (unlike the u inputs),
)

#plots - unconstrained
gr(size = (1200, 400))

u_init = get_u_prior(ekiobj)
anim_eki_unconst_cloudy = @animate for i in 1:N_iter
    u_i = get_u(ekiobj, i)

    p1 = plot(u_i[1, :], u_i[2, :], seriestype = :scatter, xlims = extrema(u_init[1, :]), ylims = extrema(u_init[2, :]))
    plot!(
        p1,
        [θ_true[1]],
        xaxis = "u1",
        yaxis = "u2",
        seriestype = "vline",
        linestyle = :dash,
        linecolor = :red,
        label = false,
        margin = 5mm,
        title = "EKI iteration = " * string(i),
    )
    plot!(p1, [θ_true[2]], seriestype = "hline", linestyle = :dash, linecolor = :red, label = "optimum")

    p2 = plot(u_i[2, :], u_i[3, :], seriestype = :scatter, xlims = extrema(u_init[2, :]), ylims = extrema(u_init[3, :]))
    plot!(
        p2,
        [θ_true[2]],
        xaxis = "u2",
        yaxis = "u3",
        seriestype = "vline",
        linestyle = :dash,
        linecolor = :red,
        label = false,
        margin = 5mm,
        title = "EKI iteration = " * string(i),
    )
    plot!(p2, [θ_true[3]], seriestype = "hline", linestyle = :dash, linecolor = :red, label = "optimum")

    p3 = plot(u_i[3, :], u_i[1, :], seriestype = :scatter, xlims = extrema(u_init[3, :]), ylims = extrema(u_init[1, :]))
    plot!(
        p3,
        [θ_true[3]],
        xaxis = "u3",
        yaxis = "u1",
        seriestype = "vline",
        linestyle = :dash,
        linecolor = :red,
        label = false,
        margin = 5mm,
        title = "EKI iteration = " * string(i),
    )
    plot!(p3, [θ_true[1]], seriestype = "hline", linestyle = :dash, linecolor = :red, label = "optimum")

    p = plot(p1, p2, p3, layout = (1, 3))
end
gif(anim_eki_unconst_cloudy, joinpath(figure_save_directory, "eki_unconst_cloudy.gif"), fps = 1) # hide

# plots - constrained
ϕ_init = transform_unconstrained_to_constrained(priors, u_init)
anim_eki_cloudy = @animate for i in 1:N_iter
    ϕ_i = get_ϕ(priors, ekiobj, i)

    p1 = plot(ϕ_i[1, :], ϕ_i[2, :], seriestype = :scatter, xlims = extrema(ϕ_init[1, :]), ylims = extrema(ϕ_init[2, :]))
    plot!(
        p1,
        [ϕ_true[1]],
        xaxis = "ϕ1",
        yaxis = "ϕ2",
        seriestype = "vline",
        linestyle = :dash,
        linecolor = :red,
        margin = 5mm,
        label = false,
        title = "EKI iteration = " * string(i),
    )
    plot!(p1, [ϕ_true[2]], seriestype = "hline", linestyle = :dash, linecolor = :red, label = "optimum")

    p2 = plot(ϕ_i[2, :], ϕ_i[3, :], seriestype = :scatter, xlims = extrema(ϕ_init[2, :]), ylims = extrema(ϕ_init[3, :]))
    plot!(
        p2,
        [ϕ_true[2]],
        xaxis = "ϕ2",
        yaxis = "ϕ3",
        seriestype = "vline",
        linestyle = :dash,
        linecolor = :red,
        margin = 5mm,
        label = false,
        title = "EKI iteration = " * string(i),
    )
    plot!(p2, [ϕ_true[3]], seriestype = "hline", linestyle = :dash, linecolor = :red, label = "optimum")

    p3 = plot(ϕ_i[3, :], ϕ_i[1, :], seriestype = :scatter, xlims = extrema(ϕ_init[3, :]), ylims = extrema(ϕ_init[1, :]))
    plot!(
        p3,
        [ϕ_true[3]],
        xaxis = "ϕ3",
        yaxis = "ϕ1",
        seriestype = "vline",
        linestyle = :dash,
        linecolor = :red,
        margin = 5mm,
        label = false,
        title = "EKI iteration = " * string(i),
    )
    plot!(p3, [ϕ_true[1]], seriestype = "hline", linestyle = :dash, linecolor = :red, label = "optimum")

    p = plot(p1, p2, p3, layout = (1, 3))
end
gif(anim_eki_cloudy, joinpath(figure_save_directory, "eki_cloudy.gif"), fps = 1) # hide


###
###  Emulate: Gaussian Process regression
###


gppackage = Emulators.GPJL()
pred_type = Emulators.YType()
gp_kernel = SE(1.0, 1.0) + Mat52Ard(zeros(3), 0.0) + Noise(log(2.0))
gauss_proc = GaussianProcess(
    gppackage;
    kernel = gp_kernel,
    prediction_type = pred_type,
    noise_learn = false,
)

# Get training points
input_output_pairs = Utilities.get_training_points(ekiobj, N_iter)
emulator = Emulator(gauss_proc, input_output_pairs, obs_noise_cov = Γy, normalize_inputs = true)
optimize_hyperparameters!(emulator)

# Check how well the Gaussian Process regression predicts on the
# true parameters
y_mean, y_var = Emulators.predict(emulator, reshape(θ_true, :, 1); transform_to_real = true)
println("GP prediction on true parameters: ")
println(vec(y_mean))
println("true data: ")
println(truth.mean)


###
###  Sample: Markov Chain Monte Carlo
###

# initial values
u0 = vec(mean(get_inputs(input_output_pairs), dims = 2))
println("initial parameters: ", u0)

# First let's run a short chain to determine a good step size
yt_sample = truth_sample
mcmc = MCMCWrapper(RWMHSampling(), yt_sample, priors, emulator; init_params = u0)
new_step = optimize_stepsize(mcmc; init_stepsize = 0.1, N = 2000, discard_initial = 0)

# Now begin the actual MCMC
println("Begin MCMC - with step size ", new_step)
chain = MarkovChainMonteCarlo.sample(mcmc, 100_000; stepsize = new_step, discard_initial = 1_000)
posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)

post_mean = mean(posterior)
post_cov = cov(posterior)
println("posterior mean")
println(post_mean)
println("posterior covariance")
println(post_cov)

# Plot the posteriors together with the priors and the true parameter values
# (in the transformed/unconstrained space)
n_params = length(get_name(posterior))

gr(size = (800, 600))

for idx in 1:n_params
    if idx == 1
        xs = collect(range(5.15, stop = 5.25, length = 1000))
    elseif idx == 2
        xs = collect(range(0.0, stop = 0.5, length = 1000))
    elseif idx == 3
        xs = collect(range(-3.0, stop = -2.0, length = 1000))
    else
        throw("not implemented")
    end

    label = "true " * param_names[idx]
    posterior_samples = dropdims(get_distribution(posterior)[param_names[idx]], dims = 1)
    histogram(
        posterior_samples,
        bins = 100,
        normed = true,
        fill = :slategray,
        thickness_scaling = 2.0,
        lab = "posterior",
        legend = :outertopright,
    )
    prior_dist = get_distribution(mcmc.prior)[param_names[idx]]
    plot!(xs, prior_dist, w = 2.6, color = :blue, lab = "prior")
    plot!([θ_true[idx]], seriestype = "vline", w = 2.6, lab = label)
    title!(param_names[idx])
    figpath = joinpath(figure_save_directory, "posterior_" * param_names[idx] * ".png")
    StatsPlots.savefig(figpath)
    linkfig(figpath)
end
