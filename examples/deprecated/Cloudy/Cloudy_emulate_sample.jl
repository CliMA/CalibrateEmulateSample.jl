# Reference the in-tree version of CalibrateEmulateSample on Julias load path
include(joinpath(@__DIR__, "../..", "ci", "linkfig.jl"))

# Import modules
using Distributions  # probability distributions and associated functions
using StatsBase
using GaussianProcesses
using LinearAlgebra
using StatsPlots
using Plots
using Plots.PlotMeasures # is this needed?
using Random
using JLD2 # is this needed?

# Import Calibrate-Emulate-Sample modules
#using EnsembleKalmanProcesses.DataContainers
using EnsembleKalmanProcesses
using EnsembleKalmanProcesses.ParameterDistributions
using EnsembleKalmanProcesses.DataContainers
using CalibrateEmulateSample.Emulators
using CalibrateEmulateSample.MarkovChainMonteCarlo
using CalibrateEmulateSample.Utilities

function get_standardizing_factors(data::Array{FT, 2}) where {FT}
    # Input: data size: N_data x N_ensembles
    # Ensemble median of the data
    norm_factor = median(data, dims = 2) # N_data x 1 array
    return norm_factor
end

################################################################################
#                                                                              #
#                      Cloudy Calibrate-Emulate-Sample Example                 #
#                                                                              #
#                                                                              #
#     This example uses Cloudy, a microphysics model that simulates the        #
#     coalescence of cloud droplets into bigger drops, to demonstrate how      #
#     the full Calibrate-Emulate-Sample pipeline can be used for Bayesian      #
#     learning and uncertainty quantification of parameters, given some        #
#     observations.                                                            #
#                                                                              #
#     Specifically, this examples shows how to learn parameters of the         #
#     initial cloud droplet mass distribution, given observations of some      #
#     moments of that mass distribution at a later time, after some of the     #
#     droplets have collided and become bigger drops.                          #
#                                                                              #
#     In this example, Cloudy is used in a "perfect model" (aka "known         #
#     truth") setting, which means that the "observations" are generated by    #
#     Cloudy itself, by running it with the true parameter values. In more     #
#     realistic applications, the observations will come from some external    #
#     measurement system.                                                      #
#                                                                              #
#     The purpose is to show how to do parameter learning using                #
#     Calibrate-Emulate-Sample in a simple (and highly artificial) setting.    #
#                                                                              #
#     For more information on Cloudy, see                                      #
#              https://github.com/CliMA/Cloudy.jl.git                          #
#                                                                              #
################################################################################


function main()

    rng_seed = 41
    Random.seed!(rng_seed)
    rng = Random.seed!(Random.GLOBAL_RNG, rng_seed)

    output_directory = joinpath(@__DIR__, "output")
    if !isdir(output_directory)
        mkdir(output_directory)
    end

    # The calibration results must be produced by running Cloudy_calibrate.jl
    # before running Cloudy_emulate_sample.jl
    data_save_file = joinpath(
        output_directory,
        "cloudy_calibrate_results.jld2"
    )

    # Check if the file exists before loading
    if isfile(data_save_file)

        ekiobj = load(data_save_file)["eki"]
        priors = load(data_save_file)["priors"]
        truth_sample_mean = load(data_save_file)["truth_sample_mean"]
        truth_sample = load(data_save_file)["truth_sample"]
        # True parameters:
        # - ϕ: in constrained space
        # - θ: in unconstrained space
        ϕ_true = load(data_save_file)["truth_input_constrained"]

    else
        error("File not found: $data_save_file. Please run 'Cloudy_calibrate.jl' first.")

    end

    θ_true = transform_constrained_to_unconstrained(priors, ϕ_true)
    param_names = get_name(priors)
    Γy = ekiobj.obs_noise_cov
    n_params = length(ϕ_true) # input dimension
    # Save data
    #@save joinpath(output_directory, "cloudy_input_output_pairs.jld2") input_output_pairs

    cases = [
        "rf-scalar",
        "gp-gpjl"  # Veeeery slow predictions
    ]

    # These settings are the same for all Gaussian Process cases
    pred_type = YType()

    # These settings are the same for all Random Feature cases
    n_features = 400
    nugget = 1e-8
    optimizer_options = Dict(
        "verbose" => true,
        "scheduler" => DataMisfitController(terminate_at = 100.0),
        "cov_sample_multiplier" => 1.0,
        "n_iteration" => 20,
    )

    # Specify cases to run (e.g., case_mask = [2] only runs the second case)
    case_mask = [1]
    println("case mask: ")
    println(case_mask)
    println(cases[case_mask])

    for case in cases[case_mask]

        println(" ")
        println("*********************************\n")
        @info "running case $case"

        if case == "gp-gpjl"

            @warn "gp-gpjl case is very slow at prediction"
            gppackage = GPJL()
            gp_kernel = SE(1.0, 1.0) + Mat52Ard(zeros(3), 0.0) + Noise(log(2.0))
            gaussian_process = GaussianProcess(
                gppackage;
                kernel = gp_kernel,
                prediction_type = pred_type,
                noise_learn = false,
            )

            input_output_pairs = get_training_points(ekiobj, length(get_u(ekiobj))-1)
            emulator = Emulator(
                gaussian_process,
                input_output_pairs,
                obs_noise_cov = Γy,
                normalize_inputs = true
            )

        elseif case == "rf-scalar"

            kernel_structure = SeparableKernel(
                LowRankFactor(n_params, nugget),
                OneDimFactor()
            )

            srfi = ScalarRandomFeatureInterface(
                n_features,
                n_params,
                kernel_structure = kernel_structure,
                optimizer_options = optimizer_options,
            )

            retained_svd_frac = 1.0
            min_iter = 1
            max_iter = 5 # number of EKP iterations to use data from is at most this
            @assert min_iter <= max_iter
            @assert max_iter <= length(get_u(ekiobj)) - 1
            @assert min_iter >= 1
            N_iter = min(max_iter, length(get_u(ekiobj)) - 1) # number of paired iterations taken from EKP
            # Get training points from the EKP iteration number in the second input term
            min_iter = min(max_iter, max(1, min_iter))
            input_output_pairs = get_training_points(ekiobj, min_iter:(N_iter - 1))
            input_output_pairs_test = get_training_points(
                ekiobj, N_iter:(length(get_u(ekiobj)) - 1)) # "next" iterations
            norm_factors = get_standardizing_factors(
                get_outputs(input_output_pairs)
            )

            emulator = Emulator(
                srfi,
                input_output_pairs;
                obs_noise_cov = Γy,
                normalize_inputs = true,
                standardize_outputs = true,
                standardize_outputs_factors = vcat(norm_factors...),
                retained_svd_frac = retained_svd_frac,
                decorrelate = true # use SVD to decorrelate outputs
            )

        else
            error("Case $case is not implemented yet.")

        end

        optimize_hyperparameters!(emulator)

        # Check how well the emulator predicts on the true parameters
        y_mean, y_var = Emulators.predict(
            emulator,
            reshape(θ_true, :, 1);
            transform_to_real = true
        )

        # Check how well the emulator predicts on the test input pairs (whose
        # corresponding output we know, as it was calculated during EKI)
        y_mean_test, y_var_test =
            Emulators.predict(
                emulator,
                get_inputs(input_output_pairs_test),
                transform_to_real = true
            )

        println("Emulator ($(case)) prediction on true parameters: ")
        println(vec(y_mean))
        println("true data: ")
        println(truth_sample) # what was used as truth
        println("Emulator ($(case)) predicted standard deviation")
        println(sqrt.(diag(y_var[1], 0)))
        println("Emulator ($(case)) MSE (truth): ")
        println(mean((truth_sample - vec(y_mean)) .^ 2))
        #println("Emulator ($(case)) MSE (next ensemble): ")
        #println(mean((get_outputs(input_output_pairs_test) - y_mean_test) .^ 2))

        ###
        ###  Sample: Markov Chain Monte Carlo
        ###

        # initial values
        u0 = vec(mean(get_inputs(input_output_pairs), dims = 2))
        println("initial parameters: ", u0)

        # First let's run a short chain to determine a good step size
        yt_sample = truth_sample
        mcmc = MCMCWrapper(
            RWMHSampling(),
            yt_sample,
            priors,
            emulator;
            init_params = u0
        )

        new_step = optimize_stepsize(
            mcmc;
            init_stepsize = 0.1,
            N = 2000,
            discard_initial = 0
        )

        # Now begin the actual MCMC
        println("Begin MCMC - with step size ", new_step)
        chain = MarkovChainMonteCarlo.sample(
            mcmc,
            100_000;
            stepsize = new_step,
            discard_initial = 1_000
        )

        posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)

        post_mean = mean(posterior)
        post_cov = cov(posterior)
        println("posterior mean")
        println(post_mean)
        println("posterior covariance")
        println(post_cov)

        # Plot the posteriors together with the priors and the true parameter
        # values (in the transformed/unconstrained space)
        n_params = length(get_name(posterior))

        gr(size = (800, 600))

        for idx in 1:n_params
            posterior_samples = dropdims(
                get_distribution(posterior)[param_names[idx]], dims = 1)

            # Find the range of the posterior samples
            xmin = minimum(posterior_samples)
            xmax = maximum(posterior_samples)
            xs = collect(range(xmin, stop = xmax, length = 1000))

            label = "true " * param_names[idx]
            histogram(
                posterior_samples,
                bins = 100,
                normed = true,
                fill = :slategray,
                thickness_scaling = 2.0,
                lab = "posterior",
                legend = :outertopright,
            )

            prior_dist = get_distribution(mcmc.prior)[param_names[idx]]
            plot!(xs, prior_dist, w = 2.6, color = :blue, lab = "prior")
            plot!([θ_true[idx]], seriestype = "vline", w = 2.6, lab = label)
            title!(param_names[idx])
            figname = "posterior_" * case * "_" * param_names[idx] * ".png"
            figpath = joinpath(output_directory, figname)
            StatsPlots.savefig(figpath)
            linkfig(figpath)

        end
    end
end


main()
