var documenterSearchIndex = {"docs":
[{"location":"examples/emulators/ishigami_3d_1d/#Global-Sensitiviy-Analysis-for-an-emulated-Ishigami-function","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"","category":"section"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"In this example, we assess directly the performance of our machine learning emulators. The task is to learn a function for use in a global sensitivity analysis. In particular, we learn the Ishigami function","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"f(x a b) = (1 + bx_3^4)sin(x_1) + a sin(x_2) forall xin -pipi^3","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"with a=7 b=01. In this example, global sensitivity analysis refers to calculation of two Sobol indices. The first index collects proportions V_i (a.k.a firstorder) of the variance of f attributable to the input x_i, and the second index collects proportions TV_i (a.k.a totalorder) of the residual variance having removed contributions attributable to inputs x_j forall jneq i. The Ishigami function has an analytic formula for these Sobol indices, it is also known that one can obtain numerical approximation through quasi-Monto-Carlo methods by evaluating the Ishigami function on a special quasi-random Sobol sequence.","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"To emulate the Ishigami function, the data consists of 300 pairs xf(x)+eta whereeta sim N(0Sigma) is additive noise, and the x are sampled from the Sobol sequence. The emulators are validated by evaluating the posterior mean function on the full 16000 points of the Sobol sequence and the Sobol indices are estimated. We rerun the experiment for many repeats of the random feature hyperparameter optimization and present the statistics of these indices, as well as plotting a realization of the emulator.","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"We use the package GlobalSensitivityAnalysis.jl for many of the GSA tools.","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/#Walkthrough-of-the-code","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"We first load some standard packages","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"using Distributions\nusing DataStructures # for `OrderedDict`\nusing Random\nusing LinearAlgebra\nusing CairoMakie, ColorSchemes ","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"and the packages for providing the Ishigami function, Sobol sequence, and evaluation of the indices","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"using GlobalSensitivityAnalysis # for `SobolData`\nconst GSA = GlobalSensitivityAnalysis","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"then the CES packages for the emulators","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"using CalibrateEmulateSample.Emulators # for `SKLJL`, `GaussianProcess`, `SeparableKernel`, `LowRankFactor`, `OneDimFactor`, `ScalarRandomFeatureInterface`, `Emulator`\nusing CalibrateEmulateSample.DataContainers # for `PairedDataContainer`\nusing CalibrateEmulateSample.EnsembleKalmanProcesses # for `DataMisfitController`","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"We set up the sampling procedure, evaluate the true ishigami function on these points, and calculate the sobol indices","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"n_data_gen = 2000 \n\ndata = SobolData(\n    params = OrderedDict(:x1 => Uniform(-π, π), :x2 => Uniform(-π, π), :x3 => Uniform(-π, π)),\n    N = n_data_gen,\n)\n\n# To perform global analysis,\n# one must generate samples using Sobol sequence (i.e. creates more than N points)\nsamples = GSA.sample(data)\nn_data = size(samples, 1) # [n_samples x 3]\n# run model (example)\ny = GSA.ishigami(samples)\n# perform Sobol Analysis\nresult = analyze(data, y)","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"Next we create the noisy training data from the sequence samples","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"n_train_pts = 300\nind = shuffle!(rng, Vector(1:n_data))[1:n_train_pts]\n# now subsample the samples data\nn_tp = length(ind)\ninput = zeros(3, n_tp)\noutput = zeros(1, n_tp)\nΓ = 1e-2\nnoise = rand(rng, Normal(0, Γ), n_tp)\nfor i in 1:n_tp\n    input[:, i] = samples[ind[i], :]\n    output[i] = y[ind[i]] + noise[i]\nend\niopairs = PairedDataContainer(input, output)","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"We have a couple of cases for the user to investigate","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"cases = [\n    \"Prior\", # Scalar random feature emulator with no hyperparameter learning\n    \"GP\", # Trained Sci-kit learn Gaussian process emulator\n    \"RF-scalar\", # Trained scalar random feature emulator\n]","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"Each case sets up a different machine learning configuration in the Emulator object.","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"For the random feature case, RF-scalar, we use a rank-3 kernel in the input space, and 500 features for prediction, though for efficiency we use only 200 when learning the hyperparameters. The relevant code snippets are","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"nugget = Float64(1e-12)\noverrides = Dict(\n    \"scheduler\" => DataMisfitController(terminate_at = 1e4),\n    \"n_features_opt\" => 200,\n)\n\nkernel_structure = SeparableKernel(LowRankFactor(3, nugget), OneDimFactor())\nn_features = 500\nmlt = ScalarRandomFeatureInterface(\n    n_features,\n    3,\n    rng = rng,\n    kernel_structure = kernel_structure,\n    optimizer_options = overrides,\n)","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"For the gaussian process case GP we use the sci-kit learn package, a default squared exponential kernel with lengthscale learnt in each input dimensions. We do not learn an additional white kernel for the noise.","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"gppackage = Emulators.SKLJL()\npred_type = Emulators.YType()\nmlt = GaussianProcess(\n    gppackage;\n    prediction_type = pred_type,\n    noise_learn = false,\n)","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"We finish by building the emulator object","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"emulator = Emulator(mlt, iopairs; obs_noise_cov = Γ * I, decorrelate = decorrelate)\noptimize_hyperparameters!(emulator) # (only needed for some Emulator packages)","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/#Results-and-plots","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Results and plots","text":"","category":"section"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"We validate the emulator by evaluating it on the entire Sobol sequence, and calculating the Sobol indices (presenting mean and std if using multiple repeats.","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"# predict on all Sobol points with emulator (example)    \ny_pred, y_var = predict(emulator, samples', transform_to_real = true)\n\n# obtain emulated Sobol indices\nresult_pred = analyze(data, y_pred')","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/#Gaussian-Process-Emulator-(sci-kit-learn-GP)","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Gaussian Process Emulator (sci-kit learn GP)","text":"","category":"section"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"Here is the plot for one emulation","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"<img src=\"../../../assets/ishigami_slices_GP.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"and the outputted table of Sobol indices","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"True Sobol Indices\n******************\n    firstorder: [0.31390519114781146, 0.44241114479004084, 0.0]\n    totalorder: [0.5575888552099592, 0.44241114479004084, 0.24368366406214775]\n \nSampled truth Sobol Indices (# points 16000)\n***************************\n    firstorder: [0.31261591941512257, 0.441887746224135, -0.005810964687365922]\n    totalorder: [0.5623611180844434, 0.44201284296404386, 0.24465876318633062]\n \nSampled Emulated Sobol Indices (# obs 300, noise var 0.01)\n***************************************************************\n    firstorder: [0.3094638183079643, 0.4518400892052567, -0.007351344957260407]\n    totalorder: [0.5502469909342245, 0.4587734278791574, 0.23542404141319245]","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/#Random-feature-emulator-(Separable-Low-rank-kernel-RF-scalar)","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Random feature emulator (Separable Low-rank kernel RF-scalar)","text":"","category":"section"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"Here is the plot for one emulation","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"<img src=\"../../../assets/ishigami_slices_RF-scalar.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"Table for 20 repeats of the algorithm","category":"page"},{"location":"examples/emulators/ishigami_3d_1d/","page":"Global Sensitiviy Analysis for an emulated Ishigami function","title":"Global Sensitiviy Analysis for an emulated Ishigami function","text":"True Sobol Indices\n******************\n    firstorder: [0.31390519114781146, 0.44241114479004084, 0.0]\n    totalorder: [0.5575888552099592, 0.44241114479004084, 0.24368366406214775]\n \nSampled truth Sobol Indices (# points 16000)\n***************************\n    firstorder: [0.31261591941512257, 0.441887746224135, -0.005810964687365922]\n    totalorder: [0.5623611180844434, 0.44201284296404386, 0.24465876318633062]\n \nSampled Emulated Sobol Indices (# obs 300, noise var 0.01)\n***************************************************************\n(mean) firstorder: [0.33605548545490044, 0.41116050093679196, -0.0012213648484969539]\n(std)  firstorder: [0.05909336956162543, 0.11484966121124164, 0.012908533302492602]\n(mean) totalorder: [0.5670345355855254, 0.4716028261179354, 0.24108222433317]\n(std)  totalorder: [0.10619345801872732, 0.1041023777237331, 0.07200225781785778]\n","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to CalibrateEmulateSample! We encourage opening issues and pull requests (PRs).","category":"page"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The easiest way to contribute is by using CalibrateEmulateSample, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of CalibrateEmulateSample insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition.","category":"page"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you are unfamiliar with git and version control, the following guides will be helpful:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Atlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"page"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Create your own fork of CalibrateEmulateSample on GitHub and check out your copy:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git clone https://github.com/<your-username>/CalibrateEmulateSample.jl.git\n$ cd CalibrateEmulateSample.jl","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Now you have access to your fork of CalibrateEmulateSample through origin. Create a branch for your feature; this will hold your contribution:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout -b <branchname>","category":"page"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"page"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"julia --project=.dev .dev/climaformat.jl .","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"from the CalibrateEmulateSample.jl directory.","category":"page"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with CalibrateEmulateSample.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To make sure you are up to date with main, you can use the following workflow:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To squash your commits, you can use the following command:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~n","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"where n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~4","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"will open the following file in (typically) vim:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Then in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git push -u origin <name_of_local_branch> --force","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can find more information about squashing here.","category":"page"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Currently a number of checks are run per commit for a given PR.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"JuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Unit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"page"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We ensure that all unit tests across several environments, Documentation builds, and integration tests (managed by Buildkite), pass before merging any PR into main. The integration tests currently run some of our example cases in examples/.","category":"page"},{"location":"API/AbstractMCMC/#AbstractMCMC-sampling-API","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The \"sample\" part of CES refers to exact sampling from the emulated posterior via Markov chain Monte Carlo (MCMC). Within this paradigm, we want to provide the flexibility to use multiple sampling algorithms; the approach we take is to use the general-purpose AbstractMCMC.jl API, provided by the Turing.jl probabilistic programming framework.","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"This page provides a summary of AbstractMCMC which augments the existing documentation ([1], [2]) and highlights how it's used by the CES package in MarkovChainMonteCarlo. It's not meant to be a complete description of the AbstractMCMC package.","category":"page"},{"location":"API/AbstractMCMC/#Use-in-MarkovChainMonteCarlo","page":"AbstractMCMC sampling API","title":"Use in MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"At present, Turing has limited support for derivative-free optimization, so we only use this abstract API and not Turing itself. We also use two related dependencies, AdvancedMH and MCMCChains. ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Julia's philosophy is to use small, composable packages rather than monoliths, but this can make it difficult to remember where methods are defined! Below we describe the relevant parts of ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The AbstractMCMC API,\nExtended to the case of Metropolis-Hastings (MH) sampling by AdvancedMH,\nFurther extended for the needs of CES in Markov chain Monte Carlo.","category":"page"},{"location":"API/AbstractMCMC/#Classes-and-methods","page":"AbstractMCMC sampling API","title":"Classes and methods","text":"","category":"section"},{"location":"API/AbstractMCMC/#Sampler","page":"AbstractMCMC sampling API","title":"Sampler","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"A Sampler is AbstractMCMC's term for an implementation of a MCMC sampling algorithm, along with all its configuration parameters. All samplers must inherit from AbstractMCMC.AbstractSampler. ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Currently CES only implements the Metropolis-Hastings (MH) algorithm. Because it's so straightforward, much of AbstractMCMC isn't needed. We implement two variants of MH with two different Samplers: RWMetropolisHastings and pCNMetropolisHastings, both of which inherit from the AdvancedMH.MHSampler base class. The public constructor for both Samplers is MetropolisHastingsSampler; the different Samplers are specified by passing a MCMCProtocol object to this constructor.","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The MH Sampler classes have only one field, proposal, which is the distribution used to generate new MH proposals via stochastic offsets to the current parameter values. This is done by AdvancedMH.propose(), which gets called for each MCMC step() (below). The difference between our two Samplers is in how this proposal is generated:","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"RWMHSampling does vanilla random-walk proposal generation with a constant, user-specified step size (this differs from the AdvancedMH implementation, which doesn't provide for a step size.)\npCNMHSampling for preconditioned Crank-Nicholson proposals. Vanilla random walk sampling doesn't have a well-defined limit for high-dimensional parameter spaces; pCN replaces the random walk with an Ornstein–Uhlenbeck [AR(1)] process so that the Metropolis acceptance probability remains non-zero in this limit. See Beskos et. al. (2008) and Cotter et. al. (2013).","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"This is the only difference: generated proposals are then either accepted or rejected according to the same MH criterion (in step(), below.)","category":"page"},{"location":"API/AbstractMCMC/#Models","page":"AbstractMCMC sampling API","title":"Models","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"In Turing, the Model is the distribution one performs inference on, which may involve observed and hidden variables and parameters. For CES, we simply want to sample from the posterior, so our Model distribution is simply the emulated likelihood (see Emulators) together with the prior. This is constructed by EmulatorPosteriorModel.","category":"page"},{"location":"API/AbstractMCMC/#Sampling-with-the-MCMC-Wrapper-object","page":"AbstractMCMC sampling API","title":"Sampling with the MCMC Wrapper object","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"At a high level, a Sampler and Model is all that's needed to do MCMC sampling. This is done by the sample method provided by AbstractMCMC (extending the method from BaseStats). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"To be more user-friendly, in CES we wrap the Sampler, Model and other necessary configuration into a MCMCWrapper object. The constructor for this object ensures that all its components are created consistently, and performs necessary bookkeeping, such as converting coordinates to the decorrelated basis. We extend sample with methods to use this object (that simply unpack its fields and call the appropriate method from AbstractMCMC.)","category":"page"},{"location":"API/AbstractMCMC/#Chain","page":"AbstractMCMC sampling API","title":"Chain","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The MCMCChain class is used to store the results of the MCMC sampling; the package provides simple diagnostics for visualization and diagnosing chain convergence.","category":"page"},{"location":"API/AbstractMCMC/#Internals:-Transitions","page":"AbstractMCMC sampling API","title":"Internals: Transitions","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Implementing MCMC involves defining states and transitions of a Markov process (whose stationary distribution is what we seek to sample from). AbstractMCMC's terminology is a bit confusing for the MH case; states of the chain are described by Transition objects, which contain the current sample (and other information like its log-probability). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"AdvancedMH defines an AbstractTransition base class for use with its methods; we implement our own child class, MCMCState, in order to record statistics on the MH acceptance ratio.","category":"page"},{"location":"API/AbstractMCMC/#Internals:-Markov-steps","page":"AbstractMCMC sampling API","title":"Internals: Markov steps","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Markov transitions of the chain are defined by overloading AbstractMCMC's step method, which takes the Sampler and current Transition and implements the Sampler's logic to returns an updated Transition representing the chain's new state (actually, a pair of Transitions, for cases where the Sampler doesn't obey detailed balance; this isn't relevant for us). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"For example, in Metropolis-Hastings sampling this is where we draw a proposal sample and accept or reject it according to the MH criterion. AdvancedMH implements this here; we re-implement this method because 1) we need to record whether a proposal was accepted or rejected, and 2) our calls to propose() are stepsize-dependent.","category":"page"},{"location":"installation_instructions/#Installation-Instructions","page":"Installation instructions","title":"Installation Instructions","text":"","category":"section"},{"location":"installation_instructions/#Installing-CalibrateEmulateSample.jl","page":"Installation instructions","title":"Installing CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Currently CalibrateEmulateSample (CES) depends on some external python dependencies  including scikit-learn wrapped by ScikitLearn.jl, which requires a couple extra  installation steps:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"First clone the project into a new local repository","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> git clone git@github.com:Clima/CalibrateEmulateSample.jl\n> cd CalibrateEmulateSample.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Install and build the project dependencies. Given that CES depends on python packages  it is easiest to set the project to use its own  Conda environment variable (set by exporting the ENV variable PYTHON=\"\").","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The scikit-learn package (along with scipy) then has to be installed if using a Julia project-specific Conda environment:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scipy=1.8.1\", channel=\"conda-forge\")'\n> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scikit-learn=1.1.1\")'\n","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"See the PyCall.jl documentation  for more information about how to configure the local Julia / Conda / Python environment. Typically it will require building in the  REPL via","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project\njulia> using Pkg\njulia> Pkg.build(\"PyCall\")","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To test that the package is working:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You need to first build the top-level project before building the documentation:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"cd CalibrateEmulateSample.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Then you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project=docs/ -e 'using Pkg; Pkg.instantiate()'\njulia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html.","category":"page"},{"location":"API/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"Modules = [CalibrateEmulateSample.Utilities]\nOrder   = [:module, :type, :function]","category":"page"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_obs_sample-Union{Tuple{IT}, Tuple{Random.AbstractRNG, EnsembleKalmanProcesses.Observations.Observation}} where IT<:Int64","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_obs_sample","text":"get_obs_sample(\n    rng::Random.AbstractRNG,\n    obs::EnsembleKalmanProcesses.Observations.Observation;\n    rng_seed\n) -> Any\n\n\nReturn a random sample from the observations, for use in the MCMC.\n\nrng - optional RNG object used to pick random sample; defaults to Random.GLOBAL_RNG.\nobs - Observation struct with the observations (extract will pick one         of the sample observations to train).\nrng_seed - optional kwarg; if provided, used to re-seed rng before sampling.\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_training_points-Union{Tuple{P}, Tuple{IT}, Tuple{FT}, Tuple{EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, Union{AbstractVector{IT}, IT}}} where {FT, IT, P}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_training_points","text":"get_training_points(\n    ekp::EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P},\n    train_iterations::Union{AbstractVector{IT}, IT} where IT\n) -> EnsembleKalmanProcesses.DataContainers.PairedDataContainer\n\n\nExtract the training points needed to train the Gaussian process regression.\n\nekp - EnsembleKalmanProcess holding the parameters and the data that were produced during the Ensemble Kalman (EK) process.\ntrain_iterations - Number (or indices) EK layers/iterations to train on.\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.posdef_correct-Tuple{AbstractMatrix}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.posdef_correct","text":"posdef_correct(mat::AbstractMatrix; tol) -> Any\n\n\nMakes square matrix mat positive definite, by symmetrizing and bounding the minimum eigenvalue below by tol\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#RandomFeatures","page":"Random Features","title":"RandomFeatures","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/RandomFeatures/#Kernel-and-Covariance-structure","page":"Random Features","title":"Kernel and Covariance structure","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"OneDimFactor\nDiagonalFactor\nCholeskyFactor\nLowRankFactor\nHierarchicalLowRankFactor\nSeparableKernel\nNonseparableKernel\ncalculate_n_hyperparameters\nhyperparameters_from_flat\nbuild_default_prior","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.OneDimFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.OneDimFactor","text":"struct OneDimFactor <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\ncovariance structure for a one-dimensional space\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.DiagonalFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.DiagonalFactor","text":"struct DiagonalFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a diagonal covariance structure\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.CholeskyFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.CholeskyFactor","text":"struct CholeskyFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a general positive-definite covariance structure\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.LowRankFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.LowRankFactor","text":"struct LowRankFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a covariance structure that deviates from the identity with a low-rank perturbation. This perturbation is diagonalized in the low-rank space\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.HierarchicalLowRankFactor","page":"Random Features","title":"CalibrateEmulateSample.Emulators.HierarchicalLowRankFactor","text":"struct HierarchicalLowRankFactor{FT<:AbstractFloat} <: CalibrateEmulateSample.Emulators.CovarianceStructureType\n\nbuilds a covariance structure that deviates from the identity with a more general low-rank perturbation\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.SeparableKernel","page":"Random Features","title":"CalibrateEmulateSample.Emulators.SeparableKernel","text":"struct SeparableKernel{CST1<:CalibrateEmulateSample.Emulators.CovarianceStructureType, CST2<:CalibrateEmulateSample.Emulators.CovarianceStructureType} <: CalibrateEmulateSample.Emulators.KernelStructureType\n\nBuilds a separable kernel, i.e. one that accounts for input and output covariance structure separately\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.NonseparableKernel","page":"Random Features","title":"CalibrateEmulateSample.Emulators.NonseparableKernel","text":"struct NonseparableKernel{CST<:CalibrateEmulateSample.Emulators.CovarianceStructureType} <: CalibrateEmulateSample.Emulators.KernelStructureType\n\nBuilds a nonseparable kernel, i.e. one that accounts for a joint input and output covariance structure\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.calculate_n_hyperparameters","page":"Random Features","title":"CalibrateEmulateSample.Emulators.calculate_n_hyperparameters","text":"calculate_n_hyperparameters(\n    d::Int64,\n    odf::CalibrateEmulateSample.Emulators.OneDimFactor\n) -> Int64\n\n\ncalculates the number of hyperparameters generated by the choice of covariance structure\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.hyperparameters_from_flat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.hyperparameters_from_flat","text":"hyperparameters_from_flat(\n    x::AbstractVector,\n    odf::CalibrateEmulateSample.Emulators.OneDimFactor\n)\n\n\nreshapes a list of hyperparameters into a covariance matrix based on the selected structure\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_default_prior","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_default_prior","text":"build_default_prior(\n    name::AbstractString,\n    n_hp::Int64,\n    odf::CalibrateEmulateSample.Emulators.OneDimFactor\n)\n\n\nbuilds a prior distribution for the kernel hyperparameters to initialize optimization.\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#Scalar-interface","page":"Random Features","title":"Scalar interface","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"ScalarRandomFeatureInterface\nScalarRandomFeatureInterface(::Int,::Int)\nbuild_models!(::ScalarRandomFeatureInterface, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict(::ScalarRandomFeatureInterface, ::M) where {M <: AbstractMatrix}","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","page":"Random Features","title":"CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","text":"struct ScalarRandomFeatureInterface{S<:AbstractString, RNG<:Random.AbstractRNG, KST<:CalibrateEmulateSample.Emulators.KernelStructureType} <: CalibrateEmulateSample.Emulators.RandomFeatureInterface\n\nStructure holding the Scalar Random Feature models. \n\nFields\n\nrfms::Vector{RandomFeatures.Methods.RandomFeatureMethod}: vector of RandomFeatureMethods, contains the feature structure, batch-sizes and regularization\nfitted_features::Vector{RandomFeatures.Methods.Fit}: vector of Fits, containing the matrix decomposition and coefficients of RF when fitted to data\nbatch_sizes::Union{Nothing, Dict{S, Int64}} where S<:AbstractString: batch sizes\nn_features::Union{Nothing, Int64}: n_features\ninput_dim::Int64: input dimension\nrng::Random.AbstractRNG: choice of random number generator\nkernel_structure::CalibrateEmulateSample.Emulators.KernelStructureType: Kernel structure type (e.g. Separable or Nonseparable)\nfeature_decomposition::AbstractString: Random Feature decomposition, choose from \"svd\" or \"cholesky\" (default)\noptimizer_options::Dict{S} where S<:AbstractString: dictionary of options for hyperparameter optimizer\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface-Tuple{Int64, Int64}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","text":"ScalarRandomFeatureInterface(\n    n_features::Int64,\n    input_dim::Int64;\n    kernel_structure,\n    batch_sizes,\n    rng,\n    feature_decomposition,\n    optimizer_options\n) -> CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface{String, Random._GLOBAL_RNG}\n\n\nConstructs a ScalarRandomFeatureInterface <: MachineLearningTool interface for the RandomFeatures.jl package for multi-input and single- (or decorrelated-)output emulators.\n\nn_features - the number of random features\ninput_dim - the dimension of the input space\nkernel_structure -  - a prescribed form of kernel structure\nbatch_sizes = nothing - Dictionary of batch sizes passed RandomFeatures.jl object (see definition there)\nrng = Random.GLOBAL_RNG - random number generator \nfeature_decomposition = \"cholesky\" - choice of how to store decompositions of random features, cholesky or svd available\noptimizer_options = nothing - Dict of options to pass into EKI optimization of hyperparameters (defaults created in ScalarRandomFeatureInterface constructor):\n\"prior\":  the prior for the hyperparameter optimization \n\"priorinscale\": use this to tune the input prior scale\n\"n_ensemble\":  number of ensemble members\n\"n_iteration\":  number of eki iterations\n\"covsamplemultiplier\": increase for more samples to estimate covariance matrix in optimization (default 10.0, minimum 0.0)  \n\"scheduler\": Learning rate Scheduler (a.k.a. EKP timestepper) Default: DataMisfitController\n\"tikhonov\":  tikhonov regularization parameter if >0\n\"inflation\":  additive inflation ∈ [0,1] with 0 being no inflation\n\"train_fraction\":  e.g. 0.8 (default)  means 80:20 train - test split\n\"multithread\": how to multithread. \"ensemble\" (default) threads across ensemble members \"tullio\" threads random feature matrix algebra\n\"verbose\" => false, verbose optimizer statements\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat}\n)\n\n\nBuilds the random feature method from hyperparameters. We use cosine activation functions and a Multivariate Normal distribution (from Distributions.jl) with mean M=0, and input covariance U built with the CovarianceStructureType.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#GaussianProcesses.predict-Union{Tuple{M}, Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface, M}} where M<:(AbstractMatrix)","page":"Random Features","title":"GaussianProcesses.predict","text":"predict(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    new_inputs::AbstractMatrix;\n    multithread\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#Vector-Interface","page":"Random Features","title":"Vector Interface","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"VectorRandomFeatureInterface\nVectorRandomFeatureInterface(::Int, ::Int, ::Int)\nbuild_models!(::VectorRandomFeatureInterface, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict(::VectorRandomFeatureInterface, ::M) where {M <: AbstractMatrix}","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","page":"Random Features","title":"CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","text":"struct VectorRandomFeatureInterface{S<:AbstractString, RNG<:Random.AbstractRNG, KST<:CalibrateEmulateSample.Emulators.KernelStructureType} <: CalibrateEmulateSample.Emulators.RandomFeatureInterface\n\nStructure holding the Vector Random Feature models. \n\nFields\n\nrfms::Vector{RandomFeatures.Methods.RandomFeatureMethod}: A vector of RandomFeatureMethods, contains the feature structure, batch-sizes and regularization\nfitted_features::Vector{RandomFeatures.Methods.Fit}: vector of Fits, containing the matrix decomposition and coefficients of RF when fitted to data\nbatch_sizes::Union{Nothing, Dict{S, Int64}} where S<:AbstractString: batch sizes\nn_features::Union{Nothing, Int64}: number of features\ninput_dim::Int64: input dimension\noutput_dim::Int64: output_dimension\nrng::Random.AbstractRNG: rng\nregularization::Vector{Union{LinearAlgebra.Diagonal, LinearAlgebra.UniformScaling, Matrix}}: regularization\nkernel_structure::CalibrateEmulateSample.Emulators.KernelStructureType: Kernel structure type (e.g. Separable or Nonseparable)\nfeature_decomposition::AbstractString: Random Feature decomposition, choose from \"svd\" or \"cholesky\" (default)\noptimizer_options::Dict: dictionary of options for hyperparameter optimizer\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface-Tuple{Int64, Int64, Int64}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","text":"VectorRandomFeatureInterface(\n    n_features::Int64,\n    input_dim::Int64,\n    output_dim::Int64;\n    kernel_structure,\n    batch_sizes,\n    rng,\n    feature_decomposition,\n    optimizer_options\n) -> CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface{String, Random._GLOBAL_RNG}\n\n\nConstructs a VectorRandomFeatureInterface <: MachineLearningTool interface for the RandomFeatures.jl package for multi-input and multi-output emulators.\n\nn_features - the number of random features\ninput_dim - the dimension of the input space\noutput_dim - the dimension of the output space\nkernel_structure -  - a prescribed form of kernel structure\nbatch_sizes = nothing - Dictionary of batch sizes passed RandomFeatures.jl object (see definition there)\nrng = Random.GLOBAL_RNG - random number generator \nfeature_decomposition = \"cholesky\" - choice of how to store decompositions of random features, cholesky or svd available\noptimizer_options = nothing - Dict of options to pass into EKI optimization of hyperparameters (defaults created in VectorRandomFeatureInterface constructor):\n\"prior\": the prior for the hyperparameter optimization\n\"priorinscale\"/\"prioroutscale\": use these to tune the input/output prior scale.\n\"n_ensemble\": number of ensemble members\n\"n_iteration\": number of eki iterations\n\"scheduler\": Learning rate Scheduler (a.k.a. EKP timestepper) Default: DataMisfitController\n\"covsamplemultiplier\": increase for more samples to estimate covariance matrix in optimization (default 10.0, minimum 0.0) \n\"tikhonov\": tikhonov regularization parameter if > 0\n\"inflation\": additive inflation ∈ [0,1] with 0 being no inflation\n\"train_fraction\": e.g. 0.8 (default)  means 80:20 train - test split\n\"multithread\": how to multithread. \"ensemble\" (default) threads across ensemble members \"tullio\" threads random feature matrix algebra\n\"verbose\" => false, verbose optimizer statements to check convergence, priors and optimal parameters.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat};\n    regularization_matrix\n) -> Vector{Union{LinearAlgebra.Diagonal, LinearAlgebra.UniformScaling, Matrix}}\n\n\nBuild Vector Random Feature model for the input-output pairs subject to regularization, and optimizes the hyperparameters with EKP. \n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#GaussianProcesses.predict-Union{Tuple{M}, Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface, M}} where M<:(AbstractMatrix)","page":"Random Features","title":"GaussianProcesses.predict","text":"predict(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    new_inputs::AbstractMatrix\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#Other-utilities","page":"Random Features","title":"Other utilities","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"get_rfms\nget_fitted_features\nget_batch_sizes\nget_n_features\nget_input_dim\nget_output_dim\nget_rng\nget_kernel_structure\nget_feature_decomposition\nget_optimizer_options\noptimize_hyperparameters!(::ScalarRandomFeatureInterface) \noptimize_hyperparameters!(::VectorRandomFeatureInterface) \nshrinkage_cov","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_rfms","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_rfms","text":"get_rfms(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.RandomFeatureMethod}\n\n\ngets the rfms field\n\n\n\n\n\nget_rfms(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.RandomFeatureMethod}\n\n\nGets the rfms field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_fitted_features","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_fitted_features","text":"get_fitted_features(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.Fit}\n\n\ngets the fitted_features field\n\n\n\n\n\nget_fitted_features(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.Fit}\n\n\nGets the fitted_features field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_batch_sizes","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_batch_sizes","text":"get_batch_sizes(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Union{Nothing, Dict{S, Int64}} where S<:AbstractString\n\n\ngets batch_sizes the field\n\n\n\n\n\nget_batch_sizes(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Union{Nothing, Dict{S, Int64}} where S<:AbstractString\n\n\nGets the batch_sizes field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_n_features","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_n_features","text":"get_n_features(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Union{Nothing, Int64}\n\n\ngets the n_features field\n\n\n\n\n\nget_n_features(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Union{Nothing, Int64}\n\n\nGets the n_features field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_input_dim","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_input_dim","text":"get_input_dim(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Int64\n\n\ngets the input_dim field\n\n\n\n\n\nget_input_dim(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Int64\n\n\nGets the input_dim field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_output_dim","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_output_dim","text":"get_output_dim(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Int64\n\n\nGets the output_dim field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_rng","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_rng","text":"get_rng(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Random.AbstractRNG\n\n\ngets the rng field\n\n\n\n\n\nget_rng(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Random.AbstractRNG\n\n\nGets the rng field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_kernel_structure","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_kernel_structure","text":"get_kernel_structure(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> CalibrateEmulateSample.Emulators.KernelStructureType\n\n\nGets the kernel_structure field\n\n\n\n\n\nget_kernel_structure(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> CalibrateEmulateSample.Emulators.KernelStructureType\n\n\nGets the kernel_structure field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_feature_decomposition","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_feature_decomposition","text":"get_feature_decomposition(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> AbstractString\n\n\ngets the feature_decomposition field\n\n\n\n\n\nget_feature_decomposition(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> AbstractString\n\n\nGets the feature_decomposition field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_optimizer_options","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_optimizer_options","text":"get_optimizer_options(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Dict{S} where S<:AbstractString\n\n\ngets the optimizer_options field\n\n\n\n\n\nget_optimizer_options(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Dict\n\n\nGets the optimizer_options field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    args...;\n    kwargs...\n)\n\n\nEmpty method, as optimization takes place within the build_models stage\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    args...;\n    kwargs...\n)\n\n\nEmpty method, as optimization takes place within the build_models stage\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.shrinkage_cov","page":"Random Features","title":"CalibrateEmulateSample.Emulators.shrinkage_cov","text":"shrinkage_cov(sample_mat::AbstractMatrix) -> Any\n\n\nCalculate the empirical covariance, additionally applying a shrinkage operator (here the Ledoit Wolf 2004 shrinkage operation). Known to have better stability properties than Monte-Carlo for low sample sizes\n\n\n\n\n\n","category":"function"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in CalibrateEmulateSample.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.","category":"page"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"Name Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta θ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-example","page":"Lorenz example","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We provide the following template for how the tools may be applied.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"For small examples typically have 2 files.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"GModel.jl Contains the forward map. The inputs should be the so-called free parameters we are interested in learning, and the output should be the measured data\nThe example script which contains the inverse problem setup and solve","category":"page"},{"location":"examples/lorenz_example/#The-structure-of-the-example-script","page":"Lorenz example","title":"The structure of the example script","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"First we create the data and the setting for the model","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Set up the forward model.\nConstruct/load the truth data. Store this data conveniently in the Observations.Observation object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we set up the inverse problem","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Define the prior distributions. Use the ParameterDistribution object\nDecide on which process tool you would like to use (we recommend you begin with Invesion()). Then initialize this with the relevant constructor\ninitialize the EnsembleKalmanProcess object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we solve the inverse problem, in a loop perform the following for as many iterations as required:","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Obtain the current parameter ensemble\nTransform them from the unbounded computational space to the physical space\ncall the forward map on the ensemble of parameters, producing an ensemble of measured data\ncall the update_ensemble! function to generate a new parameter ensemble based on the new data","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"One can then obtain the solution, dependent on the process type.","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Regression-of-\\mathbb{R}2-\\to-\\mathbb{R}2-smooth-function","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"In this example, we assess directly the performance of our machine learning emulators. The task is to learn the function:","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"Gcolon 02pi^2 to mathbbR^2 G(x_1x_2) = (sin(x_1) + cos(x_2) sin(x_1) - cos(x_2)) ","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"observed at 150 points, subject to additive (and possibly correlated) Gaussian noise N(0Sigma).","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We have several different emulator configurations in this example that the user can play around with. The goal of this example is to predict the function (i.e. posterior mean) and uncertainty (i.e posterior pointwise variance) on a 200times200 grid providing a mean square error between emulated and true function and  with plot_flag = true we also save several plots.","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We will term scalar-output Gaussin process emulator as \"GP\", scalar-output random feature emulator as \"scalar RF\", and vector-output random feature emulator as \"vector RF\" henceforth.","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Walkthrough-of-the-code","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We first import some standard packages","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"using Random\nusing StableRNGs\nusing Distributions\nusing Statistics\nusing LinearAlgebra","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"and relevant CES packages needed to define the emulators, packages and kernel structures","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"using CalibrateEmulateSample.Emulators\n# Contains `Emulator`, `GaussianProcess`, `ScalarRandomFeatureInterface`, `VectorRandomFeatureInterface`\n# `GPJL`, `SKLJL`, `SeparablKernel`, `NonSeparableKernel`, `OneDimFactor`, `LowRankFactor`, `DiagonalFactor`\nusing CalibrateEmulateSample.DataContainers # Contains `PairedDataContainer`","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"To play with the hyperparameter optimization of RF, the optimizer options sometimes require EnsembleKalmanProcesses.jl structures, so we load this too","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"using CalibrateEmulateSample.EnsembleKalmanProcesses # Contains `DataMisfitController`","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We have 9 cases that the user can toggle or customize","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"cases = [\n    \"gp-skljl\",\n    \"gp-gpjl\", # Very slow prediction...\n    \"rf-scalar\",\n    \"rf-svd-diag\",\n    \"rf-svd-nondiag\",\n    \"rf-nosvd-diag\",\n    \"rf-nosvd-nondiag\",\n    \"rf-svd-nonsep\",\n    \"rf-nosvd-nonsep\",\n]","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"The first two are for GP with either ScikitLearn.jl or GaussianProcesses.jl interface. The third is for the scalar RF interface, which most closely follows exactly replacing a GP. The rest are examples of vector RF with different types of data processing, (svd = same processing as scalar RF, nosvd = unprocessed) and different RF kernel structures in the output space of increasing complexity/flexibility (diag = Separable diagonal, nondiag = Separable nondiagonal, nonsep = nonseparable nondiagonal).","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We set up the learning problem specification, defining input and output dimensions, and number of data to train on, and the function g and the perturbed samples y with correlated additive noise","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"n = 150  # number of training points\np = 2   # input dim \nd = 2   # output dim\nX = 2.0 * π * rand(p, n)\n# G(x1, x2)\ng1x = sin.(X[1, :]) .+ cos.(X[2, :])\ng2x = sin.(X[1, :]) .- cos.(X[2, :])\ngx = zeros(2, n)\ngx[1, :] = g1x\ngx[2, :] = g2x\n# Add noise η\nμ = zeros(d)\nΣ = 0.1 * [[0.8, 0.1] [0.1, 0.5]] # d x d\nnoise_samples = rand(MvNormal(μ, Σ), n)\n# y = G(x) + η\nY = gx .+ noise_samples","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We then enter this in a paired data container, which gives a standard of how the data will be read","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"iopairs = PairedDataContainer(X, Y, data_are_columns = true)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We define some common settings for all emulators, e.g. the number of random features to use, and some hyperparameter optimizer options","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# common Gaussian feature setup\npred_type = YType()\n\n# common random feature setup\nn_features = 150\noptimizer_options = Dict(\"n_iteration\" => 10, \"scheduler\" => DataMisfitController(on_terminate = \"continue\"))\nnugget = 1e-12","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We then build the emulators. An example for GP (gp-skljl)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# use scikit learn\ngppackage = SKLJL()\n# build a GP that learns an additional white noise kernel (along with the default RBF kernel)\ngaussian_process = GaussianProcess(gppackage, noise_learn = true)\n# the data processing normalizes input data, and decorrelates output data with information from Σ\nemulator = Emulator(gaussian_process, iopairs, obs_noise_cov = Σ, normalize_inputs = true) ","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"An example for scalar RF (rf-scalar)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# build a scalar RF with a rank-2 kernel in input space (placeholder 1D kernel in output space) and use the optimizer options during training\nsrfi = ScalarRandomFeatureInterface(\n    n_features, \n    p, \n    kernel_structure = SeparableKernel(LowRankFactor(2, nugget), OneDimFactor()), \n    optimizer_options = optimizer_options,\n)\n# the data processing normalizes input data, and decorrelates output data with information from Σ\nemulator = Emulator(srfi, iopairs, obs_noise_cov = Σ, normalize_inputs = true)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"An example for vector RF (rf-nosvd-nonsep)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"# build a vector RF with a rank-4 nonseparable kernel and use the optimizer options during training\nvrfi = VectorRandomFeatureInterface(\n    n_features,\n    p,\n    d, # additionally provide the output dimensions size\n    kernel_structure = NonseparableKernel(LowRankFactor(4, nugget)),\n    optimizer_options = optimizer_options,\n)\n# the data processing normalizes input data, and does not decorrelate outputs\nemulator = Emulator(vrfi, iopairs, obs_noise_cov = Σ, normalize_inputs = true, decorrelate = false)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"For RF and some GP packages, the training occurs during construction of the Emulator, however sometimes one must call an optimize step afterwards","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Validation-and-Plots","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Validation and Plots","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We create an evaluation grid for our models, in the right shape:","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"n_pts = 200\nx1 = range(0.0, stop = 2 * π, length = n_pts)\nx2 = range(0.0, stop = 2 * π, length = n_pts)\nX1, X2 = meshgrid(x1, x2)\ninputs = permutedims(hcat(X1[:], X2[:]), (2, 1))","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We predict using the emulators at the new inputs, and transform_to_real inverts the data processing back to physical values","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"em_mean, em_cov = predict(emulator, inputs, transform_to_real = true)","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"We then plot the predicted mean and pointwise variances, and calculate the errors from the three highlighted cases:","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Gaussian-Process-Emulator-(Sci-kit-learn:-gp-skljl)","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Gaussian Process Emulator (Sci-kit learn: gp-skljl)","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"L^2 error of mean and latent truth:0.0008042391077774167","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"<img src=\"../../../assets/regression2d2d-gp-skljl_y1_predictions.png\" width=\"600\">\n<img src=\"../../../assets/regression2d2d-gp-skljl_y2_predictions.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Random-Feature-Emulator-(rf-scalar)","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Random Feature Emulator (rf-scalar)","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"L^2 error of mean and latent truth:0.0012253119679379056","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"<img src=\"../../../assets/regression2d2d-rf-scalar_y1_predictions.png\" width=\"600\">\n<img src=\"../../../assets/regression2d2d-rf-scalar_y2_predictions.png\" width=\"600\">","category":"page"},{"location":"examples/emulators/regression_2d_2d/#Random-Feature-Emulator-(vector:-rf-nosvd-nonsep)","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Random Feature Emulator (vector: rf-nosvd-nonsep)","text":"","category":"section"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"L^2 error of mean and latent truth:0.0011094292509180393","category":"page"},{"location":"examples/emulators/regression_2d_2d/","page":"Regression of mathbbR^2 to mathbbR^2 smooth function","title":"Regression of mathbbR^2 to mathbbR^2 smooth function","text":"<img src=\"../../../assets/regression2d2d-rf-nosvd-nonsep_y1_predictions.png\" width=\"600\">\n<img src=\"../../../assets/regression2d2d-rf-nosvd-nonsep_y2_predictions.png\" width=\"600\">","category":"page"},{"location":"GaussianProcessEmulator/#Gaussian-Process-Emulator","page":"Gaussian Process","title":"Gaussian Process Emulator","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"One type of MachineLearningTool we provide for emulation is a Gaussian process. Gaussian processes are a generalization of the Gaussian probability distribution, extended to functions rather than random variables. They can be used for statistical emulation, as they provide both mean and covariances. To build a Gaussian process, we first define a prior over all possible functions, by choosing the covariance function or kernel. The kernel describes how similar two outputs (y_i, y_j) are, given the similarities between their input values (x_i, x_j). Kernels encode the functional form of these relationships and are defined by hyperparameters, which are usually initially unknown to the user. To learn the posterior Gaussian process, we condition on data using Bayes theorem and optimize the hyperparameters of the kernel.  Then, we can make predictions to predict a mean function and covariance for new data points.","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"A useful resource to learn about Gaussian processes is Rasmussen and Williams (2006).","category":"page"},{"location":"GaussianProcessEmulator/#User-Interface","page":"Gaussian Process","title":"User Interface","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"CalibrateEmulateSample.jl allows the Gaussian process emulator to be built using either GaussianProcesses.jl  or ScikitLearn.jl. To use GaussianProcesses.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.GPJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"To use ScikitLearn.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.SKLJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Initialize a basic Gaussian Process with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(gppackage)","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"This initializes the prior Gaussian process.  We train the Gaussian process by feeding the gauss_proc alongside the data into the Emulator struct and optimizing the hyperparameters, described here.","category":"page"},{"location":"GaussianProcessEmulator/#Prediction-Type","page":"Gaussian Process","title":"Prediction Type","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can specify the type of prediction when initializing the Gaussian Process emulator. The default type of prediction is to predict data, YType().  You can create a latent function type prediction with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage,\n    prediction_type = FType())\n","category":"page"},{"location":"GaussianProcessEmulator/#Kernels","page":"Gaussian Process","title":"Kernels","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"The Gaussian process above assumes the default kernel: the Squared Exponential kernel, also called  the Radial Basis Function (RBF).  A different type of kernel can be specified when the Gaussian process is initialized.  Read more about kernel options here.","category":"page"},{"location":"GaussianProcessEmulator/#GPJL","page":"Gaussian Process","title":"GPJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"For the GaussianProcess.jl package, there are a range of kernels to choose from.  For example, ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nmy_kernel = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You do not need to provide useful hyperparameter values when you define the kernel, as these are learned in  optimize_hyperparameters!(emulator).","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine kernels together through linear operations, for example,","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nkernel_1 = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\nkernel_2 = GaussianProcesses.Lin(0.)               # Create a linear kernel with lengthscale=0\nmy_kernel = kernel_1 + kernel_2                    # Create a new additive kernel\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/#SKLJL","page":"Gaussian Process","title":"SKLJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Alternatively if you are using the ScikitLearn.jl package, you can find the list of kernels here.  These need this preamble:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using PyCall\nusing ScikitLearn\nconst pykernels = PyNULL()\nfunction __init__()\n    copy!(pykernels, pyimport(\"sklearn.gaussian_process.kernels\"))\nend","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Then they are accessible, for example, as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"my_kernel = pykernels.RBF(length_scale = 1)\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine multiple ScikitLearn kernels via linear operations in the same way as above.","category":"page"},{"location":"GaussianProcessEmulator/#Learning-the-noise","page":"Gaussian Process","title":"Learning the noise","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Often it is useful to learn the noise of the data by adding a white noise kernel. This is added with the  Boolean keyword noise_learn when initializing the Gaussian process. The default is true. ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = true )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"When noise_learn is true, an additional white noise kernel is added to the kernel. This white noise is present across all parameter values, including the training data.  The scale parameters of the white noise kernel are learned in optimize_hyperparameters!(emulator). ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You may not need to learn the noise if you already have a good estimate of the noise from your training data.  When noise_learn is false, additional regularization is added for stability. The default value is 1e-3 but this can be chosen through the optional argument alg_reg_noise:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = false,\n    alg_reg_noise = 1e-3 )","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Integrating-Lorenz-63-with-an-emulated-integrator","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"In this example, we assess directly the performance of our machine learning emulators. The task is to learn the forward Euler integrator of a Lorenz 63 system. The model parameters are set to their classical settings (sigma rho beta) = (1028frac83) to exhibit chaotic behavior. The discrete system is given as:","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"beginaligned\nx(t_n+1) = x(t_n) + Delta t(y(t_n) - x(t_n))\ny(t_n+1) = y(t_n) + Delta t(x(t_n)(28 - z(t_n)) - y(t_n))\nz(t_n+1) = z(t_n) + Delta t(x(t_n)y(t_n) - frac83z(t_n))\nendaligned","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"where t_n = nDelta t. The data consists of pairs  (x(t_k)y(t_k)z(t_k)) (x(t_k+1)y(t_k+1)z(t_k+1)+eta_k for 600 values of k, with each output subjected to independent, additive Gaussian noise eta_ksim N(0Gamma_y).","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We have several different emulator configurations in this example that the user can play around with. The goal of the emulator is that the posterior mean will approximate this discrete update map, or integrator, for any point on the Lorenz attractor from the sparse noisy data. To validate this, we recursively apply the trained emulator to the state, plotting the evolution of the trajectory and marginal statistics of the states over short and long timescales. We include a repeats option (n_repeats) to run the randomized training for multiple trials and illustrate robustness of marginal statistics by plotting long time marginal cdfs of the state. ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We will term scalar-output Gaussin process emulator as \"GP\", and scalar- or vector-output random feature emulator as \"RF\".","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Walkthrough-of-the-code","page":"Integrating Lorenz 63 with an emulated integrator","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We first import some standard packages","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using Random, Distributions, LinearAlgebra # utilities\nusing CairoMakie, ColorSchemes # for plots\nusing JLD2 # for saved data","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For the true integrator of the Lorenz system we import","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using OrdinaryDiffEq ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For relevant CES packages needed to define the emulators, packages and kernel structures","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using CalibrateEmulateSample.Emulators\n# Contains `Emulator`, `GaussianProcess`, `ScalarRandomFeatureInterface`, `VectorRandomFeatureInterface`\n# `GPJL`, `SeparablKernel`, `NonSeparableKernel`, `OneDimFactor`, `LowRankFactor`, `DiagonalFactor`\nusing CalibrateEmulateSample.DataContainers # Contains `PairedDataContainer`","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"and if one wants to play with optimizer options for the random feature emulators we import","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"using CalibrateEmulateSample.EnsembleKalmanProcesses # Contains `DataMisfitController`","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We generate the truth data using OrdinaryDiffEq: the time series sol is used for training data, sol_test is used for plotting short time trajectories, and sol_hist for plotting histograms of the state over long times:","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"# Run L63 from 0 -> tmax\nu0 = [1.0; 0.0; 0.0]\ntmax = 20\ndt = 0.01\ntspan = (0.0, tmax)\nprob = ODEProblem(lorenz, u0, tspan)\nsol = solve(prob, Euler(), dt = dt)\n\n# Run L63 from end for test trajectory data\ntmax_test = 100\ntspan_test = (0.0, tmax_test)\nu0_test = sol.u[end]\nprob_test = ODEProblem(lorenz, u0_test, tspan_test)\nsol_test = solve(prob_test, Euler(), dt = dt)\n\n# Run L63 from end for histogram matching data\ntmax_hist = 1000\ntspan_hist = (0.0, tmax_hist)\nu0_hist = sol_test.u[end]\nprob_hist = ODEProblem(lorenz, u0_hist, tspan_hist)\nsol_hist = solve(prob_hist, Euler(), dt = dt)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We generate the training data from sol within [tburn, tmax]. The user has the option of how many training points to take n_train_pts and whether these are selected randomly or sequentially (sample_rand). The true outputs are perturbed by noise of variance 1e-4 and pairs are stored in the compatible data format PairedDataContainer","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"tburn = 1 # NB works better with no spin-up!\nburnin = Int(floor(tburn / dt))\nn_train_pts = 600 \nsample_rand = true\nif sample_rand\n   ind = Int.(shuffle!(rng, Vector(burnin:(tmax / dt - 1)))[1:n_train_pts])\nelse\n   ind = burnin:(n_train_pts + burnin)\nend\nn_tp = length(ind)\ninput = zeros(3, n_tp)\noutput = zeros(3, n_tp)\nΓy = 1e-4 * I(3)\nnoise = rand(rng, MvNormal(zeros(3), Γy), n_tp)\nfor i in 1:n_tp\n    input[:, i] = sol.u[ind[i]]\n    output[:, i] = sol.u[ind[i] + 1] + noise[:, i]\nend\niopairs = PairedDataContainer(input, output)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We have several cases the user can play with,","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"cases = [\"GP\", \"RF-scalar\", \"RF-scalar-diagin\", \"RF-svd-nonsep\", \"RF-nosvd-nonsep\", \"RF-nosvd-sep\"]","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"Then, looping over the repeats, we first define some common hyperparamter optimization options for the \"RF-\" cases. In this case, the options are used primarily for diagnostics and acceleration (not required in general to solve this problem) ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"rf_optimizer_overrides = Dict(\n    \"verbose\" => true, # output diagnostics from the hyperparameter optimizer\n    \"scheduler\" => DataMisfitController(terminate_at = 1e4), # timestepping method for the optimizer\n    \"cov_sample_multiplier\" => 0.5, # 0.5*default number of samples to estimate covariances in optimizer\n    \"n_features_opt\" => 200, # number of features during hyperparameter optimization\n    \"n_iteration\" => 20, # iterations of the optimizer solver\n)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"Then we build the machine learning tools. Here we highlight scalar-output Gaussian process (GP), where we use the default squared-exponential kernel, and learn a lengthscale hyperparameter in each input dimension. To handle multiple outputs, we will use a decorrelation in the output space, and so will actually train three of these models.","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"gppackage = Emulators.GPJL() # use GaussianProcesses.jl\npred_type = Emulators.YType() # predicted variances are for data not latent function\nmlt = GaussianProcess(\n    gppackage;\n    prediction_type = pred_type,\n    noise_learn = false, # do not additionally learn a white kernel\n)","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"we also highlight the Vector Random Feature with nonseparable kernel (RF-nosvd-nonsep), this can natively handle multiple outputs without decorrelation of the output space. This kernel is a rank-3 representation with small nugget term.","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"nugget = 1e-12\nkernel_structure = NonseparableKernel(LowRankFactor(3, nugget))\nn_features = 500 # number of features for prediction\nmlt = VectorRandomFeatureInterface(\n    n_features,\n    3, # input dimension\n    3, # output dimension\n    rng = rng, # pin random number generator\n    kernel_structure = kernel_structure,\n    optimizer_options = rf_optimizer_overrides, \n)           ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"With machine learning tools specified, we build the emulator object","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"# decorrelate = true for `GP`\n# decorrelate = false for `RF-nosvd-nonsep`\nemulator = Emulator(mlt, iopairs; obs_noise_cov = Γy, decorrelate = decorrelate) \noptimize_hyperparameters!(emulator) # some GP packages require this additional call ","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Plots","page":"Integrating Lorenz 63 with an emulated integrator","title":"Plots","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"We predict the trained emulator mean, over the short-timescale validation trajectory","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"u_test_tmp = zeros(3, length(xspan_test))\nu_test_tmp[:, 1] = sol_test.u[1] # initialize at the final-time solution of the training period\n\nfor i in 1:(length(xspan_test) - 1)\n    rf_mean, _ = predict(emulator, u_test_tmp[:, i:i], transform_to_real = true) # 3x1 matrix\n    u_test_tmp[:, i + 1] = rf_mean\nend","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"The other trajectories are similar. We then produce the following plots. In all figures, the results from evolving the state with the true integrator is orange, and with the emulated integrators are blue.","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Gaussian-Process-Emulator-(Sci-kit-learn:-GP)","page":"Integrating Lorenz 63 with an emulated integrator","title":"Gaussian Process Emulator (Sci-kit learn: GP)","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For one example fit","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"<img src=\"../../../assets/GP_l63_test.png\" width=\"600\">\n<img src=\"../../../assets/GP_l63_attr.png\" width=\"300\"><img src=\"../../../assets/GP_l63_pdf.png\" width=\"300\">","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/#Random-Feature-Emulator-(RF-nosvd-nonsep)","page":"Integrating Lorenz 63 with an emulated integrator","title":"Random Feature Emulator (RF-nosvd-nonsep)","text":"","category":"section"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"For one example fit","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"<img src=\"../../../assets/RF-nosvd-nonsep_l63_test.png\" width=\"600\">\n<img src=\"../../../assets/RF-nosvd-nonsep_l63_attr.png\" width=\"300\"><img src=\"../../../assets/RF-nosvd-nonsep_l63_pdf.png\" width=\"300\">","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"and here are CDFs over 20 randomized trials of the random feature hyperparameter optimization","category":"page"},{"location":"examples/emulators/lorenz_integrator_3d_3d/","page":"Integrating Lorenz 63 with an emulated integrator","title":"Integrating Lorenz 63 with an emulated integrator","text":"<img src=\"../../../assets/RF-nosvd-nonsep_l63_cdfs.png\" width=\"600\">","category":"page"},{"location":"API/GaussianProcess/#GaussianProcess","page":"Gaussian Process","title":"GaussianProcess","text":"","category":"section"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"GaussianProcessesPackage\nPredictionType\nGaussianProcess\nbuild_models!(::GaussianProcess{GPJL}, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\noptimize_hyperparameters!(::GaussianProcess{GPJL})\npredict(::GaussianProcess{GPJL},  ::AbstractMatrix{FT}) where {FT <: AbstractFloat}","category":"page"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcessesPackage","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcessesPackage","text":"abstract type GaussianProcessesPackage\n\nType to dispatch which GP package to use:\n\nGPJL for GaussianProcesses.jl,\nSKLJL for the ScikitLearn GaussianProcessRegressor.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.PredictionType","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.PredictionType","text":"abstract type PredictionType\n\nPredict type for GPJL in GaussianProcesses.jl:\n\nYType\nFType latent function.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcess","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcess","text":"struct GaussianProcess{GPPackage, FT} <: CalibrateEmulateSample.Emulators.MachineLearningTool\n\nStructure holding training input and the fitted Gaussian process regression models.\n\nFields\n\nmodels::Vector{Union{Nothing, PyCall.PyObject, GaussianProcesses.GPE}}: The Gaussian Process (GP) Regression model(s) that are fitted to the given input-data pairs.\nkernel::Union{Nothing, var\"#s8\", var\"#s6\"} where {var\"#s8\"<:GaussianProcesses.Kernel, var\"#s6\"<:PyCall.PyObject}: Kernel object.\nnoise_learn::Bool: Learn the noise with the White Noise kernel explicitly?\nalg_reg_noise::Any: Additional observational or regularization noise in used in GP algorithms\nprediction_type::CalibrateEmulateSample.Emulators.PredictionType: Prediction type (y to predict the data, f to predict the latent function).\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat}\n)\n\n\nMethod to build Gaussian process models based on the package.\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}}","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    args...;\n    kwargs...\n)\n\n\nOptimize Gaussian process hyperparameters using in-build package method.\n\nWarning: if one uses GPJL() and wishes to modify positional arguments. The first positional argument must be the Optim method (default LBGFS()).\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#GaussianProcesses.predict-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, AbstractMatrix{FT}}} where FT<:AbstractFloat","page":"Gaussian Process","title":"GaussianProcesses.predict","text":"predict(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2}\n) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#MarkovChainMonteCarlo","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Top-level-class-and-methods","page":"MarkovChainMonteCarlo","title":"Top-level class and methods","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCWrapper\nMCMCWrapper(mcmc_alg::MCMCProtocol, obs_sample::AbstractVector{FT}, prior::ParameterDistribution, em::Emulator;init_params::AbstractVector{FT}, burnin::IT, kwargs...) where {FT<:AbstractFloat, IT<:Integer}\nsample\nget_posterior\noptimize_stepsize","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"struct MCMCWrapper\n\nTop-level class holding all configuration information needed for MCMC sampling: the prior,  emulated likelihood and sampling algorithm (DensityModel and Sampler, respectively, in  AbstractMCMC's terminology).\n\nFields\n\nprior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: ParameterDistribution object describing the prior distribution on parameter values.\nlog_posterior_map::AbstractMCMC.AbstractModel: AdvancedMH.DensityModel object, used to evaluate the posterior density being sampled from.\nmh_proposal_sampler::AbstractMCMC.AbstractSampler: Object describing a MCMC sampling algorithm and its settings.\nsample_kwargs::NamedTuple: NamedTuple of other arguments to be passed to AbstractMCMC.sample().\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper-Union{Tuple{IT}, Tuple{FT}, Tuple{CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol, AbstractVector{FT}, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, CalibrateEmulateSample.Emulators.Emulator}} where {FT<:AbstractFloat, IT<:Integer}","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"MCMCWrapper(\n    mcmc_alg::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol,\n    obs_sample::AbstractArray{FT<:AbstractFloat, 1},\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    em::CalibrateEmulateSample.Emulators.Emulator;\n    init_params,\n    burnin,\n    kwargs...\n)\n\n\nConstructor for MCMCWrapper which performs the same standardization (SVD  decorrelation) that was applied in the Emulator. It creates and wraps an instance of  EmulatorPosteriorModel, for sampling from the Emulator, and  MetropolisHastingsSampler, for generating the MC proposals.\n\nmcmc_alg: MCMCProtocol describing the MCMC sampling algorithm to use. Currently implemented algorithms are:\nRWMHSampling: Metropolis-Hastings sampling from a vanilla random walk with fixed stepsize.\npCNMHSampling: Metropolis-Hastings sampling using the preconditioned  Crank-Nicholson algorithm, which has a well-behaved small-stepsize limit.\nobs_sample: A single sample from the observations. Can, e.g., be picked from an  Observation struct using get_obs_sample.\nprior: ParameterDistribution  object containing the parameters' prior distributions.\nem: Emulator to sample from. \nstepsize: MCMC step size, applied as a scaling to the prior covariance.\ninit_params: Starting parameter values for MCMC sampling.\nburnin: Initial number of MCMC steps to discard from output (pre-convergence).\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#StatsBase.sample","page":"MarkovChainMonteCarlo","title":"StatsBase.sample","text":"sample([rng,] mcmc::MCMCWrapper, args...; kwargs...)\n\nExtends the sample methods of AbstractMCMC (which extends StatsBase) to sample from the  emulated posterior, using the MCMC sampling algorithm and Emulator configured in  MCMCWrapper. Returns a MCMCChains.Chains  object containing the samples. \n\nSupported methods are:\n\nsample([rng, ]mcmc, N; kwargs...)\nReturn a MCMCChains.Chains object containing N samples from the emulated posterior.\nsample([rng, ]mcmc, isdone; kwargs...)\nSample from the model with the Markov chain Monte Carlo sampler until a convergence  criterion isdone returns true, and return the samples. The function isdone has the  signature\n    isdone(rng, model, sampler, samples, state, iteration; kwargs...)\nwhere state and iteration are the current state and iteration of the sampler,  respectively. It should return true when sampling should end, and false otherwise.\nsample([rng, ]mcmc, parallel_type, N, nchains; kwargs...)\nSample nchains Monte Carlo Markov chains in parallel according to parallel_type, which may be MCMCThreads() or MCMCDistributed() for thread and parallel sampling,  respectively.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","text":"get_posterior(\n    mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper,\n    chain::MCMCChains.Chains\n) -> EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n\n\nReturns a ParameterDistribution object corresponding to the empirical distribution of the  samples in chain.\n\nnote: Note\nThis method does not currently support combining samples from multiple Chains.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","text":"optimize_stepsize(\n    rng::Random.AbstractRNG,\n    mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper;\n    init_stepsize,\n    N,\n    max_iter,\n    sample_kwargs...\n) -> Float64\n\n\nUses a heuristic to return a stepsize for the mh_proposal_sampler element of  MCMCWrapper which yields fast convergence of the Markov chain.\n\nThe criterion used is that Metropolis-Hastings proposals should be accepted between 15% and  35% of the time.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"See AbstractMCMC sampling API for background on our use of Turing.jl's  AbstractMCMC API for  MCMC sampling.","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Sampler-algorithms","page":"MarkovChainMonteCarlo","title":"Sampler algorithms","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCProtocol\nRWMHSampling\npCNMHSampling\nMetropolisHastingsSampler","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","text":"abstract type MCMCProtocol\n\nType used to dispatch different methods of the MetropolisHastingsSampler  constructor, corresponding to different sampling algorithms.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","text":"struct RWMHSampling <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters as as vanilla random walk, based on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","text":"struct pCNMHSampling <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters according to the preconditioned Crank-Nicholson (pCN) algorithm, which is  usable for MCMC in the stepsize → 0 limit, unlike the vanilla random walk. Steps are based  on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","text":"MetropolisHastingsSampler(\n    _::CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling,\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n) -> CalibrateEmulateSample.MarkovChainMonteCarlo.RWMetropolisHastings\n\n\nConstructor for all Sampler objects, with one method for each supported MCMC algorithm.\n\nwarning: Warning\nBoth currently implemented Samplers use a Gaussian approximation to the prior:  specifically, new Metropolis-Hastings proposals are a scaled combination of the old  parameter values and a random shift distributed as a Gaussian with the same covariance as the prior. This suffices for our current use case, in which our priors are Gaussian (possibly in a transformed space) and we assume enough fidelity in the Emulator that  inference isn't prior-dominated.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Emulated-posterior-(Model)","page":"MarkovChainMonteCarlo","title":"Emulated posterior (Model)","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"EmulatorPosteriorModel","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","text":"EmulatorPosteriorModel(\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    obs_sample::AbstractArray{FT<:AbstractFloat, 1}\n) -> AdvancedMH.DensityModel\n\n\nFactory which constructs AdvancedMH.DensityModel objects given a prior on the model  parameters (prior) and an Emulator of the log-likelihood of the data given  parameters. Together these yield the log posterior density we're attempting to sample from  with the MCMC, which is the role of the DensityModel class in the AbstractMCMC interface.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-MCMC-State","page":"MarkovChainMonteCarlo","title":"Internals - MCMC State","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCState\naccept_ratio","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","text":"struct MCMCState{T, L<:Real} <: AdvancedMH.AbstractTransition\n\nExtends the AdvancedMH.Transition (which encodes the current state of the MC during sampling) with a boolean flag to record whether this state is new (arising from accepting a Metropolis-Hastings proposal) or old (from rejecting a proposal).\n\nFields\n\nparams::Any: Sampled value of the parameters at the current state of the MCMC chain.\nlog_density::Real: Log probability of params, as computed by the model using the prior.\naccepted::Bool: Whether this state resulted from accepting a new MH proposal.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","text":"accept_ratio(chain::MCMCChains.Chains) -> Any\n\n\nFraction of MC proposals in chain which were accepted (according to Metropolis-Hastings.)\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-Other","page":"MarkovChainMonteCarlo","title":"Internals - Other","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"to_decorrelated","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","text":"to_decorrelated(\n    data::AbstractArray{FT<:AbstractFloat, 2},\n    em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}\n) -> Any\n\n\nTransform samples from the original (correlated) coordinate system to the SVD-decorrelated coordinate system used by Emulator. Used in the constructor for MCMCWrapper.\n\n\n\n\n\n","category":"function"},{"location":"random_feature_emulator/#Random-Feature-Emulator","page":"Random Features","title":"Random Feature Emulator","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Have a go with Gaussian processes first\nWe recommend that users first try GaussianProcess for their problems. As random features are a more recent tool, the training procedures and interfaces are still experimental and in development. ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Random features provide a flexible framework to approximates a Gaussian process. Using random sampling of features, the method is a low-rank approximation leading to advantageous scaling properties (with the number of training points, input, and output dimensions). In the infinite sample limit, there are often (known) explicit Gaussian process kernels that the random feature representation converges to.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We provide two types of MachineLearningTool for random feature emulation, the ScalarRandomFeatureInterface and the VectorRandomFeatureInterface.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The ScalarRandomFeatureInterface closely mimics the role of a GaussianProcess package, by training a scalar-output function distribution. It can be applied to multidimensional output problems (as with GaussianProcess) by relying on data processing tools, such as performed when the decorrelate=true keyword argument is provided to the Emulator.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The VectorRandomFeatureInterface, when applied to multidimensional problems, directly trains a function distribution between multi-dimensional spaces. This approach is not restricted to the data processing of the scalar method (though this can still be helpful). It can be cheaper to evaluate, but on the other hand the training can be more challenging/computationally expensive.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Building a random feature interface is similar to building a Gaussian process: one defines a kernel to encode similarities between outputs (y_iy_j) based on inputs (x_ix_j). Additionally, one must specify the number of random feature samples to be taken to build the emulator.","category":"page"},{"location":"random_feature_emulator/#User-Interface","page":"Random Features","title":"User Interface","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"CalibrateEmulateSample.jl allows the random feature emulator to be built using the external package RandomFeatures.jl. In the notation of this package's documentation, our interface allows for families of RandomFourierFeature objects to be constructed with different Gaussian distributions of the \"xi\" a.k.a weight distribution, and with a learnable \"sigma\", a.k.a scaling parameter.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Relating features and kernels\nThe parallels of random features and gaussian processes can be quite strong. For example:The restriction to RandomFourierFeature objects is a restriction to the approximation of shift-invariant kernels (i.e. K(xy) = K(x-y))\nThe restriction of the weight (\"xi\") distribution to Gaussians is a restriction of approximating squared-exponential kernels. Other distributions (e.g. student-t) leads to other kernels (e.g. Matern)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The interfaces are defined minimally with","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"srfi = ScalarRandomFeatureInterface(n_features, input_dim; ...)\nvrfi = VectorRandomFeatureInterface(n_features, input_dim, output_dim; ...)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"This will build an interface around a random feature family based on n_features features and mapping between spaces of dimenstion input_dim to 1 (scalar), or output_dim (vector).","category":"page"},{"location":"random_feature_emulator/#The-kernel_structure-keyword-for-flexibility","page":"Random Features","title":"The kernel_structure keyword - for flexibility","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To adjust the expressivity of the random feature family one can define the keyword argument kernel_structure. The more expressive the kernel, the more hyperparameters are learnt in the optimization.  ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We have two types,","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"separable_kernel = Separable(input_cov_structure, output_cov_structure)\nnonseparable_kernel = Nonseparable(cov_structure)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"where the cov_structure implies some imposed user structure on the covariance structure. The basic covariance structures are given by ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"1d_cov_structure = OneDimFactor() # the problem dimension is 1\ndiagonal_structure = DiagonalFactor() # impose diagonal structure (e.g. ARD kernel)\ncholesky_structure = CholeskyFactor() # general positive definite matrix\nlr_perturbation = LowRankFactor(r) # assume structure is a rank-r perturbation from identity","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"All covariance structures (except OneDimFactor) have their final positional argument being a \"nugget\" term adding +epsilon I to the covariance structure. Set to 1 by default.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The current default kernels are as follows:","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"scalar_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(input_dim)))), OneDimFactor())\nvector_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(output_dim)))), LowRankFactor(Int(ceil(sqrt(output_dim)))))","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Relating covariance structure and training\nThe parallels between random feature and Gaussian process also extends to the hyperparameter learning. For example,A ScalarRandomFeatureInterface with a DiagonalFactor input covariance structure approximates a Gaussian process with automatic relevance determination (ARD) kernel, where one learns a lengthscale in each dimension of the input space","category":"page"},{"location":"random_feature_emulator/#The-optimizer_options-keyword-for-performance","page":"Random Features","title":"The optimizer_options keyword - for performance","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Passed as a dictionary, this keyword allows the user to configure many options from their defaults in the hyperparameter optimization. The optimizer itself relies on the EnsembleKalmanProcesses package.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We recommend users experiment with a subset of these flags. At first enable","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\"verbose\" => true)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If the covariance sampling takes too long, run with multithreading (e.g. julia --project -t n_threads script.jl). Sampling is embarassingly parallel so this acheives near linear scaling,","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If sampling still takes too long, try setting","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\n    \"cov_sample_multiplier\" => csm,\n    \"train_fraction\" => tf,\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Decreasing csm (default 10.0) towards 0.0 directly reduces the number of samples to estimate a covariance matrix in the optimizer, by using a shrinkage estimator - the more shrinkage the more approximation (suggestion, keep shrinkage amount below 0.2).\nIncreasing tf towards 1 changes the train-validate split, reducing samples but increasing cost-per-sample and reducing the available validation data (default 0.8, suggested range (0.5,0.95)).","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If optimizer convergence stagnates or is too slow, or if it terminates before producing good results, try:","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\n    \"n_ensemble\" => n_e, \n    \"n_iteration\" => n_i,\n    \"localization\" => loc,\n    \"scheduler\" => sch,\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We suggest looking at the EnsembleKalmanProcesses documentation for more details; but to summarize","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Reducing optimizer samples n_e and iterations n_i reduces computation time.\nIf n_e becomes less than the number of hyperparameters, the updates will fail and a localizer must be specified in loc.\nIf the algorithm terminates at T=1 and resulting emulators looks unacceptable one can change or add arguments in sch e.g. DataMisfitController(\"on_terminate\"=continue)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Note\nWidely robust defaults here are a work in progress","category":"page"},{"location":"random_feature_emulator/#Key-methods","page":"Random Features","title":"Key methods","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To interact with the kernel/covariance structures we have standard get_* methods along with some useful functions","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"cov_structure_from_string(string,dim) creates a basic covariance structure from a predefined string: onedim, diagonal, cholesky, lowrank etc. and a dimension\ncalculate_n_hyperparameters(in_dim, out_dim, kernel_structure) calculates the number of hyperparameters created by using the given kernel structure (can be applied to the covariance structure individually too)\nbuild_default_priors(in_dim, out_dim, kernel_structure) creates a ParameterDistribution for the hyperparameters based on the kernel structure. This serves as the initialization of the training procedure.","category":"page"},{"location":"random_feature_emulator/#Example-families-and-their-hyperparameters","page":"Random Features","title":"Example families and their hyperparameters","text":"","category":"section"},{"location":"random_feature_emulator/#Scalar:-\\mathbb{R}5-\\to-\\mathbb{R}-at-defaults","page":"Random Features","title":"Scalar: mathbbR^5 to mathbbR at defaults","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 5\n# build the default scalar kernel directly (here it will be a rank-3 perturbation from the identity)\nscalar_default_kernel = SeparableKernel(\n    cov_structure_from_string(\"lowrank\", input_dim),\n    cov_structure_from_string(\"onedim\", 1)\n) \n\ncalculate_n_hyperparameters(input_dim, scalar_default_kernel) \n# answer = 19, 18 for the covariance structure, and one scaling parameter\n\nbuild_default_prior(input_dim, scalar_default_kernel)\n# builds a 3-entry distribution\n# 3-dim positive distribution 'input_lowrank_diagonal'\n# 15-dim unbounded distribution 'input_lowrank_U'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/#Vector,-separable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-at-defaults","page":"Random Features","title":"Vector, separable: mathbbR^25 to mathbbR^50 at defaults","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Or take a diagonalized 8-dimensional input, and assume full 6-dimensional output","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\n# build the default vector kernel directly (here it will be a rank-5 input and rank-8 output)\nvector_default_kernel = SeparableKernel(\n    cov_structure_from_string(\"lowrank\", input_dim),\n    cov_structure_from_string(\"lowrank\", output_dim)\n)\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_default_kernel) \n# answer = 539; 130 for input, 408 for the output, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_default_kernel)\n# builds a 5-entry distribution\n# 5-dim positive distribution 'input_lowrank_diagonal'\n# 125-dim unbounded distribution 'input_lowrank_U'\n# 8-dim positive distribution 'output_lowrank_diagonal'\n# 400-dim unbounded distribution 'output_lowrank_U'\n# 1-dim postive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/#Vector,-nonseparable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}","page":"Random Features","title":"Vector, nonseparable: mathbbR^25 to mathbbR^50","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The following represents the most general kernel case.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Use low-rank/diagonls representations where possible\nThe following is far too general, leading to large numbers of hyperparameters","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\neps = 1e-8\n# build a full-rank nonseparable vector kernel\nvector_general_kernel = NonseparableKernel(CholeskyFactor(eps))\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_general_kernel)\n# answer = 781876; 781875 for the joint input-output space, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_default_kernel)\n# builds a 2-entry distribution\n# 781875-dim unbounded distribution 'full_cholesky'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"See the API for more details.","category":"page"},{"location":"calibrate/#The-Calibrate-stage","page":"Calibrate","title":"The Calibrate stage","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration of the computer model entails finding an optimal parameter theta^* that maximizes the posterior probability","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"rho(thetavert y Gamma_y) = dfrace^-mathcalL(theta y)Z(yvertGamma_y)rho_mathrmprior(theta) qquad mathcalL(theta y) = langle mathcalG(theta) - y    Gamma_y^-1 left ( mathcalG(theta) - y right ) rangle","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"where mathcalL is the loss or negative log-likelihood, Z(yvertGamma) is a normalizing constant, y represents the data, Gamma_y is the noise covariance matrix and rho_mathrmprior(theta) is the prior density. Calibration is performed using ensemble Kalman processes, which generate input-output pairs theta mathcalG(theta) in high density from the prior initial guess to the found optimal parameter theta^*. These input-output pairs are then used as the data to train an emulator of the forward model mathcalG.","category":"page"},{"location":"calibrate/#Ensemble-Kalman-Processes","page":"Calibrate","title":"Ensemble Kalman Processes","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration can be performed using different ensemble Kalman processes: ensemble Kalman inversion (Iglesias et al, 2013), ensemble Kalman sampler (Garbuno-Inigo et al, 2020), and unscented Kalman inversion (Huang et al, 2022). All algorithms are implemented in EnsembleKalmanProcesses.jl. Documentation of each algorithm is available in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"calibrate/#Typical-construction-of-the-EnsembleKalmanProcess","page":"Calibrate","title":"Typical construction of the EnsembleKalmanProcess","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Documentation on how to construct an EnsembleKalmanProcess from the computer model and the data can be found in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"API/Emulators/#Emulators","page":"General Interface","title":"Emulators","text":"","category":"section"},{"location":"API/Emulators/","page":"General Interface","title":"General Interface","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/Emulators/","page":"General Interface","title":"General Interface","text":"Emulator\noptimize_hyperparameters!(::Emulator)\npredict\nnormalize\nstandardize\nreverse_standardize\nsvd_transform\nsvd_reverse_transform_mean_cov","category":"page"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.Emulator","page":"General Interface","title":"CalibrateEmulateSample.Emulators.Emulator","text":"struct Emulator{FT<:AbstractFloat}\n\nStructure used to represent a general emulator, independently of the algorithm used.\n\nFields\n\nmachine_learning_tool::CalibrateEmulateSample.Emulators.MachineLearningTool: Machine learning tool, defined as a struct of type MachineLearningTool.\ntraining_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT} where FT<:AbstractFloat: Normalized, standardized, transformed pairs given the Booleans normalize_inputs, standardize_outputs, retained_svd_frac.\ninput_mean::AbstractVector{FT} where FT<:AbstractFloat: Mean of input; length input_dim.\nnormalize_inputs::Bool: If normalizing: whether to fit models on normalized inputs ((inputs - input_mean) * sqrt_inv_input_cov).\nnormalization::Union{Nothing, LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat: (Linear) normalization transformation; size input_dim × input_dim.\nstandardize_outputs::Bool: Whether to fit models on normalized outputs: outputs / standardize_outputs_factor.\nstandardize_outputs_factors::Union{Nothing, AbstractVector{FT}} where FT<:AbstractFloat: If standardizing: Standardization factors (characteristic values of the problem).\ndecomposition::Union{Nothing, LinearAlgebra.SVD}: The singular value decomposition of obs_noise_cov, such that obs_noise_cov = decomposition.U * Diagonal(decomposition.S) * decomposition.Vt. NB: the SVD may be reduced in dimensions.\nretained_svd_frac::AbstractFloat: Fraction of singular values kept in decomposition. A value of 1 implies full SVD spectrum information.\n\n\n\n\n\n","category":"type"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.Emulator}","page":"General Interface","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    args...;\n    kwargs...\n) -> Any\n\n\nOptimizes the hyperparameters in the machine learning tool.\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#GaussianProcesses.predict","page":"General Interface","title":"GaussianProcesses.predict","text":"predict(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2}\n) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\npredict(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    new_inputs::AbstractMatrix;\n    multithread\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\npredict(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    new_inputs::AbstractMatrix\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\npredict(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2};\n    transform_to_real,\n    vector_rf_unstandardize,\n    mlt_kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction using the emulator on new inputs (each new inputs given as data columns). Default is to predict in the decorrelated space.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.normalize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.normalize","text":"normalize(\n    emulator::CalibrateEmulateSample.Emulators.Emulator,\n    inputs::AbstractVecOrMat\n) -> Any\n\n\nNormalize the input data, with a normalizing function.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.standardize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.standardize","text":"standardize(\n    outputs::AbstractVecOrMat,\n    output_covs::Union{AbstractVector{<:AbstractMatrix}, AbstractVector{<:LinearAlgebra.UniformScaling}},\n    factors::AbstractVector\n) -> Tuple{Any, Union{AbstractVector{<:AbstractMatrix}, AbstractVector{<:LinearAlgebra.UniformScaling}}}\n\n\nStandardize with a vector of factors (size equal to output dimension).\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.reverse_standardize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.reverse_standardize","text":"reverse_standardize(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    outputs::AbstractVecOrMat,\n    output_covs::AbstractVecOrMat\n) -> Tuple{Any, Any}\n\n\nReverse a previous standardization with the stored vector of factors (size equal to output  dimension). output_cov is a Vector of covariance matrices, such as is returned by svd_reverse_transform_mean_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_transform","page":"General Interface","title":"CalibrateEmulateSample.Emulators.svd_transform","text":"svd_transform(\n    data::AbstractArray{FT<:AbstractFloat, 2},\n    obs_noise_cov::Union{Nothing, AbstractArray{FT<:AbstractFloat, 2}};\n    retained_svd_frac\n) -> Tuple{Any, Any}\n\n\nApply a singular value decomposition (SVD) to the data\n\ndata - GP training data/targets; size output_dim × N_samples\nobs_noise_cov - covariance of observational noise\ntruncate_svd - Project onto this fraction of the largest principal components. Defaults to 1.0 (no truncation).\n\nReturns the transformed data and the decomposition, which is a matrix  factorization of type LinearAlgebra.SVD. \n\nNote: If F::SVD is the factorization object, U, S, V and Vt can be obtained via  F.U, F.S, F.V and F.Vt, such that A = U * Diagonal(S) * Vt. The singular values  in S are sorted in descending order.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","page":"General Interface","title":"CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","text":"svd_reverse_transform_mean_cov(\n    μ::AbstractArray{FT<:AbstractFloat, 2},\n    σ2::AbstractVector,\n    decomposition::LinearAlgebra.SVD\n) -> Tuple{Any, Any}\n\n\nTransform the mean and covariance back to the original (correlated) coordinate system\n\nμ - predicted mean; size output_dim × N_predicted_points.\nσ2 - predicted variance; size output_dim × N_predicted_points.      - predicted covariance; size N_predicted_points array of size output_dim × output_dim.\ndecomposition - SVD decomposition of obs_noise_cov.\n\nReturns the transformed mean (size output_dim × N_predicted_points) and variance.  Note that transforming the variance back to the original coordinate system results in non-zero off-diagonal elements, so instead of just returning the  elements on the main diagonal (i.e., the variances), we return the full  covariance at each point, as a vector of length N_predicted_points, where  each element is a matrix of size output_dim × output_dim.\n\n\n\n\n\n","category":"function"},{"location":"emulate/#The-Emulate-stage","page":"Emulate","title":"The Emulate stage","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Emulation is performed through the construction of an Emulator object, which has two components","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"A wrapper for any statistical emulator,\nData-processing and dimensionality reduction functionality.","category":"page"},{"location":"emulate/#Typical-construction-from-Lorenz_example.jl","page":"Emulate","title":"Typical construction from Lorenz_example.jl","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"First, obtain data in a PairedDataContainer, for example, get this from an EnsembleKalmanProcess ekpobj generated during the Calibrate stage, or see the constructor here","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"using CalibrateEmulateSample.Utilities\ninput_output_pairs = Utilities.get_training_points(ekpobj, 5) # use first 5 iterations as data","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Wrapping a predefined machine learning tool, e.g. a Gaussian process gauss_proc, the Emulator can then be built:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"emulator = Emulator(\n    gauss_proc, \n    input_output_pairs; # optional arguments after this\n    obs_noise_cov = Γy,\n    normalize_inputs = true,\n    standardize_outputs = true,\n    standardize_outputs_factors = factor_vector,\n    retained_svd_frac = 0.95,\n)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"The optional arguments above relate to the data processing.","category":"page"},{"location":"emulate/#Emulator-Training","page":"Emulate","title":"Emulator Training","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"The emulator is trained when we combine the machine learning tool and the data into the Emulator above.  For any machine learning tool, we must also optimize the hyperparameters:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"In the Lorenz example, this line learns the hyperparameters of the Gaussian process, which depend on the choice of kernel. Predictions at new inputs can then be made using","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"y, cov = Emulator.predict(emulator, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This returns both a mean value and a covariance.","category":"page"},{"location":"emulate/#Data-processing","page":"Emulate","title":"Data processing","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Some effects of the following are outlined in a practical setting in the results and appendices of Howland, Dunbar, Schneider, (2022).","category":"page"},{"location":"emulate/#Diagonalization-and-output-dimension-reduction","page":"Emulate","title":"Diagonalization and output dimension reduction","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"obs_noise_cov = Γy (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"We always use singular value decomposition to diagonalize the output space, requiring output covariance Γy. Why? If we need to train a mathbbR^10 to mathbbR^100 emulator, diagonalization allows us to instead train 100 mathbbR^10 to mathbbR^1 emulators (far cheaper).","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"retained_svd_frac = 0.95 (default 1.0)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Performance is increased further by throwing away less informative output dimensions, if 95% of the information (i.e., variance) is in the first 40 diagonalized output dimensions then setting retained_svd_frac=0.95 will train only 40 emulators.","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"note: Note\nDiagonalization is an approximation. It is however a good approximation when the observational covariance varies slowly in the parameter space.","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"warn: Warn\nSevere approximation errors can occur if obs_noise_cov is not provided.","category":"page"},{"location":"emulate/#Normalization-and-standardization","page":"Emulate","title":"Normalization and standardization","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"normalize_inputs = true (default: true)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"We normalize the input data in a standard way by centering, and scaling with the empirical covariance","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"standardize_outputs = true (default: false)\nstandardize_outputs_factors = factor_vector (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"To help with poor conditioning of the covariance matrix, users can also standardize each output dimension with by a multiplicative factor given by the elements of factor_vector","category":"page"},{"location":"emulate/#Modular-interface","page":"Emulate","title":"Modular interface","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Each statistical emulator has the following supertype and methods:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"abstract type MachineLearningTool end\nfunction build_models!(mlt, iopairs)\nfunction optimize_hyperparameters!(mlt)\nfunction predict(mlt, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Add a new tool as follows:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Create MyMLToolName.jl, and include \"MyMLToolName.jl\" in Emulators.jl\nCreate a struct MyMLTool <: MachineLearningTool \nCreate these three methods to build, train, and predict with your tool (use GaussianProcess.jl as a guide)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"note: Note\nThe predict method currently needs to return both a predicted mean and a predicted (co)variance at new inputs, which are used in the Sample stage.","category":"page"},{"location":"#CalibrateEmulateSample.jl","page":"Home","title":"CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl solves parameter estimation problems using accelerated (and approximate) Bayesian inversion.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The framework can be applied currently to learn:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the joint distribution for a moderate numbers of parameters (<40),\nit is not inherently restricted to unimodal distributions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It can be used with computer models that:","category":"page"},{"location":"","page":"Home","title":"Home","text":"can be noisy or chaotic,\nare non-differentiable,\ncan only be treated as black-box (interfaced only with parameter files).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The computer model is supplied by the user, as a parameter-to-data map mathcalG(theta) mathbbR^p rightarrow mathbbR^d. For example, mathcalG could be a map from any given parameter configuration theta to a collection of statistics of a dynamical system trajectory. mathcalG is referred to as the forward model in the Bayesian inverse problem setting.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The data produced by the forward model are compared to observations y, which are assumed to be corrupted by additive noise eta, such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"y = mathcalG(theta) + eta","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the noise eta is drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y).","category":"page"},{"location":"#The-inverse-problem","page":"Home","title":"The inverse problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Given an observation y, the computer model mathcalG, the observational noise Gamma_y, and some broad prior information on theta, we return the joint distribution of a data-informed distribution for \"theta given y\".","category":"page"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, CalibrateEmulateSample.jl breaks this problem into a sequence of three steps: calibration, emulation, and sampling. A comprehensive treatment of the calibrate-emulate-sample approach to Bayesian inverse problems can be found in Cleary et al. (2020).","category":"page"},{"location":"#The-three-steps-of-the-algorithm:","page":"Home","title":"The three steps of the algorithm:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The calibrate step of the algorithm consists of an application of Ensemble Kalman Processes, which generates input-output pairs theta mathcalG(theta) in high density around an optimal parameter theta^*. This theta^* will be near a mode of the posterior distribution (Note: This the only time we interface with the forward model mathcalG).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The emulate step takes these pairs theta mathcalG(theta) and trains a statistical surrogate model (e.g., a Gaussian process), emulating the forward map mathcalG.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The sample step uses this surrogate in place of mathcalG in a sampling method (Markov chain Monte Carlo) to sample the posterior distribution of theta.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl contains the following modules:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nCalibrateEmulateSample.jl Pulls in the Ensemble Kalman Processes package\nEmulator.jl Emulate: Modular template for emulators\nGaussianProcess.jl - A Gaussian process emulator\nMarkovChainMonteCarlo.jl Sample: Modular template for MCMC\nUtilities.jl Helper functions","category":"page"},{"location":"","page":"Home","title":"Home","text":"The best way to get started is to have a look at the examples!","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl is being developed by the Climate Modeling Alliance.","category":"page"},{"location":"examples/edmf_example/#Extended-Eddy-Diffusivity-Mass-Flux-(EDMF)-Scheme","page":"Turbulence example","title":"Extended Eddy-Diffusivity Mass-Flux (EDMF) Scheme","text":"","category":"section"},{"location":"examples/edmf_example/#Background","page":"Turbulence example","title":"Background","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The extended EDMF scheme is a unified model of turbulence and convection. More information about the model can be found here. This example builds an emulator of the extended EDMF scheme from input-output pairs obtained during a calibration process, and runs emulator-based MCMC to obtain an estimate of the joint parameter distribution.","category":"page"},{"location":"examples/edmf_example/#What-is-being-solved-here","page":"Turbulence example","title":"What is being solved here","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"This example reads calibration data containing input-output pairs obtained during calibration of the EDMF scheme. The calibration is performed using ensemble Kalman inversion, an ensemble-based algorithm that updates the location of the input parameters from the prior to the posterior, thus ensuring an optimal placement of the data used to train the emulator. In this example, the input is formed by either two or five EDMF parameters, and the output is the time-averaged liquid water path (LWP) at 40 locations in the eastern Pacific Ocean. The calibration data also contains the prior distribution of EDMF parameters and the variance of the observed variables (LWP in this case), which is used as a proxy for the magnitude of observational noise.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"More information about EDMF calibration can be found here. The calibration data is used to train the emulator.","category":"page"},{"location":"examples/edmf_example/#Running-the-examples","page":"Turbulence example","title":"Running the examples","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"We have two example scenario data (output from a (C)alibration run) that must be simply unzipped before calibration:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"ent-det-calibration.zip # two-parameter calibration\nent-det-tked-tkee-stab-calibration.zip # five-parameter calibration","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To perform uncertainty quantification use the file uq_for_EDMF.jl. Set the experiment name, and date (for outputs), e.g.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"exp_name = \"ent-det-tked-tkee-stab-calibration\" \ndate_of_run = Date(year, month, day)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"and call,","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"> julia --project uq_for_EDMF.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Info\nThese runs take currently take ~1 hour to complete","category":"page"},{"location":"examples/edmf_example/#Solution-and-output","page":"Turbulence example","title":"Solution and output","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The solution is the posterior distribution, stored in the file posterior.jld2.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior is visualized by using plot_posterior.jl, which produces corner-type scatter plots of posterior distribution, which show pairwise correlations. Again, set the exp_name and date_of_run values, then call","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"julia --project plot_posterior.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior samples can also be investigated directly. They are stored as a ParameterDistribution-type Samples object. One can load this and extract an array of parameters with:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"# input:\n# path to posterior.jld2: posterior_filepath (string)\n\nusing CalibrateEmulateSample.ParameterDistribution\nposterior = load(posterior_filepath)[\"posterior\"]\nposterior_samples = vcat([get_distribution(posterior)[name] for name in get_name(posterior)]...) # samples are columns","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To transform these samples into physical parameter space use the following:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"transformed_posterior_samples =\nmapslices(x -> transform_unconstrained_to_constrained(posterior, x), posterior_samples, dims = 1)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Computational vs Physical space\nThe computational theta-space are the parameters on which the algorithms act. Statistics (e.g. mean/covariance) are most meaningful when taken in this space. The physical phi-space is a (nonlinear) transformation of the computational space to apply parameter constraints. To pass parameter values back into the forward model, one must transform them. Full details and examples can be found here","category":"page"}]
}
