<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Cloudy example · CalibrateEmulateSample.jl</title><meta name="title" content="Cloudy example · CalibrateEmulateSample.jl"/><meta property="og:title" content="Cloudy example · CalibrateEmulateSample.jl"/><meta property="twitter:title" content="Cloudy example · CalibrateEmulateSample.jl"/><meta name="description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="og:description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="twitter:description" content="Documentation for CalibrateEmulateSample.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="CalibrateEmulateSample.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CalibrateEmulateSample.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation_instructions/">Installation instructions</a></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../sinusoid_example/">Simple example walkthrough</a></li><li><a class="tocitem" href="../lorenz_example/">Lorenz example</a></li><li><a class="tocitem" href="../edmf_example/">Turbulence example</a></li><li class="is-active"><a class="tocitem" href>Cloudy example</a></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Emulator testing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../emulators/regression_2d_2d/">Regression of <span>$\mathbb{R}^2 \to \mathbb{R}^2$</span> smooth function</a></li><li><a class="tocitem" href="../emulators/lorenz_integrator_3d_3d/">Integrating Lorenz 63 with an emulated integrator</a></li><li><a class="tocitem" href="../emulators/global_sens_analysis/">Global Sensitiviy Analysis (GSA) test functions</a></li></ul></li></ul></li><li><a class="tocitem" href="../../calibrate/">Calibrate</a></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Emulate</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../emulate/">Emulator</a></li><li><a class="tocitem" href="../../GaussianProcessEmulator/">Gaussian Process</a></li><li><a class="tocitem" href="../../random_feature_emulator/">Random Features</a></li></ul></li><li><a class="tocitem" href="../../sample/">Sample</a></li><li><a class="tocitem" href="../../glossary/">Glossary</a></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">API</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-9-1" type="checkbox"/><label class="tocitem" for="menuitem-9-1"><span class="docs-label">CalibrateEmulateSample</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-9-1-1" type="checkbox"/><label class="tocitem" for="menuitem-9-1-1"><span class="docs-label">Emulators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../API/Emulators/">General Interface</a></li><li><a class="tocitem" href="../../API/GaussianProcess/">Gaussian Process</a></li><li><a class="tocitem" href="../../API/RandomFeatures/">Random Features</a></li></ul></li><li><a class="tocitem" href="../../API/MarkovChainMonteCarlo/">MarkovChainMonteCarlo</a></li><li><a class="tocitem" href="../../API/Utilities/">Utilities</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Cloudy example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Cloudy example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl/blob/main/docs/src/examples/Cloudy_example.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Cloudy-example"><a class="docs-heading-anchor" href="#Cloudy-example">Learning the initial parameters of a droplet mass distribution in Cloudy</a><a id="Cloudy-example-1"></a><a class="docs-heading-anchor-permalink" href="#Cloudy-example" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">How do I run this code?</header><div class="admonition-body"><p>The full code is found in the <a href="https://github.com/CliMA/CalibrateEmulateSample.jl/tree/main/examples"><code>examples/</code></a> directory of the github repository</p></div></div><div class="admonition is-category-warn"><header class="admonition-header">version control for Cloudy</header><div class="admonition-body"><p>Due to rapid developments in Cloudy, this example will not work with the latest version. It is known to work pinned to specific commit <code>b4fa7e3</code>, please add Cloudy to the example Project using command <code>add Cloudy#b4fa7e3</code> in <code>Pkg</code> to avoid errors.</p></div></div><p>This example is based on <a href="https://github.com/CliMA/Cloudy.jl.git">Cloudy</a>, a microphysics model that simulates how cloud droplets collide and coalesce into larger drops. Collision-coalescence is a crucial process for the formation of rain. </p><p>Cloudy is initialized with a mass distribution of the cloud droplets; this distribution is then stepped forward in time, with more and more droplets colliding and combining into bigger drops according to the droplet-droplet interactions specified by a collision-coalescence kernel. The evolution of the droplet distribution is completely determined by the shape of the initial distribution and the form of the kernel.</p><p>We will show how Calibrate-Emulate-Sample (CES) can be used to learn the parameters of the initial cloud droplet mass distribution from observations of the moments of that mass distribution at a later time. </p><p>Cloudy is used here in a &quot;perfect model&quot; (aka &quot;known truth&quot;) setting, which means that the &quot;observations&quot; are generated by Cloudy itself, by running it with the true parameter values–-in more realistic applications, one would use actual measurements of cloud properties.</p><p>The following schematic gives an overview of the example:</p><p><img src="../../assets/cloudy_ces_schematic.png" alt="cloudy_schematic"/></p><p>The input to the CES algorithm consists of data <span>$y$</span>, the observational covariance <span>$Γ$</span>, and prior parameter distributions. The data, a vector of moments of the droplet mass distribution, are obtained by running Cloudy with the parameters set to their true values. The covariance is obtained from model output. The calibration stage is performed by an ensemble Kalman inversion (EKI), in which Cloudy has to be run once per iteration and for each ensemble member. The resulting input-output pairs <span>$\{\theta_i, \mathcal{G}(\theta_i)\}_i$</span> are used to train an emulator model. This emulator <span>$\widetilde{\mathcal{G}}(\theta)$</span> is cheap to evaluate; it replaces the original parameter-to-data map in the Markov chain Monte Carlo (MCMC) sampling, which produces (approximate) samples of the posterior parameter distribution. These samples are the final output of the CES algorithm. </p><p><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022MS002994">This paper</a> describes Cloudy in much more detail and shows results of experiments using CES to learn model parameters.</p><h3 id="Running-the-example"><a class="docs-heading-anchor" href="#Running-the-example">Running the example</a><a id="Running-the-example-1"></a><a class="docs-heading-anchor-permalink" href="#Running-the-example" title="Permalink"></a></h3><p><code>Cloudy_calibrate.jl</code> performs the calibration using ensemble Kalman inversion; <code>Cloudy_emulate_sample.jl</code> fits an emulator and uses it to sample the posterior distributions of the parameters. Once Cloudy is installed, the example can be run from the julia REPL:</p><pre><code class="language-julia hljs">include(&quot;Cloudy_calibrate.jl&quot;)
include(&quot;Cloudy_emulate_sample.jl&quot;)</code></pre><h3 id="Walkthrough-of-the-code:-Cloudy_calibrate.jl"><a class="docs-heading-anchor" href="#Walkthrough-of-the-code:-Cloudy_calibrate.jl">Walkthrough of the code: <code>Cloudy_calibrate.jl</code></a><a id="Walkthrough-of-the-code:-Cloudy_calibrate.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Walkthrough-of-the-code:-Cloudy_calibrate.jl" title="Permalink"></a></h3><p>This file performs the calibration stage of CES.</p><h4 id="Import-packagages-and-modules"><a class="docs-heading-anchor" href="#Import-packagages-and-modules">Import packagages and modules</a><a id="Import-packagages-and-modules-1"></a><a class="docs-heading-anchor-permalink" href="#Import-packagages-and-modules" title="Permalink"></a></h4><p>First we load standard packages,</p><pre><code class="language-julia hljs">using Distributions
using StatsBase
using LinearAlgebra
using StatsPlots
using Plots
using Plots.PlotMeasures
using Random
using JLD2</code></pre><p>the Cloudy modules,</p><pre><code class="language-julia hljs">using Cloudy
using Cloudy.ParticleDistributions
using Cloudy.KernelTensors

# Import the module that runs Cloudy
include(joinpath(@__DIR__, &quot;DynamicalModel.jl&quot;))</code></pre><p>and finally the EKP packages.</p><pre><code class="language-julia hljs">using EnsembleKalmanProcesses
using EnsembleKalmanProcesses.ParameterDistributions
using EnsembleKalmanProcesses.DataContainers
using EnsembleKalmanProcesses.PlotRecipes</code></pre><p>The module <code>DynamicalModel.jl</code> is the forward solver; it provides a function that runs Cloudy with a given instance of the parameter vector we want to learn.</p><pre><code class="language-julia hljs">include(&quot;DynamicalModel.jl&quot;)</code></pre><h4 id="Define-the-true-parameters"><a class="docs-heading-anchor" href="#Define-the-true-parameters">Define the true parameters</a><a id="Define-the-true-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-true-parameters" title="Permalink"></a></h4><p>We define the true parameters–-they are known here because this is a known truth example. Knowing the true parameters will allow us to assess how well Calibrate-Emulate-Sample has managed to solve the inverse problem.</p><p>We will assume that the true particle mass distribution is a Gamma distribution, which at time <span>$t = 0$</span> has parameters <span>$\phi_0 = [N_{0, 0}, k_0, \theta_0]$</span>. We will then try to learn these parameters from observations <span>$y = [M_0(t_{end}), M_1(t_{end}), M_2(t_{end})]$</span> of the zeroth-, first-, and second-order moments of the distribution at time <span>$t_{end} &gt; 0$</span> (where <code>t_end = 1.0</code> in this example). The true parameters <span>$\phi_{0, \text{true}}$</span> are defined as follows:</p><pre><code class="language-julia hljs">param_names = [&quot;N0&quot;, &quot;θ&quot;, &quot;k&quot;]
n_params = length(param_names)
N0_true = 300.0  # number of particles (scaling factor for Gamma distribution)
θ_true = 1.5597  # scale parameter of Gamma distribution
k_true = 0.0817  # shape parameter of Gamma distribution
ϕ_true = [N0_true, θ_true, k_true] # true parameters in constrained space
dist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)</code></pre><h4 id="Define-priors-for-the-parameters"><a class="docs-heading-anchor" href="#Define-priors-for-the-parameters">Define priors for the parameters</a><a id="Define-priors-for-the-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Define-priors-for-the-parameters" title="Permalink"></a></h4><p>As we are working with Bayesian methods, we treat the parameters we want to learn as random variables whose prior distributions we specify here. The prior distributions will behave like an &quot;initial guess&quot; for the likely region of parameter space where we expect the solution to be located. We use <code>constrained_gaussian</code> to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity (and numerical stability). </p><pre><code class="language-julia hljs"># We choose to use normal distributions to represent the prior distributions of
# the parameters in the transformed (unconstrained) space.
prior_N0 = constrained_gaussian(param_names[1], 400, 300, 0.4 * N0_true, Inf)
prior_θ = constrained_gaussian(param_names[2], 1.0, 5.0, 1e-1, Inf)
prior_k = constrained_gaussian(param_names[3], 0.2, 1.0, 1e-4, Inf)
priors = combine_distributions([prior_N0, prior_θ, prior_k])</code></pre><p>The plot recipe for <code>ParameterDistribution</code> types allows for quick visualization of the priors:</p><pre><code class="language-julia hljs"># Plot the priors
p = plot(priors, constrained=false)</code></pre><p><img src="../../assets/cloudy_priors.png" alt="priors"/></p><h4 id="Generate-(synthetic)-observations"><a class="docs-heading-anchor" href="#Generate-(synthetic)-observations">Generate (synthetic) observations</a><a id="Generate-(synthetic)-observations-1"></a><a class="docs-heading-anchor-permalink" href="#Generate-(synthetic)-observations" title="Permalink"></a></h4><p>We generate synthetic observations by running Cloudy 100 times with the true parameters (i.e., with the true initial Gamma distribution of droplet masses) and then adding noise to simulate measurement error.</p><pre><code class="language-julia hljs">dyn_model_settings_true = DynamicalModel.ModelSettings(
    kernel, dist_true, moments, tspan)

G_t = DynamicalModel.run_dyn_model(ϕ_true, dyn_model_settings_true)
n_samples = 100
y_t = zeros(length(G_t), n_samples)
# In a perfect model setting, the &quot;observational noise&quot; represents the 
# internal model variability. Since Cloudy is a purely deterministic model, 
# there is no straightforward way of coming up with a covariance structure 
# for this internal model variability. We decide to use a diagonal 
# covariance, with entries (variances) largely proportional to their 
# corresponding data values, G_t
Γy = convert(Array, Diagonal([100.0, 5.0, 30.0]))
μ = zeros(length(G_t))

# Add noise
for i in 1:n_samples
    y_t[:, i] = G_t .+ rand(MvNormal(μ, Γy))
end

truth = Observation(
    Dict(
        &quot;samples&quot; =&gt; vec(mean(y_t, dims = 2)),
        &quot;covariances&quot; =&gt; Γy,
        &quot;names&quot; =&gt; data_names,
    )
)</code></pre><h4 id="Perform-ensemble-Kalman-inversion"><a class="docs-heading-anchor" href="#Perform-ensemble-Kalman-inversion">Perform ensemble Kalman inversion</a><a id="Perform-ensemble-Kalman-inversion-1"></a><a class="docs-heading-anchor-permalink" href="#Perform-ensemble-Kalman-inversion" title="Permalink"></a></h4><p>We sample the initial ensemble from the prior and create the <code>EnsembleKalmanProcess</code> object as an ensemble Kalman inversion (EKI) algorithm using the <code>Inversion()</code> keyword. We also use the <code>DataMisfitController()</code> learning rate scheduler. The number of ensemble members must be larger than the dimension of the parameter space to ensure a full rank ensemble covariance.</p><pre><code class="language-julia hljs">N_ens = 50 # number of ensemble members
N_iter = 8 # number of EKI iterations
# initial parameters: n_params x N_ens
initial_params = construct_initial_ensemble(rng, priors, N_ens)
ekiobj = EnsembleKalmanProcess(
    initial_params,
    truth,
    Inversion(),
    scheduler=DataMisfitController()
)</code></pre><p>We perform the inversion loop. Remember that within calls to <code>get_ϕ_final</code> the EKP transformations are applied, thus the ensemble that is returned will be the gamma distribution parameters that can be used directly to run the forward model, rather than their corresponding values in the unconstrained space where the EKI takes place. Each ensemble member is stored as a column and therefore for uses such as plotting one needs to reshape to the desired dimension.</p><pre><code class="language-julia hljs"># Initialize a ParticleDistribution with dummy parameters. The parameters 
# will then be set within `run_dyn_model`
dummy = ones(n_params)
dist_type = ParticleDistributions.GammaPrimitiveParticleDistribution(dummy...)
model_settings = DynamicalModel.ModelSettings(kernel, dist_type, moments, tspan)
# EKI iterations
for n in 1:N_iter
    # Return transformed parameters in physical/constrained space
    ϕ_n = get_ϕ_final(priors, ekiobj)
    # Evaluate forward map
    G_n = [DynamicalModel.run_dyn_model(ϕ_n[:, i], model_settings) for i in 1:N_ens]
    G_ens = hcat(G_n...)  # reformat
    EnsembleKalmanProcesses.update_ensemble!(ekiobj, G_ens)
end</code></pre><h4 id="Visualize-and-store-the-results-of-the-calibration"><a class="docs-heading-anchor" href="#Visualize-and-store-the-results-of-the-calibration">Visualize and store the results of the calibration</a><a id="Visualize-and-store-the-results-of-the-calibration-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-and-store-the-results-of-the-calibration" title="Permalink"></a></h4><p>The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration, which is printed to standard output. An output directory is created, where a file <code>cloudy_calibrate_results.jld2</code> is stored, which contains all parameters and model output from the ensemble Kalman iterations (both as <code>DataContainers.DataContainer</code> objects), the mean and one sample of the synthetic observations, as well as the true prameters and their priors. In addition, an animation is produced that shows the evolution of the ensemble of particles over subsequent iterations of the optimization, both in the computational (unconstrained) and physical (constrained) spaces.</p><p><img src="../../assets/cloudy_eki.gif" alt="eki_iterations_animation"/></p><h3 id="Walkthrough-of-the-code:-Cloudy_emulate_sample.jl"><a class="docs-heading-anchor" href="#Walkthrough-of-the-code:-Cloudy_emulate_sample.jl">Walkthrough of the code: <code>Cloudy_emulate_sample.jl</code></a><a id="Walkthrough-of-the-code:-Cloudy_emulate_sample.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Walkthrough-of-the-code:-Cloudy_emulate_sample.jl" title="Permalink"></a></h3><p>This file performs the emulation and sampling stages of the CES algorithm.</p><h4 id="Import-packages-and-modules"><a class="docs-heading-anchor" href="#Import-packages-and-modules">Import packages and modules</a><a id="Import-packages-and-modules-1"></a><a class="docs-heading-anchor-permalink" href="#Import-packages-and-modules" title="Permalink"></a></h4><p>First, we import some standard packages</p><pre><code class="language-julia hljs">using Distributions
using StatsBase
using GaussianProcesses
using LinearAlgebra
using Random
using JLD2
ENV[&quot;GKSwstype&quot;] = &quot;100&quot;
using CairoMakie, PairPlots</code></pre><p>as well as the relevant CES packages needed to construct the emulators and perform the Markov chain Monte Carlo (MCMC) sampling. We also need some functionality of <code>EnsembleKalmanProcesses.jl</code>.</p><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
using CalibrateEmulateSample.MarkovChainMonteCarlo
using CalibrateEmulateSample.Utilities
using EnsembleKalmanProcesses
using EnsembleKalmanProcesses.ParameterDistributions
using EnsembleKalmanProcesses.DataContainers</code></pre><h4 id="Load-the-calibration-results"><a class="docs-heading-anchor" href="#Load-the-calibration-results">Load the calibration results</a><a id="Load-the-calibration-results-1"></a><a class="docs-heading-anchor-permalink" href="#Load-the-calibration-results" title="Permalink"></a></h4><p>We will train the emulator on the input-output pairs we obtained during the calibration. They are stored within the <code>EnsembleKalmanProcess</code> object (<code>ekiobj</code>), which is loaded here together with the other information that was saved in the calibration step.</p><pre><code class="language-julia hljs">ekiobj = load(data_save_file)[&quot;eki&quot;]
priors = load(data_save_file)[&quot;priors&quot;]
truth_sample_mean = load(data_save_file)[&quot;truth_sample_mean&quot;]
truth_sample = load(data_save_file)[&quot;truth_sample&quot;]
# True parameters:
# - ϕ: in constrained space
# - θ: in unconstrained space
ϕ_true = load(data_save_file)[&quot;truth_input_constrained&quot;]
θ_true = transform_constrained_to_unconstrained(priors, ϕ_true)</code></pre><p>The user can choose one or more of the following three emulators: a Gaussian Process (GP) emulator with <code>GaussianProcesses.jl</code> interface (<code>gp-gpjl</code>), a scalar Random Feature (RF) interface (<code>rf-scalar</code>), and vector RF with a nonseparable, nondiagonal kernel structure in the output space (<code>rf-nosvd-nonsep</code>). See  <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/examples/emulators/regression_2d_2d">here</a> for a complete overview of the available emulators. </p><pre><code class="language-julia hljs">cases = [
    &quot;rf-scalar&quot;,
    &quot;gp-gpjl&quot;,  # Veeeery slow predictions
    &quot;rf-nosvd-nonsep&quot;
]</code></pre><p>We first define some settings for the two emulators, e.g., the prediction type for the GP emulator, or the number of features and hyperparameter optimizer options for the RF emulator. The docs for <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/GaussianProcessEmulator/">GPs</a> and <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/random_feature_emulator/">RFs</a> explain the different options in more detail and provide some useful heuristics for how to customize the settings depending on the problem at hand.</p><pre><code class="language-julia hljs"># These settings are the same for all Gaussian Process cases
pred_type = YType() # we want to predict data

# These settings are the same for all Random Feature cases
n_features = 400 
nugget = 1e-8
optimizer_options = Dict(
    &quot;verbose&quot; =&gt; true,
    &quot;scheduler&quot; =&gt; DataMisfitController(terminate_at = 100.0),
    &quot;cov_sample_multiplier&quot; =&gt; 1.0,
    &quot;n_iteration&quot; =&gt; 20,
)</code></pre><p>Emulation is performed through the construction of an <code>Emulator</code> object from the following components:</p><ul><li>a wrapper for the machine learning tool (<code>mlt</code>) to be used as emulator</li><li>the input-output pairs on which the emulator will be trained</li><li>optional arguments specifying data processing and dimensionality reduction functionality </li></ul><p>For <code>gp-gpjl</code>, this looks as follows:</p><pre><code class="language-julia hljs">gppackage = GPJL()
# Kernel is the sum of a squared exponential (SE), Matérn 5/2, and white noise
gp_kernel = SE(1.0, 1.0) + Mat52Ard(zeros(3), 0.0) + Noise(log(2.0))

# Wrapper for GP
mlt = GaussianProcess(
    gppackage;
    kernel = gp_kernel,
    prediction_type = pred_type,
    noise_learn = false,
)

decorrelate = true
standardize_outputs = true</code></pre><p>And similarly for <code>rf-scalar</code></p><pre><code class="language-julia hljs">kernel_rank = 3
kernel_structure = SeparableKernel(
    LowRankFactor(kernel_rank, nugget),
    OneDimFactor()
)

mlt = ScalarRandomFeatureInterface(
    n_features,
    n_params,
    kernel_structure = kernel_structure,
    optimizer_options = optimizer_options,
)

decorrelate = true
standardize_outputs = true</code></pre><p>and for <code>rf-nosvd-nonsep</code>:</p><pre><code class="nohighlight hljs">kernel_rank = 4
mlt = VectorRandomFeatureInterface(
    n_features,
    n_params,
    n_outputs,
    kernel_structure = NonseparableKernel(LowRankFactor(kernel_rank, nugget)),
    optimizer_options = optimizer_options
)

# Vector RF does not require decorrelation of outputs
decorrelate = false
standardize_outputs = false</code></pre><p>We construct the emulator using the input-output pairs obtained in the calibration stage (note that we&#39;re not using all available input-output pairs–-using all of them may not give the best results, especially if the EKI parameter converges rapidly and then &quot;stays in the same place&quot; during the remaining iterations). For the <code>gp-gpjl</code> and <code>rf-scalar</code> cases, we want the output data to be decorrelated with information from Γy, but for the vector RF case decorrelation is not required.</p><pre><code class="nohighlight hljs">input_output_pairs = get_training_points(ekiobj,
                                          length(get_u(ekiobj))-2)

# Use the medians of the outputs as standardizing factors
norm_factors = get_standardizing_factors(
    get_outputs(input_output_pairs)
)

# The data processing normalizes input data, and decorrelates
# output data with information from Γy, if required
# Note: The `standardize_outputs_factors` are only used under the
# condition that `standardize_outputs` is true.
emulator = Emulator(
    mlt,
    input_output_pairs,
    decorrelate = decorrelate,
    obs_noise_cov = Γy,
    standardize_outputs = true,
    standardize_outputs_factors = vcat(norm_factors...),
)</code></pre><h4 id="Train-the-emulator"><a class="docs-heading-anchor" href="#Train-the-emulator">Train the emulator</a><a id="Train-the-emulator-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-emulator" title="Permalink"></a></h4><p>The emulator is trained when we combine the machine learning tool and the data into the <code>Emulator</code> above. We must also optimize the hyperparameters:</p><pre><code class="language-julia hljs">optimize_hyperparameters!(emulator)</code></pre><p>To test the predictive skill of the emulator, we can e.g. compare its prediction on the true parameters to the true data. (One could also hold out a subset of the input-output pairs from the training and evaluate the emulator&#39;s predictions on them).</p><pre><code class="language-julia hljs"># Check how well the emulator predicts on the true parameters
y_mean, y_var = Emulators.predict(
    emulator,
    reshape(θ_true, :, 1);
    transform_to_real = true
)

println(&quot;Emulator ($(case)) prediction on true parameters: &quot;)
println(vec(y_mean))
println(&quot;true data: &quot;)
println(truth_sample) # what was used as truth</code></pre><p>The emulator predicts both a mean value and a covariance.</p><h3 id="Sample-the-posterior-distributions-of-the-parameters"><a class="docs-heading-anchor" href="#Sample-the-posterior-distributions-of-the-parameters">Sample the posterior distributions of the parameters</a><a id="Sample-the-posterior-distributions-of-the-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-the-posterior-distributions-of-the-parameters" title="Permalink"></a></h3><p>The last step is to plug the emulator into an MCMC algorithm, which is then used to produce samples from the posterior distribution of the parameters. Essentially, the emulator acts as a stand-in for the original forward model (which in most cases of interest is computationally expensive to run) during the MCMC sampling process.</p><p>We use the mean across all parameter ensembles from the EKI as the initial parameters for the MCMC. Before running the actual MCMC chain, we determine a good step size by running chains of length <code>N = 2000</code>:</p><pre><code class="language-julia hljs"># initial values
u0 = vec(mean(get_inputs(input_output_pairs), dims = 2))
println(&quot;initial parameters: &quot;, u0)

# First let&#39;s run a short chain to determine a good step size
yt_sample = truth_sample
mcmc = MCMCWrapper(
    RWMHSampling(),
    truth_sample,
    priors,
    emulator;
    init_params = u0
)

new_step = optimize_stepsize(
    mcmc;
    init_stepsize = 0.1,
    N = 2000,
    discard_initial = 0
)</code></pre><p>We choose a sample size of 100,000 for the actual MCMC, discarding the first 1,000 samples as burn-in:</p><pre><code class="language-julia hljs"># Now begin the actual MCMC
println(&quot;Begin MCMC - with step size &quot;, new_step)
chain = MarkovChainMonteCarlo.sample(
    mcmc,
    100_000;
    stepsize = new_step,
    discard_initial = 1_000
)</code></pre><p>After running the MCMC, we can extract the posterior samples as follows:</p><pre><code class="language-julia hljs">posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)</code></pre><p>The samples of the posterior distributions represent the ultimate output of the CES process. By constructing histograms of these samples and comparing them with the known true parameter values, we can evaluate the results&#39; accuracy. Ideally, the peak of the posterior distribution should be located near the true values, indicating a high-quality estimation. Additionally, visualizing the prior distributions alongside the posteriors shows the distributional change effected by the Bayesian learning process.</p><h3 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h3><p>We first produce pair plots (also known as corner plots or scatter plot matrices) to visualize the posterior parameter distributions as a grid of histograms. Recall that the task was to solve the inverse problem of finding the parameters <span>$N_{0, 0}$</span>, <span>$k_0$</span>, and <span>$\theta_0$</span>, which define a gamma distribution of droplet masses in Cloudy at time <span>$t = 0$</span>.</p><pre><code class="language-julia hljs"># Make pair plots of the posterior distributions in the unconstrained
# and in the constrained space (this uses `PairPlots.jl`)
figpath_unconstr = joinpath(output_directory,
                            &quot;joint_posterior_unconstr.png&quot;)
figpath_constr = joinpath(output_directory,
                          &quot;joint_posterior_constr.png&quot;)
labels = get_name(posterior)

data_unconstr = (; [(Symbol(labels[i]),
                     posterior_samples_unconstr[i, :]) for i in 1:length(labels)]...)
data_constr = (; [(Symbol(labels[i]),
                   posterior_samples_constr[i, :]) for i in 1:length(labels)]...)

p_unconstr = pairplot(data_unconstr =&gt; (PairPlots.Scatter(),))
p_constr = pairplot(data_constr =&gt; (PairPlots.Scatter(),))</code></pre><p>For the GP emulator, the results (shown in the constrained/physical space) look as follows:</p><p><img src="../../assets/cloudy_pairplot_posterior_constr_gp-gpjl.png" alt="pairplot_posterior_gpjl"/></p><p>And we can plot the same for the scalar RF emulator...</p><p><img src="../../assets/cloudy_pairplot_posterior_constr_rf-scalar.png" alt="pairplot_posterior_rf-scalar"/></p><p>...and for the vector RF emulator:</p><p><img src="../../assets/cloudy_pairplot_posterior_constr_rf-nosvd-nonsep.png" alt="pairplot_posterior_rf-vec"/></p><p>In addition, we plot the marginals of the posterior distributions–-we are showing them here for the GP emulator case:</p><pre><code class="language-julia hljs">for idx in 1:n_params

    # Find the range of the posterior samples
    xmin = minimum(posterior_samples_constr[idx, :])
    xmax = maximum(posterior_samples_constr[idx, :])

    # Create a figure and axis for plotting
    fig = Figure(; size = (800, 600))
    ax = Axis(fig[1, 1])

    # Histogram for posterior samples
    hist!(ax, posterior_samples_constr[idx, :], bins = 100,
          color = :darkorange, label = &quot;posterior&quot;)

    # Plotting the prior distribution
    hist!(ax, prior_samples_constr[idx, :], bins = 10000,
          color = :slategray)

    # Adding a vertical line for the true value
    vlines!(ax, [ϕ_true[idx]], color = :indigo, linewidth = 2.6,
            label = &quot;true &quot; * param_names[idx])

    xlims!(ax, xmin, xmax)
    ylims!(ax, 0, Inf)

    # Setting title and labels
    ax.title = param_names[idx]
    ax.xlabel = &quot;Value&quot;
    ax.ylabel = &quot;Density&quot;</code></pre><p>This is what the marginal distributions of the three parameters look like, for the case of the GP emulator, and in the constrained/physical space:</p><p><img src="../../assets/cloudy_marginal_posterior_constr_gp-gpjl_N0.png" alt="posterior_N0_gpjl"/></p><p><img src="../../assets/cloudy_marginal_posterior_constr_gp-gpjl_theta.png" alt="posterior_theta_gpjl"/></p><p><img src="../../assets/cloudy_marginal_posterior_constr_gp-gpjl_k.png" alt="posterior_k_gpjl"/></p><p>Here, the posterior distributions are shown as orange histograms, the prior distribution are shown as grey histograms (though with the exception of the parmaeter <code>k</code>, they are barely visible), and the true parameter values are marked as vertical purple lines.</p><h3 id="Appendix:-What-Does-Cloudy-Do?"><a class="docs-heading-anchor" href="#Appendix:-What-Does-Cloudy-Do?">Appendix: What Does Cloudy Do?</a><a id="Appendix:-What-Does-Cloudy-Do?-1"></a><a class="docs-heading-anchor-permalink" href="#Appendix:-What-Does-Cloudy-Do?" title="Permalink"></a></h3><p>For the purpose of Bayesian parameter learning, the forward model can be treated as a black box that processes input parameters to yield specific outputs. However, for those who wish to learn more about the inner workings of Cloudy, we refer to <a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022MS002994">his paper</a> and offer a brief outline below:</p><p>The mathematical starting point of <a href="https://github.com/CliMA/Cloudy.jl.git">Cloudy</a> is the stochastic collection equation (SCE; sometimes also called <a href="https://en.wikipedia.org/wiki/Smoluchowski_coagulation_equation#:~:text=In%20statistical%20physics%2C%20the%20Smoluchowski,size%20x%20at%20time%20t.">Smoluchowski equation</a> after Marian Smoluchowski), which describes the time rate of change of <span>$f = f(m, t)$</span>, the mass distribution function of liquid water droplets, due to the process of collision and coalescence. The distribution function <span>$f$</span> depends on droplet mass <span>$m$</span> and time <span>$t$</span> and is defined such that <span>$f(m) \text{ d}m$</span> denotes the number of droplets with masses in the interval <span>$[m, m + dm]$</span> per unit volume. </p><p>The stochastic collection equation is an integro-differential equation that can be written as </p><p class="math-container">\[    \frac{\partial f(m, t)}{\partial t} = \frac{1}{2} \int_{m&#39;=0}^{\infty} f(m&#39;, t) f(m-m&#39;, t)  \mathcal{C}(m&#39;, m-m&#39;)\text{d}m&#39; - f(m, t) \int_{m&#39;=0}^\infty f(m&#39;, t)\mathcal{C}(m, m&#39;) \text{d}m&#39;, \]</p><p>where <span>$\mathcal{C}(m&#39;, m&#39;&#39;)$</span> is the collision-coalescence kernel, which  encapsulates the physics of droplet-droplet interactions – it describes the rate at which two drops of masses <span>$m&#39;$</span> and <span>$m&#39;&#39;$</span> come into contact and coalesce into a drop of mass <span>$m&#39; + m&#39;&#39;$</span>. The first term on the right-hand side of the SCE describes the rate of increase of the number of drops having a mass <span>$m$</span> due to collision and coalescence of drops of masses <span>$m&#39;$</span> and <span>$m-m&#39;$</span> (where the factor <span>$\frac{1}{2}$</span> avoids double counting), while the second term describes the rate of reduction of drops of mass <span>$m$</span> due to collision and coalescence of drops having a mass <span>$m$</span> with other drops. </p><p>We can rewrite the SCE in terms of the moments <span>$M_k$</span> of <span>$f$</span>, which are the prognostic variables in Cloudy. They are defined by</p><p class="math-container">\[    M_k = \int_0^\infty m^k f(m, t) \text{d}m\]</p><p>The time rate of change of the k-th moment of <span>$f$</span> is obtained by multiplying the SCE by <span>$m^k$</span> and integrating over the entire range of droplet masses (from <span>$m=0$</span> to <span>$\infty$</span>), which yields</p><p class="math-container">\[    \frac{\partial M_k(t)}{\partial t} = \frac{1}{2}\int_0^\infty \left((m+m&#39;)^k - m^k - {m&#39;}^k\right) \mathcal{C}(m, m&#39;)f(m, t)f(m&#39;, t) \, \text{d}m\, \text{d}m&#39; ~~~~~~~~ (1)\]</p><p>In this example, the kernel is set to be constant – <span>$\mathcal{C}(m&#39;, m&#39;&#39;) = B = \text{const}$</span> – and the cloud droplet mass distribution is assumed to be a <span>$\text{Gamma}(k_t, \theta_t)$</span> distribution, scaled by a factor <span>$N_{0,t}$</span> which denotes the droplet number concentration:</p><p class="math-container">\[f(m, t) = \frac{N_{0,t}}{\Gamma(k_t)\theta_t^k} m^{k_t-1} \exp{(-m/\theta_t)}\]</p><p>The parameter vector <span>$\phi_t= [N_{0,t}, k_t, \theta_t]$</span> changes over time (as indicated by the subscript <span>$t$</span>), as the shape of the distribution evolves. In fact, there is a priori no reason to assume that the distribution would retain its Gamma shape over time, but this is a common assumption that is made in order to solve the closure problem (without this assumption, one would have to keep track of infinitely many moments of the mass distribution in order to uniquely identify the distribution <span>$f$</span> at each time step, which is obviously not practicable).</p><p>For Gamma mass distribution functions, specifying the first three moments (<span>$M_0$</span>, <span>$M_1$</span>, and <span>$M_2$</span>) is sufficient to uniquely determine the parameter vector <span>$\phi_t$</span>, hence Cloudy solves equation (1) for <span>$k = 0, 1, 2$</span>. This mapping of the parameters of the initial cloud droplet mass distribution to the (zeroth-, first-, and second-order) moments of the distribution at a specified end time is done by <code>DynamicalModel.jl</code>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../edmf_example/">« Turbulence example</a><a class="docs-footer-nextpage" href="../emulators/regression_2d_2d/">Regression of <span>$\mathbb{R}^2 \to \mathbb{R}^2$</span> smooth function »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Monday 10 February 2025 20:43">Monday 10 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
