var documenterSearchIndex = {"docs":
[{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to CalibrateEmulateSample! We encourage opening issues and pull requests (PRs).","category":"page"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The easiest way to contribute is by using CalibrateEmulateSample, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of CalibrateEmulateSample insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition.","category":"page"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you are unfamiliar with git and version control, the following guides will be helpful:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Atlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"page"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Create your own fork of CalibrateEmulateSample on GitHub and check out your copy:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git clone https://github.com/<your-username>/CalibrateEmulateSample.jl.git\n$ cd CalibrateEmulateSample.jl","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Now you have access to your fork of CalibrateEmulateSample through origin. Create a branch for your feature; this will hold your contribution:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout -b <branchname>","category":"page"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"page"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"julia --project=.dev .dev/climaformat.jl .","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"from the CalibrateEmulateSample.jl directory.","category":"page"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with CalibrateEmulateSample.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To make sure you are up to date with main, you can use the following workflow:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To squash your commits, you can use the following command:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~n","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"where n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~4","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"will open the following file in (typically) vim:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Then in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git push -u origin <name_of_local_branch> --force","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can find more information about squashing here.","category":"page"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Currently a number of checks are run per commit for a given PR.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"JuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Unit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"page"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use bors to manage merging PR's in the the CalibrateEmulateSample repo. If you're a collaborator and have the necessary permissions, you can type bors try in a comment on a PR to have integration test suite run on that PR, or bors r+ to try and merge the code.  Bors ensures that all integration tests for a given PR always pass before merging into main. The integration tests currently run example cases in examples/. Any breaking changes will need to also update the examples/, else bors will fail.","category":"page"},{"location":"API/AbstractMCMC/#AbstractMCMC-sampling-API","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The \"sample\" part of CES refers to exact sampling from the emulated posterior via Markov chain Monte Carlo (MCMC). Within this paradigm, we want to provide the flexibility to use multiple sampling algorithms; the approach we take is to use the general-purpose AbstractMCMC.jl API, provided by the Turing.jl probabilistic programming framework.","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"This page provides a summary of AbstractMCMC which augments the existing documentation ([1], [2]) and highlights how it's used by the CES package in MarkovChainMonteCarlo. It's not meant to be a complete description of the AbstractMCMC package.","category":"page"},{"location":"API/AbstractMCMC/#Use-in-MarkovChainMonteCarlo","page":"AbstractMCMC sampling API","title":"Use in MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"At present, Turing has limited support for derivative-free optimization, so we only use this abstract API and not Turing itself. We also use two related dependencies, AdvancedMH and MCMCChains. ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Julia's philosophy is to use small, composable packages rather than monoliths, but this can make it difficult to remember where methods are defined! Below we describe the relevant parts of ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The AbstractMCMC API,\nExtended to the case of Metropolis-Hastings (MH) sampling by AdvancedMH,\nFurther extended for the needs of CES in Markov chain Monte Carlo.","category":"page"},{"location":"API/AbstractMCMC/#Classes-and-methods","page":"AbstractMCMC sampling API","title":"Classes and methods","text":"","category":"section"},{"location":"API/AbstractMCMC/#Sampler","page":"AbstractMCMC sampling API","title":"Sampler","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"A Sampler is AbstractMCMC's term for an implementation of a MCMC sampling algorithm, along with all its configuration parameters. All samplers must inherit from AbstractMCMC.AbstractSampler. ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Currently CES only implements the Metropolis-Hastings (MH) algorithm. Because it's so straightforward, much of AbstractMCMC isn't needed. We implement two variants of MH with two different Samplers: RWMetropolisHastings and pCNMetropolisHastings, both of which inherit from the AdvancedMH.MHSampler base class. The public constructor for both Samplers is MetropolisHastingsSampler; the different Samplers are specified by passing a MCMCProtocol object to this constructor.","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The MH Sampler classes have only one field, proposal, which is the distribution used to generate new MH proposals via stochastic offsets to the current parameter values. This is done by AdvancedMH.propose(), which gets called for each MCMC step() (below). The difference between our two Samplers is in how this proposal is generated:","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"RWMHSampling does vanilla random-walk proposal generation with a constant, user-specified step size (this differs from the AdvancedMH implementation, which doesn't provide for a step size.)\npCNMHSampling for preconditioned Crank-Nicholson proposals. Vanilla random walk sampling doesn't have a well-defined limit for high-dimensional parameter spaces; pCN replaces the random walk with an Ornstein–Uhlenbeck [AR(1)] process so that the Metropolis acceptance probability remains non-zero in this limit. See Beskos et. al. (2008) and Cotter et. al. (2013).","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"This is the only difference: generated proposals are then either accepted or rejected according to the same MH criterion (in step(), below.)","category":"page"},{"location":"API/AbstractMCMC/#Models","page":"AbstractMCMC sampling API","title":"Models","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"In Turing, the Model is the distribution one performs inference on, which may involve observed and hidden variables and parameters. For CES, we simply want to sample from the posterior, so our Model distribution is simply the emulated likelihood (see Emulators) together with the prior. This is constructed by EmulatorPosteriorModel.","category":"page"},{"location":"API/AbstractMCMC/#Sampling-with-the-MCMC-Wrapper-object","page":"AbstractMCMC sampling API","title":"Sampling with the MCMC Wrapper object","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"At a high level, a Sampler and Model is all that's needed to do MCMC sampling. This is done by the sample method provided by AbstractMCMC (extending the method from BaseStats). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"To be more user-friendly, in CES we wrap the Sampler, Model and other necessary configuration into a MCMCWrapper object. The constructor for this object ensures that all its components are created consistently, and performs necessary bookkeeping, such as converting coordinates to the decorrelated basis. We extend sample with methods to use this object (that simply unpack its fields and call the appropriate method from AbstractMCMC.)","category":"page"},{"location":"API/AbstractMCMC/#Chain","page":"AbstractMCMC sampling API","title":"Chain","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The MCMCChain class is used to store the results of the MCMC sampling; the package provides simple diagnostics for visualization and diagnosing chain convergence.","category":"page"},{"location":"API/AbstractMCMC/#Internals:-Transitions","page":"AbstractMCMC sampling API","title":"Internals: Transitions","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Implementing MCMC involves defining states and transitions of a Markov process (whose stationary distribution is what we seek to sample from). AbstractMCMC's terminology is a bit confusing for the MH case; states of the chain are described by Transition objects, which contain the current sample (and other information like its log-probability). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"AdvancedMH defines an AbstractTransition base class for use with its methods; we implement our own child class, MCMCState, in order to record statistics on the MH acceptance ratio.","category":"page"},{"location":"API/AbstractMCMC/#Internals:-Markov-steps","page":"AbstractMCMC sampling API","title":"Internals: Markov steps","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Markov transitions of the chain are defined by overloading AbstractMCMC's step method, which takes the Sampler and current Transition and implements the Sampler's logic to returns an updated Transition representing the chain's new state (actually, a pair of Transitions, for cases where the Sampler doesn't obey detailed balance; this isn't relevant for us). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"For example, in Metropolis-Hastings sampling this is where we draw a proposal sample and accept or reject it according to the MH criterion. AdvancedMH implements this here; we re-implement this method because 1) we need to record whether a proposal was accepted or rejected, and 2) our calls to propose() are stepsize-dependent.","category":"page"},{"location":"installation_instructions/#Installation-Instructions","page":"Installation instructions","title":"Installation Instructions","text":"","category":"section"},{"location":"installation_instructions/#Installing-CalibrateEmulateSample.jl","page":"Installation instructions","title":"Installing CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Currently CalibrateEmulateSample (CES) depends on some external python dependencies  including scikit-learn wrapped by ScikitLearn.jl, which requires a couple extra  installation steps:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"First clone the project into a new local repository","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> git clone git@github.com:Clima/CalibrateEmulateSample.jl\n> cd CalibrateEmulateSample.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Install and build the project dependencies. Given that CES depends on python packages  it is easiest to set the project to use its own  Conda environment variable (set by exporting the ENV variable PYTHON=\"\").","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The scikit-learn package (along with scipy) then has to be installed if using a Julia project-specific Conda environment:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scipy=1.8.1\", channel=\"conda-forge\")'\n> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scikit-learn=1.1.1\")'\n","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"See the PyCall.jl documentation  for more information about how to configure the local Julia / Conda / Python environment. Typically it will require building in the  REPL via","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project\njulia> using Pkg\njulia> Pkg.build(\"PyCall\")","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To test that the package is working:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You need to first build the top-level project before building the documentation:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"cd CalibrateEmulateSample.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Then you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project=docs/ -e 'using Pkg; Pkg.instantiate()'\njulia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html.","category":"page"},{"location":"API/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"Modules = [CalibrateEmulateSample.Utilities]\nOrder   = [:module, :type, :function]","category":"page"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_obs_sample-Union{Tuple{IT}, Tuple{Random.AbstractRNG, EnsembleKalmanProcesses.Observations.Observation}} where IT<:Int64","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_obs_sample","text":"get_obs_sample(\n    rng::Random.AbstractRNG,\n    obs::EnsembleKalmanProcesses.Observations.Observation;\n    rng_seed\n) -> Any\n\n\nReturn a random sample from the observations, for use in the MCMC.\n\nrng - optional RNG object used to pick random sample; defaults to Random.GLOBAL_RNG.\nobs - Observation struct with the observations (extract will pick one         of the sample observations to train).\nrng_seed - optional kwarg; if provided, used to re-seed rng before sampling.\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_training_points-Union{Tuple{P}, Tuple{IT}, Tuple{FT}, Tuple{EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, Union{AbstractVector{IT}, IT}}} where {FT, IT, P}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_training_points","text":"get_training_points(\n    ekp::EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P},\n    train_iterations::Union{AbstractVector{IT}, IT} where IT\n) -> EnsembleKalmanProcesses.DataContainers.PairedDataContainer\n\n\nExtract the training points needed to train the Gaussian process regression.\n\nekp - EnsembleKalmanProcess holding the parameters and the data that were produced during the Ensemble Kalman (EK) process.\ntrain_iterations - Number (or indices) EK layers/iterations to train on.\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.posdef_correct-Tuple{AbstractMatrix}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.posdef_correct","text":"posdef_correct(mat::AbstractMatrix; tol) -> Any\n\n\nMakes square matrix mat positive definite, by symmetrizing and bounding the minimum eigenvalue below by tol\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#RandomFeatures","page":"Random Features","title":"RandomFeatures","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/RandomFeatures/#Scalar-interface","page":"Random Features","title":"Scalar interface","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"ScalarRandomFeatureInterface\nScalarRandomFeatureInterface(::Int,::Int)\nbuild_models!(::ScalarRandomFeatureInterface, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict(::ScalarRandomFeatureInterface, ::M) where {M <: AbstractMatrix}","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","page":"Random Features","title":"CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","text":"struct ScalarRandomFeatureInterface{S<:AbstractString, RNG<:Random.AbstractRNG, KST<:CalibrateEmulateSample.Emulators.KernelStructureType} <: CalibrateEmulateSample.Emulators.RandomFeatureInterface\n\nStructure holding the Scalar Random Feature models. \n\nFieldsWhen calibrated with ocean LES,\n\nrfms::Vector{RandomFeatures.Methods.RandomFeatureMethod}: vector of RandomFeatureMethods, contains the feature structure, batch-sizes and regularization\nfitted_features::Vector{RandomFeatures.Methods.Fit}: vector of Fits, containing the matrix decomposition and coefficients of RF when fitted to data\nbatch_sizes::Union{Nothing, Dict{S, Int64}} where S<:AbstractString: batch sizes\nn_features::Union{Nothing, Int64}: n_features\ninput_dim::Int64: input dimension\nrng::Random.AbstractRNG: choice of random number generator\nkernel_structure::CalibrateEmulateSample.Emulators.KernelStructureType: Kernel structure type (e.g. Separable or Nonseparable)\nfeature_decomposition::AbstractString: Random Feature decomposition, choose from \"svd\" or \"cholesky\" (default)\noptimizer_options::Dict{S} where S<:AbstractString: dictionary of options for hyperparameter optimizer\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface-Tuple{Int64, Int64}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface","text":"ScalarRandomFeatureInterface(\n    n_features::Int64,\n    input_dim::Int64;\n    kernel_structure,\n    batch_sizes,\n    rng,\n    feature_decomposition,\n    optimizer_options\n) -> CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface{String, Random._GLOBAL_RNG}\n\n\nConstructs a ScalarRandomFeatureInterface <: MachineLearningTool interface for the RandomFeatures.jl package for multi-input and single- (or decorrelated-)output emulators.\n\nn_features - the number of random features\ninput_dim - the dimension of the input space\nkernel_structure -  - a prescribed form of kernel structure\nbatch_sizes = nothing - Dictionary of batch sizes passed RandomFeatures.jl object (see definition there)\nrng = Random.GLOBAL_RNG - random number generator \nfeature_decomposition = \"cholesky\" - choice of how to store decompositions of random features, cholesky or svd available\noptimizer_options = nothing - Dict of options to pass into EKI optimization of hyperparameters (defaults created in ScalarRandomFeatureInterface constructor):\n\"prior\":  the prior for the hyperparameter optimization \n\"priorinscale\": use this to tune the input prior scale\n\"n_ensemble\":  number of ensemble members\n\"n_iteration\":  number of eki iterations\n\"covsamplemultiplier\": increase for more samples to estimate covariance matrix in optimization (default 10.0, minimum 0.0)  \n\"scheduler\": Learning rate Scheduler (a.k.a. EKP timestepper) Default: DataMisfitController\n\"tikhonov\":  tikhonov regularization parameter if >0\n\"inflation\":  additive inflation ∈ [0,1] with 0 being no inflation\n\"train_fraction\":  e.g. 0.8 (default)  means 80:20 train - test split\n\"multithread\": how to multithread. \"ensemble\" (default) threads across ensemble members \"tullio\" threads random feature matrix algebra\n\"verbose\" => false, verbose optimizer statements\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat}\n)\n\n\nBuilds the random feature method from hyperparameters. We use cosine activation functions and a Multivariate Normal distribution (from Distributions.jl) with mean M=0, and input covariance U built with the CovarianceStructureType.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#GaussianProcesses.predict-Union{Tuple{M}, Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface, M}} where M<:(AbstractMatrix)","page":"Random Features","title":"GaussianProcesses.predict","text":"predict(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    new_inputs::AbstractMatrix;\n    multithread\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#Vector-Interface","page":"Random Features","title":"Vector Interface","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"VectorRandomFeatureInterface\nVectorRandomFeatureInterface(::Int, ::Int, ::Int)\nbuild_models!(::VectorRandomFeatureInterface, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\npredict(::VectorRandomFeatureInterface, ::M) where {M <: AbstractMatrix}","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","page":"Random Features","title":"CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","text":"struct VectorRandomFeatureInterface{S<:AbstractString, RNG<:Random.AbstractRNG, KST<:CalibrateEmulateSample.Emulators.KernelStructureType} <: CalibrateEmulateSample.Emulators.RandomFeatureInterface\n\nStructure holding the Vector Random Feature models. \n\nFields\n\nrfms::Vector{RandomFeatures.Methods.RandomFeatureMethod}: A vector of RandomFeatureMethods, contains the feature structure, batch-sizes and regularization\nfitted_features::Vector{RandomFeatures.Methods.Fit}: vector of Fits, containing the matrix decomposition and coefficients of RF when fitted to data\nbatch_sizes::Union{Nothing, Dict{S, Int64}} where S<:AbstractString: batch sizes\nn_features::Union{Nothing, Int64}: number of features\ninput_dim::Int64: input dimension\noutput_dim::Int64: output_dimension\nrng::Random.AbstractRNG: rng\nregularization::Vector{Union{Matrix, LinearAlgebra.Diagonal, LinearAlgebra.UniformScaling}}: regularization\nkernel_structure::CalibrateEmulateSample.Emulators.KernelStructureType: Kernel structure type (e.g. Separable or Nonseparable)\nfeature_decomposition::AbstractString: Random Feature decomposition, choose from \"svd\" or \"cholesky\" (default)\noptimizer_options::Dict: dictionary of options for hyperparameter optimizer\n\n\n\n\n\n","category":"type"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface-Tuple{Int64, Int64, Int64}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface","text":"VectorRandomFeatureInterface(\n    n_features::Int64,\n    input_dim::Int64,\n    output_dim::Int64;\n    kernel_structure,\n    batch_sizes,\n    rng,\n    feature_decomposition,\n    optimizer_options\n) -> CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface{String, Random._GLOBAL_RNG}\n\n\nConstructs a VectorRandomFeatureInterface <: MachineLearningTool interface for the RandomFeatures.jl package for multi-input and multi-output emulators.\n\nn_features - the number of random features\ninput_dim - the dimension of the input space\noutput_dim - the dimension of the output space\nkernel_structure -  - a prescribed form of kernel structure\nbatch_sizes = nothing - Dictionary of batch sizes passed RandomFeatures.jl object (see definition there)\nrng = Random.GLOBAL_RNG - random number generator \nfeature_decomposition = \"cholesky\" - choice of how to store decompositions of random features, cholesky or svd available\noptimizer_options = nothing - Dict of options to pass into EKI optimization of hyperparameters (defaults created in VectorRandomFeatureInterface constructor):\n\"prior\": the prior for the hyperparameter optimization\n\"priorinscale\"/\"prioroutscale\": use these to tune the input/output prior scale.\n\"n_ensemble\": number of ensemble members\n\"n_iteration\": number of eki iterations\n\"scheduler\": Learning rate Scheduler (a.k.a. EKP timestepper) Default: DataMisfitController\n\"covsamplemultiplier\": increase for more samples to estimate covariance matrix in optimization (default 10.0, minimum 0.0) \n\"tikhonov\": tikhonov regularization parameter if > 0\n\"inflation\": additive inflation ∈ [0,1] with 0 being no inflation\n\"train_fraction\": e.g. 0.8 (default)  means 80:20 train - test split\n\"multithread\": how to multithread. \"ensemble\" (default) threads across ensemble members \"tullio\" threads random feature matrix algebra\n\"verbose\" => false, verbose optimizer statements to check convergence, priors and optimal parameters.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Random Features","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat};\n    regularization_matrix\n)\n\n\nBuild Vector Random Feature model for the input-output pairs subject to regularization, and optimizes the hyperparameters with EKP. \n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#GaussianProcesses.predict-Union{Tuple{M}, Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface, M}} where M<:(AbstractMatrix)","page":"Random Features","title":"GaussianProcesses.predict","text":"predict(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    new_inputs::AbstractMatrix\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#Other-utilities","page":"Random Features","title":"Other utilities","text":"","category":"section"},{"location":"API/RandomFeatures/","page":"Random Features","title":"Random Features","text":"get_rfms\nget_fitted_features\nget_batch_sizes\nget_n_features\nget_input_dim\nget_output_dim\nget_rng\nget_kernel_structure\nget_feature_decomposition\nget_optimizer_options\noptimize_hyperparameters!(::ScalarRandomFeatureInterface) \noptimize_hyperparameters!(::VectorRandomFeatureInterface) ","category":"page"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_rfms","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_rfms","text":"get_rfms(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.RandomFeatureMethod}\n\n\ngets the rfms field\n\n\n\n\n\nget_rfms(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.RandomFeatureMethod}\n\n\nGets the rfms field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_fitted_features","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_fitted_features","text":"get_fitted_features(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.Fit}\n\n\ngets the fitted_features field\n\n\n\n\n\nget_fitted_features(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Vector{RandomFeatures.Methods.Fit}\n\n\nGets the fitted_features field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_batch_sizes","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_batch_sizes","text":"get_batch_sizes(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Union{Nothing, Dict{S, Int64}} where S<:AbstractString\n\n\ngets batch_sizes the field\n\n\n\n\n\nget_batch_sizes(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Union{Nothing, Dict{S, Int64}} where S<:AbstractString\n\n\nGets the batch_sizes field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_n_features","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_n_features","text":"get_n_features(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Union{Nothing, Int64}\n\n\ngets the n_features field\n\n\n\n\n\nget_n_features(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Union{Nothing, Int64}\n\n\nGets the n_features field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_input_dim","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_input_dim","text":"get_input_dim(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Int64\n\n\ngets the input_dim field\n\n\n\n\n\nget_input_dim(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Int64\n\n\nGets the input_dim field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_output_dim","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_output_dim","text":"get_output_dim(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Int64\n\n\nGets the output_dim field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_rng","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_rng","text":"get_rng(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Random.AbstractRNG\n\n\ngets the rng field\n\n\n\n\n\nget_rng(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Random.AbstractRNG\n\n\nGets the rng field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_kernel_structure","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_kernel_structure","text":"get_kernel_structure(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> CalibrateEmulateSample.Emulators.KernelStructureType\n\n\nGets the kernel_structure field\n\n\n\n\n\nget_kernel_structure(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> CalibrateEmulateSample.Emulators.KernelStructureType\n\n\nGets the kernel_structure field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_feature_decomposition","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_feature_decomposition","text":"get_feature_decomposition(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> AbstractString\n\n\ngets the feature_decomposition field\n\n\n\n\n\nget_feature_decomposition(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> AbstractString\n\n\nGets the feature_decomposition field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.get_optimizer_options","page":"Random Features","title":"CalibrateEmulateSample.Emulators.get_optimizer_options","text":"get_optimizer_options(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface\n) -> Dict{S} where S<:AbstractString\n\n\ngets the optimizer_options field\n\n\n\n\n\nget_optimizer_options(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface\n) -> Dict\n\n\nGets the optimizer_options field\n\n\n\n\n\n","category":"function"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    args...;\n    kwargs...\n)\n\n\nEmpty method, as optimization takes place within the build_models stage\n\n\n\n\n\n","category":"method"},{"location":"API/RandomFeatures/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface}","page":"Random Features","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    args...;\n    kwargs...\n)\n\n\nEmpty method, as optimization takes place within the build_models stage\n\n\n\n\n\n","category":"method"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in CalibrateEmulateSample.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.","category":"page"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"Name Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta θ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-example","page":"Lorenz example","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We provide the following template for how the tools may be applied.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"For small examples typically have 2 files.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"GModel.jl Contains the forward map. The inputs should be the so-called free parameters we are interested in learning, and the output should be the measured data\nThe example script which contains the inverse problem setup and solve","category":"page"},{"location":"examples/lorenz_example/#The-structure-of-the-example-script","page":"Lorenz example","title":"The structure of the example script","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"First we create the data and the setting for the model","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Set up the forward model.\nConstruct/load the truth data. Store this data conveniently in the Observations.Observation object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we set up the inverse problem","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Define the prior distributions. Use the ParameterDistribution object\nDecide on which process tool you would like to use (we recommend you begin with Invesion()). Then initialize this with the relevant constructor\ninitialize the EnsembleKalmanProcess object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we solve the inverse problem, in a loop perform the following for as many iterations as required:","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Obtain the current parameter ensemble\nTransform them from the unbounded computational space to the physical space\ncall the forward map on the ensemble of parameters, producing an ensemble of measured data\ncall the update_ensemble! function to generate a new parameter ensemble based on the new data","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"One can then obtain the solution, dependent on the process type.","category":"page"},{"location":"GaussianProcessEmulator/#Gaussian-Process-Emulator","page":"Gaussian Process","title":"Gaussian Process Emulator","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"One type of MachineLearningTool we provide for emulation is a Gaussian process. Gaussian processes are a generalization of the Gaussian probability distribution, extended to functions rather than random variables. They can be used for statistical emulation, as they provide both mean and covariances. To build a Gaussian process, we first define a prior over all possible functions, by choosing the covariance function or kernel. The kernel describes how similar two outputs (y_i, y_j) are, given the similarities between their input values (x_i, x_j). Kernels encode the functional form of these relationships and are defined by hyperparameters, which are usually initially unknown to the user. To learn the posterior Gaussian process, we condition on data using Bayes theorem and optimize the hyperparameters of the kernel.  Then, we can make predictions to predict a mean function and covariance for new data points.","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"A useful resource to learn about Gaussian processes is Rasmussen and Williams (2006).","category":"page"},{"location":"GaussianProcessEmulator/#User-Interface","page":"Gaussian Process","title":"User Interface","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"CalibrateEmulateSample.jl allows the Gaussian process emulator to be built using either GaussianProcesses.jl  or ScikitLearn.jl. To use GaussianProcesses.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.GPJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"To use ScikitLearn.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.SKLJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Initialize a basic Gaussian Process with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(gppackage)","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"This initializes the prior Gaussian process.  We train the Gaussian process by feeding the gauss_proc alongside the data into the Emulator struct and optimizing the hyperparameters, described here.","category":"page"},{"location":"GaussianProcessEmulator/#Prediction-Type","page":"Gaussian Process","title":"Prediction Type","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can specify the type of prediction when initializing the Gaussian Process emulator. The default type of prediction is to predict data, YType().  You can create a latent function type prediction with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage,\n    prediction_type = FType())\n","category":"page"},{"location":"GaussianProcessEmulator/#Kernels","page":"Gaussian Process","title":"Kernels","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"The Gaussian process above assumes the default kernel: the Squared Exponential kernel, also called  the Radial Basis Function (RBF).  A different type of kernel can be specified when the Gaussian process is initialized.  Read more about kernel options here.","category":"page"},{"location":"GaussianProcessEmulator/#GPJL","page":"Gaussian Process","title":"GPJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"For the GaussianProcess.jl package, there are a range of kernels to choose from.  For example, ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nmy_kernel = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You do not need to provide useful hyperparameter values when you define the kernel, as these are learned in  optimize_hyperparameters!(emulator).","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine kernels together through linear operations, for example,","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nkernel_1 = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\nkernel_2 = GaussianProcesses.Lin(0.)               # Create a linear kernel with lengthscale=0\nmy_kernel = kernel_1 + kernel_2                    # Create a new additive kernel\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/#SKLJL","page":"Gaussian Process","title":"SKLJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Alternatively if you are using the ScikitLearn.jl package, you can find the list of kernels here.  These need this preamble:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using PyCall\nusing ScikitLearn\nconst pykernels = PyNULL()\nfunction __init__()\n    copy!(pykernels, pyimport(\"sklearn.gaussian_process.kernels\"))\nend","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Then they are accessible, for example, as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"my_kernel = pykernels.RBF(length_scale = 1)\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine multiple ScikitLearn kernels via linear operations in the same way as above.","category":"page"},{"location":"GaussianProcessEmulator/#Learning-the-noise","page":"Gaussian Process","title":"Learning the noise","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Often it is useful to learn the noise of the data by adding a white noise kernel. This is added with the  Boolean keyword noise_learn when initializing the Gaussian process. The default is true. ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = true )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"When noise_learn is true, an additional white noise kernel is added to the kernel. This white noise is present across all parameter values, including the training data.  The scale parameters of the white noise kernel are learned in optimize_hyperparameters!(emulator). ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You may not need to learn the noise if you already have a good estimate of the noise from your training data.  When noise_learn is false, additional regularization is added for stability. The default value is 1e-3 but this can be chosen through the optional argument alg_reg_noise:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = false,\n    alg_reg_noise = 1e-3 )","category":"page"},{"location":"API/GaussianProcess/#GaussianProcess","page":"Gaussian Process","title":"GaussianProcess","text":"","category":"section"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"GaussianProcessesPackage\nPredictionType\nGaussianProcess\nbuild_models!(::GaussianProcess{GPJL}, ::PairedDataContainer{FT}) where {FT <: AbstractFloat}\noptimize_hyperparameters!(::GaussianProcess{GPJL})\npredict(::GaussianProcess{GPJL},  ::AbstractMatrix{FT}) where {FT <: AbstractFloat}","category":"page"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcessesPackage","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcessesPackage","text":"abstract type GaussianProcessesPackage\n\nType to dispatch which GP package to use:\n\nGPJL for GaussianProcesses.jl,\nSKLJL for the ScikitLearn GaussianProcessRegressor.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.PredictionType","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.PredictionType","text":"abstract type PredictionType\n\nPredict type for GPJL in GaussianProcesses.jl:\n\nYType\nFType latent function.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcess","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcess","text":"struct GaussianProcess{GPPackage, FT} <: CalibrateEmulateSample.Emulators.MachineLearningTool\n\nStructure holding training input and the fitted Gaussian process regression models.\n\nFields\n\nmodels::Vector{Union{Nothing, PyCall.PyObject, GaussianProcesses.GPE}}: The Gaussian Process (GP) Regression model(s) that are fitted to the given input-data pairs.\nkernel::Union{Nothing, var\"#s8\", var\"#s7\"} where {var\"#s8\"<:GaussianProcesses.Kernel, var\"#s7\"<:PyCall.PyObject}: Kernel object.\nnoise_learn::Bool: Learn the noise with the White Noise kernel explicitly?\nalg_reg_noise::Any: Additional observational or regularization noise in used in GP algorithms\nprediction_type::CalibrateEmulateSample.Emulators.PredictionType: Prediction type (y to predict the data, f to predict the latent function).\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.build_models!-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT}}} where FT<:AbstractFloat","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat}\n)\n\n\nMethod to build Gaussian process models based on the package.\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}}","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    args...;\n    kwargs...\n)\n\n\nOptimize Gaussian process hyperparameters using in-build package method.\n\nWarning: if one uses GPJL() and wishes to modify positional arguments. The first positional argument must be the Optim method (default LBGFS()).\n\n\n\n\n\n","category":"method"},{"location":"API/GaussianProcess/#GaussianProcesses.predict-Union{Tuple{FT}, Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, AbstractMatrix{FT}}} where FT<:AbstractFloat","page":"Gaussian Process","title":"GaussianProcesses.predict","text":"predict(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2}\n) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#MarkovChainMonteCarlo","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Top-level-class-and-methods","page":"MarkovChainMonteCarlo","title":"Top-level class and methods","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCWrapper\nMCMCWrapper(mcmc_alg::MCMCProtocol, obs_sample::AbstractVector{FT}, prior::ParameterDistribution, em::Emulator;init_params::AbstractVector{FT}, burnin::IT, kwargs...) where {FT<:AbstractFloat, IT<:Integer}\nsample\nget_posterior\noptimize_stepsize","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"struct MCMCWrapper\n\nTop-level class holding all configuration information needed for MCMC sampling: the prior,  emulated likelihood and sampling algorithm (DensityModel and Sampler, respectively, in  AbstractMCMC's terminology).\n\nFields\n\nprior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: ParameterDistribution object describing the prior distribution on parameter values.\nlog_posterior_map::AbstractMCMC.AbstractModel: AdvancedMH.DensityModel object, used to evaluate the posterior density being sampled from.\nmh_proposal_sampler::AbstractMCMC.AbstractSampler: Object describing a MCMC sampling algorithm and its settings.\nsample_kwargs::NamedTuple: NamedTuple of other arguments to be passed to AbstractMCMC.sample().\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper-Union{Tuple{IT}, Tuple{FT}, Tuple{CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol, AbstractVector{FT}, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, CalibrateEmulateSample.Emulators.Emulator}} where {FT<:AbstractFloat, IT<:Integer}","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"MCMCWrapper(\n    mcmc_alg::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol,\n    obs_sample::AbstractArray{FT<:AbstractFloat, 1},\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    em::CalibrateEmulateSample.Emulators.Emulator;\n    init_params,\n    burnin,\n    kwargs...\n)\n\n\nConstructor for MCMCWrapper which performs the same standardization (SVD  decorrelation) that was applied in the Emulator. It creates and wraps an instance of  EmulatorPosteriorModel, for sampling from the Emulator, and  MetropolisHastingsSampler, for generating the MC proposals.\n\nmcmc_alg: MCMCProtocol describing the MCMC sampling algorithm to use. Currently implemented algorithms are:\nRWMHSampling: Metropolis-Hastings sampling from a vanilla random walk with fixed stepsize.\npCNMHSampling: Metropolis-Hastings sampling using the preconditioned  Crank-Nicholson algorithm, which has a well-behaved small-stepsize limit.\nobs_sample: A single sample from the observations. Can, e.g., be picked from an  Observation struct using get_obs_sample.\nprior: ParameterDistribution  object containing the parameters' prior distributions.\nem: Emulator to sample from. \nstepsize: MCMC step size, applied as a scaling to the prior covariance.\ninit_params: Starting parameter values for MCMC sampling.\nburnin: Initial number of MCMC steps to discard from output (pre-convergence).\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#StatsBase.sample","page":"MarkovChainMonteCarlo","title":"StatsBase.sample","text":"sample([rng,] mcmc::MCMCWrapper, args...; kwargs...)\n\nExtends the sample methods of AbstractMCMC (which extends StatsBase) to sample from the  emulated posterior, using the MCMC sampling algorithm and Emulator configured in  MCMCWrapper. Returns a MCMCChains.Chains  object containing the samples. \n\nSupported methods are:\n\nsample([rng, ]mcmc, N; kwargs...)\nReturn a MCMCChains.Chains object containing N samples from the emulated posterior.\nsample([rng, ]mcmc, isdone; kwargs...)\nSample from the model with the Markov chain Monte Carlo sampler until a convergence  criterion isdone returns true, and return the samples. The function isdone has the  signature\n    isdone(rng, model, sampler, samples, state, iteration; kwargs...)\nwhere state and iteration are the current state and iteration of the sampler,  respectively. It should return true when sampling should end, and false otherwise.\nsample([rng, ]mcmc, parallel_type, N, nchains; kwargs...)\nSample nchains Monte Carlo Markov chains in parallel according to parallel_type, which may be MCMCThreads() or MCMCDistributed() for thread and parallel sampling,  respectively.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","text":"get_posterior(\n    mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper,\n    chain::MCMCChains.Chains\n) -> EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n\n\nReturns a ParameterDistribution object corresponding to the empirical distribution of the  samples in chain.\n\nnote: Note\nThis method does not currently support combining samples from multiple Chains.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","text":"optimize_stepsize(\n    rng::Random.AbstractRNG,\n    mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper;\n    init_stepsize,\n    N,\n    max_iter,\n    sample_kwargs...\n) -> Float64\n\n\nUses a heuristic to return a stepsize for the mh_proposal_sampler element of  MCMCWrapper which yields fast convergence of the Markov chain.\n\nThe criterion used is that Metropolis-Hastings proposals should be accepted between 15% and  35% of the time.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"See AbstractMCMC sampling API for background on our use of Turing.jl's  AbstractMCMC API for  MCMC sampling.","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Sampler-algorithms","page":"MarkovChainMonteCarlo","title":"Sampler algorithms","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCProtocol\nRWMHSampling\npCNMHSampling\nMetropolisHastingsSampler","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","text":"abstract type MCMCProtocol\n\nType used to dispatch different methods of the MetropolisHastingsSampler  constructor, corresponding to different sampling algorithms.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","text":"struct RWMHSampling <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters as as vanilla random walk, based on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","text":"struct pCNMHSampling <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters according to the preconditioned Crank-Nicholson (pCN) algorithm, which is  usable for MCMC in the stepsize → 0 limit, unlike the vanilla random walk. Steps are based  on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","text":"MetropolisHastingsSampler(\n    _::CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling,\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n) -> CalibrateEmulateSample.MarkovChainMonteCarlo.RWMetropolisHastings\n\n\nConstructor for all Sampler objects, with one method for each supported MCMC algorithm.\n\nwarning: Warning\nBoth currently implemented Samplers use a Gaussian approximation to the prior:  specifically, new Metropolis-Hastings proposals are a scaled combination of the old  parameter values and a random shift distributed as a Gaussian with the same covariance as the prior. This suffices for our current use case, in which our priors are Gaussian (possibly in a transformed space) and we assume enough fidelity in the Emulator that  inference isn't prior-dominated.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Emulated-posterior-(Model)","page":"MarkovChainMonteCarlo","title":"Emulated posterior (Model)","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"EmulatorPosteriorModel","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","text":"EmulatorPosteriorModel(\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    obs_sample::AbstractArray{FT<:AbstractFloat, 1}\n) -> AdvancedMH.DensityModel\n\n\nFactory which constructs AdvancedMH.DensityModel objects given a prior on the model  parameters (prior) and an Emulator of the log-likelihood of the data given  parameters. Together these yield the log posterior density we're attempting to sample from  with the MCMC, which is the role of the DensityModel class in the AbstractMCMC interface.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-MCMC-State","page":"MarkovChainMonteCarlo","title":"Internals - MCMC State","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCState\naccept_ratio","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","text":"struct MCMCState{T, L<:Real} <: AdvancedMH.AbstractTransition\n\nExtends the AdvancedMH.Transition (which encodes the current state of the MC during sampling) with a boolean flag to record whether this state is new (arising from accepting a Metropolis-Hastings proposal) or old (from rejecting a proposal).\n\nFields\n\nparams::Any: Sampled value of the parameters at the current state of the MCMC chain.\nlog_density::Real: Log probability of params, as computed by the model using the prior.\naccepted::Bool: Whether this state resulted from accepting a new MH proposal.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","text":"accept_ratio(chain::MCMCChains.Chains) -> Any\n\n\nFraction of MC proposals in chain which were accepted (according to Metropolis-Hastings.)\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-Other","page":"MarkovChainMonteCarlo","title":"Internals - Other","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"to_decorrelated","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","text":"to_decorrelated(\n    data::AbstractArray{FT<:AbstractFloat, 2},\n    em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}\n) -> Any\n\n\nTransform samples from the original (correlated) coordinate system to the SVD-decorrelated coordinate system used by Emulator. Used in the constructor for MCMCWrapper.\n\n\n\n\n\n","category":"function"},{"location":"random_feature_emulator/#Random-Feature-Emulator","page":"Random Features","title":"Random Feature Emulator","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"note: Have a go with Gaussian processes first\nWe recommend that users first try GaussianProcess for their problems. As random features are a more recent tool, the training procedures and interfaces are still experimental and in development. ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Random features provide a more scalable (numbers of training points, input-output dimensions) and flexible framework  that approximates a Gaussian process through a random sampling of these features. Theoretical work guarantees convergence of random features to Gaussian processes with the number of sampled features, and several feature distributions are known for common kernel families.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We provide two types of MachineLearningTool for RandomFeatures, the ScalarRandomFeatureInterface and the VectorRandomFeatureInterface.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The ScalarRandomFeatureInterface closely mimics the role of the GaussianProcessEmulator package, by training a scalar-output function distribution. It can be applied to multidimensional output problems as with GaussianProcessEmulators by relying on a decorrelation of the output space, followed by training a series of independent scalar functions.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The VectorRandomFeatureInterface directly trains the mapping between multi-dimensional spaces. Therefore it does not rely on decorrelation of the output space (though this can still be helpful), and can be cheap to evaluate; on the other hand the training can be more challenging/computationally expensive.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To build a random feature emulator, as with Gaussian process one defines a kernel to encode similarities between outputs (y_iy_j) based on inputs (x_ix_j). Additionally one must specify the number of random feature samples to be taken to build the emulator.","category":"page"},{"location":"random_feature_emulator/#User-Interface","page":"Random Features","title":"User Interface","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"CalibrateEmulateSample.jl allows the random feature emulator to be built using the external package RandomFeatures.jl. In the notation of this package, our interface allows for families of RandomFourierFeature objects to be constructed with different kernels defining different structures of the \"xi\" a.k.a weight distribution, and with a learnable \"sigma\", a.k.a scaling parameter.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The interfaces are defined minimally with","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"srfi = ScalarRandomFeatureInterface(n_features, input_dim; ...)\nvrfi = VectorRandomFeatureInterface(n_features, input_dim, output_dim; ...)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"This will build an interface around a random feature family based on n_features features and mapping between spaces of dimenstion input_dim to 1 (scalar), or output_dim (vector).","category":"page"},{"location":"random_feature_emulator/#The-kernel_structure-keyword-for-flexibility","page":"Random Features","title":"The kernel_structure keyword - for flexibility","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To adjust the expressivity of the random feature family one can define the keyword argument kernel_structure. The more expressive the kernel, the more hyperparameters are learnt in the optimization.  ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We have two types,","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"separable_kernel = Separable(input_cov_structure, output_cov_structure)\nnonseparable_kernel = Nonseparable(cov_structure)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"where the cov_structure implies some imposed user structure on the covariance structure. The basic covariance structures are given by ","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"1d_cov_structure = OneDimFactor() # the problem dimension is 1\ndiagonal_structure = DiagonalFactor() # impose diagonal structure (e.g. ARD kernel)\ncholesky_structure = CholeskyFactor() # general positive definite matrix\nlr_perturbation = LowRankFactor(r) # assume structure is a rank-r perturbation from identity","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"All covariance structures (except OneDimFactor) have their final positional argument being a \"nugget\" term adding `+\\epsilon I to the covariance structure. Set to 1 by default.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"The current default kernels are as follows:","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"scalar_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(input_dim)))), OneDimFactor())\nvector_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(output_dim)))), LowRankFactor(Int(ceil(sqrt(output_dim)))))","category":"page"},{"location":"random_feature_emulator/#The-optimizer_options-keyword-for-performance","page":"Random Features","title":"The optimizer_options keyword - for performance","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Passed as a dictionary this allows the user to configure many options from their defaults in the hyperparameter optimization. The optimizer itself relies on the EnsembleKalmanProcesses package.","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We recommend users experiment with a subset of these flags. At first enable","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\"verbose\" => true)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Then if the covariance sampling takes too long, run with multithreading (e.g. julia --project -t n_threads script.jl) if it still takes too long, try","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\n    \"cov_sample_multiplier\" => csm,\n    \"train_fraction\" => tf,\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Reducing csm below 1.0 (towards 0.0) directly reduces the number of samples to estimate a covariance matrix in the optimizer, by using a shrinkage estimator - the more shrinkage the more approximation (suggestion, keep shrinkage below 0.2).\nIncreasing tf towards 1 changes the train-validate split, reducing samples but increasing cost-per-sample and reducing the available validation data (suggested range (0.5,0.95)).","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"If optimizer convergence stagnates or is too slow, or if it terminates before producing good results, try:","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Dict(\n    \"n_ensemble\" => n_e, \n    \"n_iteration\" => n_i,\n    \"localization\" => loc,\n    \"scheduler\" => sch,\n)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We suggest looking at the EnsembleKalmanProcesses documentation for more details; but to summarize","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Reducing optimizer samples n_e and iterations n_i reduces computation time.\nIf n_e becomes less than the number of hyperparameters, the updates will fail and a localizer must be specified in loc.\nIf the algorithm terminates at T=1 and resulting emulators looks unacceptable one can change or add arguments in sch e.g. DataMisfitController(\"on_terminate\"=continue)","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"widely robust defaults here are a work in progress","category":"page"},{"location":"random_feature_emulator/#Key-methods","page":"Random Features","title":"Key methods","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"To interact with the kernel/covariance structures we have standard get_* methods along with some useful functions","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"cov_structure_from_string(string,dim) creates a basic covariance structure from a predefined string: onedim, diagonal, cholesky, lowrank etc. and a dimension\ncalculate_n_hyperparameters(in_dim, out_dim, kernel_structure) calculates the number of hyperparameters created by using the given kernel structure (can be applied to the covariance structure individually too)\nbuild_default_priors(in_dim, out_dim, kernel_structure) creates a ParameterDistribution for the hyperparameters based on the kernel structure. This serves as the initialization of the training procedure.","category":"page"},{"location":"random_feature_emulator/#Example:-5D-to-1D-at-defaults-(scalar)","page":"Random Features","title":"Example: 5D-to-1D at defaults (scalar)","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 5\n# build the default scalar kernel directly (here it will be a rank-3 perturbation from the identity)\nscalar_default_kernel = SeparableKernel(\n    cov_structure_from_string(\"lowrank\", input_dim),\n    cov_structure_from_string(\"onedim\",1)\n) \n\ncalculate_n_hyperparameters(input_dim, scalar_default_kernel) \n# answer = 19, 18 for the covariance structure, and one scaling parameter\n\nbuild_default_prior(input_dim, scalar_default_kernel)\n# builds a 3-entry distribution\n# 3-dim positive distribution 'input_lowrank_diagonal'\n# 15-dim unbounded distribution 'input_lowrank_U'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/#Example-25D-to-50D-at-defaults-(vector)","page":"Random Features","title":"Example 25D to 50D at defaults (vector)","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"Or take a diagonalized 8-dimensional input, and assume full 6-dimensional output","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\n# build the default vector kernel directly (here it will be a rank-5 input and rank-8 output)\nvector_default_kernel = SeparableKernel(\n    cov_structure_from_string(\"lowrank\", input_dim),\n    cov_structure_from_string(\"lowrank\", output_dim)\n)\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_default_kernel) \n# answer = 539; 130 for input, 408 for the output, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_default_kernel)\n# builds a 5-entry distribution\n# 5-dim positive distribution 'input_lowrank_diagonal'\n# 125-dim unbounded distribution 'input_lowrank_U'\n# 8-dim positive distribution 'output_lowrank_diagonal'\n# 400-dim unbounded distribution 'output_lowrank_U'\n# 1-dim postive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/#Example-25D-to-50D-(nonseparable-vector)","page":"Random Features","title":"Example 25D to 50D (nonseparable vector)","text":"","category":"section"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"We see how the low-rank kernels are useful, when investigating the most general kernel case. The following is far too general, leading to large numbers of hyperparameters","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"using CalibrateEmulateSample.Emulators\ninput_dim = 25\noutput_dim = 50\neps = 1e-8\n# build a full-rank nonseparable vector kernel\nvector_general_kernel = NonseparableKernel(CholeskyFactor(eps))\n\ncalculate_n_hyperparameters(input_dim, output_dim, vector_general_kernel)\n# answer = 781876; 781875 for the joint input-output space, and 1 scaling\n\nbuild_default_prior(input_dim, output_dim, vector_default_kernel)\n# builds a 2-entry distribution\n# 781875-dim unbounded distribution 'full_cholesky'\n# 1-dim positive distribution `sigma`","category":"page"},{"location":"random_feature_emulator/","page":"Random Features","title":"Random Features","text":"See the API for more details.","category":"page"},{"location":"calibrate/#The-Calibrate-stage","page":"Calibrate","title":"The Calibrate stage","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration of the computer model entails finding an optimal parameter theta^* that maximizes the posterior probability","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"rho(thetavert y Gamma_y) = dfrace^-mathcalL(theta y)Z(yvertGamma_y)rho_mathrmprior(theta) qquad mathcalL(theta y) = langle mathcalG(theta) - y    Gamma_y^-1 left ( mathcalG(theta) - y right ) rangle","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"where mathcalL is the loss or negative log-likelihood, Z(yvertGamma) is a normalizing constant, y represents the data, Gamma_y is the noise covariance matrix and rho_mathrmprior(theta) is the prior density. Calibration is performed using ensemble Kalman processes, which generate input-output pairs theta mathcalG(theta) in high density from the prior initial guess to the found optimal parameter theta^*. These input-output pairs are then used as the data to train an emulator of the forward model mathcalG.","category":"page"},{"location":"calibrate/#Ensemble-Kalman-Processes","page":"Calibrate","title":"Ensemble Kalman Processes","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration can be performed using different ensemble Kalman processes: ensemble Kalman inversion (Iglesias et al, 2013), ensemble Kalman sampler (Garbuno-Inigo et al, 2020), and unscented Kalman inversion (Huang et al, 2022). All algorithms are implemented in EnsembleKalmanProcesses.jl. Documentation of each algorithm is available in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"calibrate/#Typical-construction-of-the-EnsembleKalmanProcess","page":"Calibrate","title":"Typical construction of the EnsembleKalmanProcess","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Documentation on how to construct an EnsembleKalmanProcess from the computer model and the data can be found in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"API/Emulators/#Emulators","page":"General Interface","title":"Emulators","text":"","category":"section"},{"location":"API/Emulators/","page":"General Interface","title":"General Interface","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/Emulators/","page":"General Interface","title":"General Interface","text":"Emulator\noptimize_hyperparameters!(::Emulator)\npredict\nnormalize\nstandardize\nreverse_standardize\nsvd_transform\nsvd_reverse_transform_mean_cov","category":"page"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.Emulator","page":"General Interface","title":"CalibrateEmulateSample.Emulators.Emulator","text":"struct Emulator{FT<:AbstractFloat}\n\nStructure used to represent a general emulator, independently of the algorithm used.\n\nFields\n\nmachine_learning_tool::CalibrateEmulateSample.Emulators.MachineLearningTool: Machine learning tool, defined as a struct of type MachineLearningTool.\ntraining_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT} where FT<:AbstractFloat: Normalized, standardized, transformed pairs given the Booleans normalize_inputs, standardize_outputs, retained_svd_frac.\ninput_mean::AbstractVector{FT} where FT<:AbstractFloat: Mean of input; length input_dim.\nnormalize_inputs::Bool: If normalizing: whether to fit models on normalized inputs ((inputs - input_mean) * sqrt_inv_input_cov).\nnormalization::Union{Nothing, LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat: (Linear) normalization transformation; size input_dim × input_dim.\nstandardize_outputs::Bool: Whether to fit models on normalized outputs: outputs / standardize_outputs_factor.\nstandardize_outputs_factors::Union{Nothing, AbstractVector{FT}} where FT<:AbstractFloat: If standardizing: Standardization factors (characteristic values of the problem).\ndecomposition::Union{Nothing, LinearAlgebra.SVD}: The singular value decomposition of obs_noise_cov, such that obs_noise_cov = decomposition.U * Diagonal(decomposition.S) * decomposition.Vt. NB: the SVD may be reduced in dimensions.\nretained_svd_frac::AbstractFloat: Fraction of singular values kept in decomposition. A value of 1 implies full SVD spectrum information.\n\n\n\n\n\n","category":"type"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.Emulator}","page":"General Interface","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    args...;\n    kwargs...\n) -> Any\n\n\nOptimizes the hyperparameters in the machine learning tool.\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#GaussianProcesses.predict","page":"General Interface","title":"GaussianProcesses.predict","text":"predict(\n    gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2}\n) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\npredict(\n    srfi::CalibrateEmulateSample.Emulators.ScalarRandomFeatureInterface,\n    new_inputs::AbstractMatrix;\n    multithread\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\npredict(\n    vrfi::CalibrateEmulateSample.Emulators.VectorRandomFeatureInterface,\n    new_inputs::AbstractMatrix\n) -> Tuple{Any, Any}\n\n\nPrediction of data observation (not latent function) at new inputs (passed in as columns in a matrix). That is, we add the observational noise into predictions.\n\n\n\n\n\npredict(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    new_inputs::AbstractArray{FT<:AbstractFloat, 2};\n    transform_to_real,\n    vector_rf_unstandardize,\n    mlt_kwargs...\n) -> Tuple{Any, Any}\n\n\nMakes a prediction using the emulator on new inputs (each new inputs given as data columns). Default is to predict in the decorrelated space.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.normalize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.normalize","text":"normalize(\n    emulator::CalibrateEmulateSample.Emulators.Emulator,\n    inputs::AbstractVecOrMat\n) -> Any\n\n\nNormalize the input data, with a normalizing function.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.standardize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.standardize","text":"standardize(\n    outputs::AbstractVecOrMat,\n    output_covs::Union{AbstractVector{<:AbstractMatrix}, AbstractVector{<:LinearAlgebra.UniformScaling}},\n    factors::AbstractVector\n) -> Tuple{Any, Union{AbstractVector{<:AbstractMatrix}, AbstractVector{<:LinearAlgebra.UniformScaling}}}\n\n\nStandardize with a vector of factors (size equal to output dimension).\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.reverse_standardize","page":"General Interface","title":"CalibrateEmulateSample.Emulators.reverse_standardize","text":"reverse_standardize(\n    emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat},\n    outputs::AbstractVecOrMat,\n    output_covs::AbstractVecOrMat\n) -> Tuple{Any, Any}\n\n\nReverse a previous standardization with the stored vector of factors (size equal to output  dimension). output_cov is a Vector of covariance matrices, such as is returned by svd_reverse_transform_mean_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_transform","page":"General Interface","title":"CalibrateEmulateSample.Emulators.svd_transform","text":"svd_transform(\n    data::AbstractArray{FT<:AbstractFloat, 2},\n    obs_noise_cov::Union{Nothing, AbstractArray{FT<:AbstractFloat, 2}};\n    retained_svd_frac\n) -> Tuple{Any, Any}\n\n\nApply a singular value decomposition (SVD) to the data\n\ndata - GP training data/targets; size output_dim × N_samples\nobs_noise_cov - covariance of observational noise\ntruncate_svd - Project onto this fraction of the largest principal components. Defaults to 1.0 (no truncation).\n\nReturns the transformed data and the decomposition, which is a matrix  factorization of type LinearAlgebra.SVD. \n\nNote: If F::SVD is the factorization object, U, S, V and Vt can be obtained via  F.U, F.S, F.V and F.Vt, such that A = U * Diagonal(S) * Vt. The singular values  in S are sorted in descending order.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","page":"General Interface","title":"CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","text":"svd_reverse_transform_mean_cov(\n    μ::AbstractArray{FT<:AbstractFloat, 2},\n    σ2::AbstractVector,\n    decomposition::LinearAlgebra.SVD\n) -> Tuple{Any, Any}\n\n\nTransform the mean and covariance back to the original (correlated) coordinate system\n\nμ - predicted mean; size output_dim × N_predicted_points.\nσ2 - predicted variance; size output_dim × N_predicted_points.      - predicted covariance; size N_predicted_points array of size output_dim × output_dim.\ndecomposition - SVD decomposition of obs_noise_cov.\n\nReturns the transformed mean (size output_dim × N_predicted_points) and variance.  Note that transforming the variance back to the original coordinate system results in non-zero off-diagonal elements, so instead of just returning the  elements on the main diagonal (i.e., the variances), we return the full  covariance at each point, as a vector of length N_predicted_points, where  each element is a matrix of size output_dim × output_dim.\n\n\n\n\n\n","category":"function"},{"location":"emulate/#The-Emulate-stage","page":"Emulate","title":"The Emulate stage","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Emulation is performed through the construction of an Emulator object, which has two components","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"A wrapper for any statistical emulator,\nData-processing and dimensionality reduction functionality.","category":"page"},{"location":"emulate/#Typical-construction-from-Lorenz_example.jl","page":"Emulate","title":"Typical construction from Lorenz_example.jl","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"First, obtain data in a PairedDataContainer, for example, get this from an EnsembleKalmanProcess ekpobj generated during the Calibrate stage, or see the constructor here","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"using CalibrateEmulateSample.Utilities\ninput_output_pairs = Utilities.get_training_points(ekpobj, 5) # use first 5 iterations as data","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Wrapping a predefined machine learning tool, e.g. a Gaussian process gauss_proc, the Emulator can then be built:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"emulator = Emulator(\n    gauss_proc, \n    input_output_pairs; # optional arguments after this\n    obs_noise_cov = Γy,\n    normalize_inputs = true,\n    standardize_outputs = true,\n    standardize_outputs_factors = factor_vector,\n    retained_svd_frac = 0.95,\n)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"The optional arguments above relate to the data processing.","category":"page"},{"location":"emulate/#Emulator-Training","page":"Emulate","title":"Emulator Training","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"The emulator is trained when we combine the machine learning tool and the data into the Emulator above.  For any machine learning tool, we must also optimize the hyperparameters:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"In the Lorenz example, this line learns the hyperparameters of the Gaussian process, which depend on the choice of kernel. Predictions at new inputs can then be made using","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"y, cov = Emulator.predict(emulator, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This returns both a mean value and a covariance.","category":"page"},{"location":"emulate/#Data-processing","page":"Emulate","title":"Data processing","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Some effects of the following are outlined in a practical setting in the results and appendices of Howland, Dunbar, Schneider, (2022).","category":"page"},{"location":"emulate/#Diagonalization-and-output-dimension-reduction","page":"Emulate","title":"Diagonalization and output dimension reduction","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"obs_noise_cov = Γy (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"We always use singular value decomposition to diagonalize the output space, requiring output covariance Γy. Why? If we need to train a mathbbR^10 to mathbbR^100 emulator, diagonalization allows us to instead train 100 mathbbR^10 to mathbbR^1 emulators (far cheaper).","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"retained_svd_frac = 0.95 (default 1.0)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Performance is increased further by throwing away less informative output dimensions, if 95% of the information (i.e., variance) is in the first 40 diagonalized output dimensions then setting retained_svd_frac=0.95 will train only 40 emulators.","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"note: Note\nDiagonalization is an approximation. It is however a good approximation when the observational covariance varies slowly in the parameter space.","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"warn: Warn\nSevere approximation errors can occur if obs_noise_cov is not provided.","category":"page"},{"location":"emulate/#Normalization-and-standardization","page":"Emulate","title":"Normalization and standardization","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"normalize_inputs = true (default: true)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"We normalize the input data in a standard way by centering, and scaling with the empirical covariance","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"standardize_outputs = true (default: false)\nstandardize_outputs_factors = factor_vector (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"To help with poor conditioning of the covariance matrix, users can also standardize each output dimension with by a multiplicative factor given by the elements of factor_vector","category":"page"},{"location":"emulate/#Modular-interface","page":"Emulate","title":"Modular interface","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Each statistical emulator has the following supertype and methods:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"abstract type MachineLearningTool end\nfunction build_models!(mlt, iopairs)\nfunction optimize_hyperparameters!(mlt)\nfunction predict(mlt, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Add a new tool as follows:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Create MyMLToolName.jl, and include \"MyMLToolName.jl\" in Emulators.jl\nCreate a struct MyMLTool <: MachineLearningTool \nCreate these three methods to build, train, and predict with your tool (use GaussianProcess.jl as a guide)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"note: Note\nThe predict method currently needs to return both a predicted mean and a predicted (co)variance at new inputs, which are used in the Sample stage.","category":"page"},{"location":"#CalibrateEmulateSample.jl","page":"Home","title":"CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl solves parameter estimation problems using accelerated (and approximate) Bayesian inversion.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The framework can be applied currently to learn:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the joint distribution for a moderate numbers of parameters (<40),\nit is not inherently restricted to unimodal distributions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It can be used with computer models that:","category":"page"},{"location":"","page":"Home","title":"Home","text":"can be noisy or chaotic,\nare non-differentiable,\ncan only be treated as black-box (interfaced only with parameter files).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The computer model is supplied by the user, as a parameter-to-data map mathcalG(theta) mathbbR^p rightarrow mathbbR^d. For example, mathcalG could be a map from any given parameter configuration theta to a collection of statistics of a dynamical system trajectory. mathcalG is referred to as the forward model in the Bayesian inverse problem setting.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The data produced by the forward model are compared to observations y, which are assumed to be corrupted by additive noise eta, such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"y = mathcalG(theta) + eta","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the noise eta is drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y).","category":"page"},{"location":"#The-inverse-problem","page":"Home","title":"The inverse problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Given an observation y, the computer model mathcalG, the observational noise Gamma_y, and some broad prior information on theta, we return the joint distribution of a data-informed distribution for \"theta given y\".","category":"page"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, CalibrateEmulateSample.jl breaks this problem into a sequence of three steps: calibration, emulation, and sampling. A comprehensive treatment of the calibrate-emulate-sample approach to Bayesian inverse problems can be found in Cleary et al. (2020).","category":"page"},{"location":"#The-three-steps-of-the-algorithm:","page":"Home","title":"The three steps of the algorithm:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The calibrate step of the algorithm consists of an application of Ensemble Kalman Processes, which generates input-output pairs theta mathcalG(theta) in high density around an optimal parameter theta^*. This theta^* will be near a mode of the posterior distribution (Note: This the only time we interface with the forward model mathcalG).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The emulate step takes these pairs theta mathcalG(theta) and trains a statistical surrogate model (e.g., a Gaussian process), emulating the forward map mathcalG.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The sample step uses this surrogate in place of mathcalG in a sampling method (Markov chain Monte Carlo) to sample the posterior distribution of theta.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl contains the following modules:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nCalibrateEmulateSample.jl Pulls in the Ensemble Kalman Processes package\nEmulator.jl Emulate: Modular template for emulators\nGaussianProcess.jl - A Gaussian process emulator\nMarkovChainMonteCarlo.jl Sample: Modular template for MCMC\nUtilities.jl Helper functions","category":"page"},{"location":"","page":"Home","title":"Home","text":"The best way to get started is to have a look at the examples!","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl is being developed by the Climate Modeling Alliance.","category":"page"},{"location":"examples/edmf_example/#Extended-Eddy-Diffusivity-Mass-Flux-(EDMF)-Scheme","page":"Turbulence example","title":"Extended Eddy-Diffusivity Mass-Flux (EDMF) Scheme","text":"","category":"section"},{"location":"examples/edmf_example/#Background","page":"Turbulence example","title":"Background","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The extended EDMF scheme is a unified model of turbulence and convection. More information about the model can be found here. This example builds an emulator of the extended EDMF scheme from input-output pairs obtained during a calibration process, and runs emulator-based MCMC to obtain an estimate of the joint parameter distribution.","category":"page"},{"location":"examples/edmf_example/#What-is-being-solved-here","page":"Turbulence example","title":"What is being solved here","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"This example reads calibration data containing input-output pairs obtained during calibration of the EDMF scheme. The calibration is performed using ensemble Kalman inversion, an ensemble-based algorithm that updates the location of the input parameters from the prior to the posterior, thus ensuring an optimal placement of the data used to train the emulator. In this example, the input is formed by either two or five EDMF parameters, and the output is the time-averaged liquid water path (LWP) at 40 locations in the eastern Pacific Ocean. The calibration data also contains the prior distribution of EDMF parameters and the variance of the observed variables (LWP in this case), which is used as a proxy for the magnitude of observational noise.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"More information about EDMF calibration can be found here. The calibration data is used to train the emulator.","category":"page"},{"location":"examples/edmf_example/#Running-the-examples","page":"Turbulence example","title":"Running the examples","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"We have two example scenario data (output from a (C)alibration run) that must be simply unzipped before calibration:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"ent-det-calibration.zip # two-parameter calibration\nent-det-tked-tkee-stab-calibration.zip # five-parameter calibration","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To perform uncertainty quantification use the file uq_for_EDMF.jl. Set the experiment name, and date (for outputs), e.g.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"exp_name = \"ent-det-tked-tkee-stab-calibration\" \ndate_of_run = Date(year, month, day)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"and call,","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"> julia --project uq_for_EDMF.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Info\nThese runs take currently take ~1 hour to complete","category":"page"},{"location":"examples/edmf_example/#Solution-and-output","page":"Turbulence example","title":"Solution and output","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The solution is the posterior distribution, stored in the file posterior.jld2.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior is visualized by using plot_posterior.jl, which produces corner-type scatter plots of posterior distribution, which show pairwise correlations. Again, set the exp_name and date_of_run values, then call","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"julia --project plot_posterior.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior samples can also be investigated directly. They are stored as a ParameterDistribution-type Samples object. One can load this and extract an array of parameters with:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"# input:\n# path to posterior.jld2: posterior_filepath (string)\n\nusing CalibrateEmulateSample.ParameterDistribution\nposterior = load(posterior_filepath)[\"posterior\"]\nposterior_samples = vcat([get_distribution(posterior)[name] for name in get_name(posterior)]...) # samples are columns","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To transform these samples into physical parameter space use the following:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"transformed_posterior_samples =\nmapslices(x -> transform_unconstrained_to_constrained(posterior, x), posterior_samples, dims = 1)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Computational vs Physical space\nThe computational theta-space are the parameters on which the algorithms act. Statistics (e.g. mean/covariance) are most meaningful when taken in this space. The physical phi-space is a (nonlinear) transformation of the computational space to apply parameter constraints. To pass parameter values back into the forward model, one must transform them. Full details and examples can be found here","category":"page"}]
}
