<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Random Features · CalibrateEmulateSample.jl</title><meta name="title" content="Random Features · CalibrateEmulateSample.jl"/><meta property="og:title" content="Random Features · CalibrateEmulateSample.jl"/><meta property="twitter:title" content="Random Features · CalibrateEmulateSample.jl"/><meta name="description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="og:description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="twitter:description" content="Documentation for CalibrateEmulateSample.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="CalibrateEmulateSample.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">CalibrateEmulateSample.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation_instructions/">Installation instructions</a></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../examples/sinusoid_example/">Simple example walkthrough</a></li><li><a class="tocitem" href="../examples/lorenz_example/">Lorenz example</a></li><li><a class="tocitem" href="../examples/edmf_example/">Turbulence example</a></li><li><a class="tocitem" href="../examples/Cloudy_example/">Cloudy example</a></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Emulator testing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../examples/emulators/regression_2d_2d/">Regression of <span>$\mathbb{R}^2 \to \mathbb{R}^2$</span> smooth function</a></li><li><a class="tocitem" href="../examples/emulators/lorenz_integrator_3d_3d/">Integrating Lorenz 63 with an emulated integrator</a></li><li><a class="tocitem" href="../examples/emulators/global_sens_analysis/">Global Sensitiviy Analysis (GSA) test functions</a></li></ul></li></ul></li><li><a class="tocitem" href="../calibrate/">Calibrate</a></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox" checked/><label class="tocitem" for="menuitem-6"><span class="docs-label">Emulate</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../emulate/">Emulator</a></li><li><a class="tocitem" href="../GaussianProcessEmulator/">Gaussian Process</a></li><li class="is-active"><a class="tocitem" href>Random Features</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Recommended-configuration"><span>Recommended configuration</span></a></li><li class="toplevel"><a class="tocitem" href="#User-Interface"><span>User Interface</span></a></li><li><a class="tocitem" href="#The-kernel_structure-keyword-for-flexibility"><span>The <code>kernel_structure</code> keyword - for flexibility</span></a></li><li><a class="tocitem" href="#The-optimizer_options-keyword-for-performance"><span>The <code>optimizer_options</code> keyword - for performance</span></a></li><li><a class="tocitem" href="#Key-methods"><span>Key methods</span></a></li><li><a class="tocitem" href="#Example-families-and-their-hyperparameters"><span>Example families and their hyperparameters</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../sample/">Sample</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">API</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-9-1" type="checkbox"/><label class="tocitem" for="menuitem-9-1"><span class="docs-label">CalibrateEmulateSample</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-9-1-1" type="checkbox"/><label class="tocitem" for="menuitem-9-1-1"><span class="docs-label">Emulators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../API/Emulators/">General Interface</a></li><li><a class="tocitem" href="../API/GaussianProcess/">Gaussian Process</a></li><li><a class="tocitem" href="../API/RandomFeatures/">Random Features</a></li></ul></li><li><a class="tocitem" href="../API/MarkovChainMonteCarlo/">MarkovChainMonteCarlo</a></li><li><a class="tocitem" href="../API/Utilities/">Utilities</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Emulate</a></li><li class="is-active"><a href>Random Features</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Random Features</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl/blob/main/docs/src/random_feature_emulator.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Random-Feature-Emulator"><a class="docs-heading-anchor" href="#Random-Feature-Emulator">Random Feature Emulator</a><a id="Random-Feature-Emulator-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Feature-Emulator" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Have a go with Gaussian processes first</header><div class="admonition-body"><p>We recommend that users first try <code>GaussianProcess</code> for their problems. As random features are a more recent tool, the training procedures and interfaces are still experimental and in development. </p></div></div><p>Random features provide a flexible framework to approximates a Gaussian process. Using random sampling of features, the method is a low-rank approximation leading to advantageous scaling properties (with the number of training points, input, and output dimensions). In the infinite sample limit, there are often (known) explicit Gaussian process kernels that the random feature representation converges to.</p><p>We provide two types of <code>MachineLearningTool</code> for random feature emulation, the <code>ScalarRandomFeatureInterface</code> and the <code>VectorRandomFeatureInterface</code>.</p><p>The <code>ScalarRandomFeatureInterface</code> closely mimics the role of a <code>GaussianProcess</code> package, by training a scalar-output function distribution. It can be applied to multidimensional output problems (as with <code>GaussianProcess</code>) by relying on data processing tools, such as performed when the <code>decorrelate=true</code> keyword argument is provided to the <code>Emulator</code>.</p><p>The <code>VectorRandomFeatureInterface</code>, when applied to multidimensional problems, directly trains a function distribution between multi-dimensional spaces. This approach is not restricted to the data processing of the scalar method (though this can still be helpful). It can be cheaper to evaluate, but on the other hand the training can be more challenging/computationally expensive.</p><p>Building a random feature interface is similar to building a Gaussian process: one defines a kernel to encode similarities between outputs <span>$(y_i,y_j)$</span> based on inputs <span>$(x_i,x_j)$</span>. Additionally, one must specify the number of random feature samples to be taken to build the emulator.</p><h1 id="Recommended-configuration"><a class="docs-heading-anchor" href="#Recommended-configuration">Recommended configuration</a><a id="Recommended-configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-configuration" title="Permalink"></a></h1><p>Below is listed a recommended configuration that is flexible and requires learning relatively few parameters. Users can increase <code>r</code> to balance flexibility against having more kernel hyperparameters to learn.</p><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
# given input_dim, output_dim, and a PairedDataContainer

# define number of features for prediction
n_features = 400 # number of features for prediction 

# define kernel 
nugget = 1e8*eps() # small nugget term
r = 1 # start with smallest rank 
lr_perturbation = LowRankFactor(r, nugget) 
nonsep_lrp_kernel = NonseparableKernel(lr_perturbation) 

# configure optimizer
optimizer_options = Dict(
    &quot;verbose&quot; =&gt; true, # print diagnostics for optimizer
    &quot;n_features_opt&quot; =&gt; 100, # use less features during hyperparameter optimization/kernel learning
    &quot;cov_sample_multiplier&quot; =&gt; 1.0, # use to reduce/increase number of samples in initial cov estimation stage
)

machine_learning_tool = VectorRandomFeatureInterface(
    n_features,
    input_dim,
    output_dim,
    kernel_structure = nonsep_lrp_kernel,
    optimizer_options = optimizer_options
)</code></pre><p>Users can change the kernel complexity with <code>r</code>, and the number of features for prediciton with <code>n_features</code> and optimization with <code>n_features_opt</code>. </p><h1 id="User-Interface"><a class="docs-heading-anchor" href="#User-Interface">User Interface</a><a id="User-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#User-Interface" title="Permalink"></a></h1><p><code>CalibrateEmulateSample.jl</code> allows the random feature emulator to be built using the external package <a href="https://github.com/CliMA/RandomFeatures.jl"><code>RandomFeatures.jl</code></a>. In the notation of this package&#39;s documentation, our interface allows for families of <code>RandomFourierFeature</code> objects to be constructed with different Gaussian distributions of the &quot;<code>xi</code>&quot; a.k.a weight distribution, and with a learnable &quot;<code>sigma</code>&quot;, a.k.a scaling parameter.</p><div class="admonition is-info"><header class="admonition-header">Relating features and kernels</header><div class="admonition-body"><p>The parallels of random features and gaussian processes can be quite strong. For example:</p><ul><li>The restriction to <code>RandomFourierFeature</code> objects is a restriction to the approximation of shift-invariant kernels (i.e. <span>$K(x,y) = K(x-y)$</span>)</li><li>The restriction of the weight (&quot;<code>xi</code>&quot;) distribution to Gaussians is a restriction of approximating squared-exponential kernels. Other distributions (e.g. student-t) leads to other kernels (e.g. Matern)</li></ul></div></div><p>The interfaces are defined minimally with</p><pre><code class="language-julia hljs">srfi = ScalarRandomFeatureInterface(n_features, input_dim; ...)
vrfi = VectorRandomFeatureInterface(n_features, input_dim, output_dim; ...)</code></pre><p>This will build an interface around a random feature family based on <code>n_features</code> features and mapping between spaces of dimenstion <code>input_dim</code> to <code>1</code> (scalar), or <code>output_dim</code> (vector).</p><h2 id="The-kernel_structure-keyword-for-flexibility"><a class="docs-heading-anchor" href="#The-kernel_structure-keyword-for-flexibility">The <code>kernel_structure</code> keyword - for flexibility</a><a id="The-kernel_structure-keyword-for-flexibility-1"></a><a class="docs-heading-anchor-permalink" href="#The-kernel_structure-keyword-for-flexibility" title="Permalink"></a></h2><p>To adjust the expressivity of the random feature family one can define the keyword argument <code>kernel_structure</code>. The more expressive the kernel, the more hyperparameters are learnt in the optimization.  </p><p>We have two types,</p><pre><code class="language-julia hljs">separable_kernel = SeparableKernel(input_cov_structure, output_cov_structure)
nonseparable_kernel = NonseparableKernel(cov_structure)</code></pre><p>where the <code>cov_structure</code> implies some imposed user structure on the covariance structure. The basic covariance structures are given by </p><pre><code class="language-julia hljs">oned_cov_structure = OneDimFactor() # the problem dimension is 1
diagonal_structure = DiagonalFactor() # impose diagonal structure (e.g. ARD kernel)
cholesky_structure = CholeskyFactor() # general positive definite matrix
lr_perturbation = LowRankFactor(r) # assume structure is a rank-r perturbation from identity</code></pre><p>All covariance structures (except <code>OneDimFactor</code>) have their final positional argument being a &quot;nugget&quot; term adding <span>$+\epsilon I$</span> to the covariance structure. Set to 1 by default.</p><p>The current default kernels are as follows:</p><pre><code class="language-julia hljs">scalar_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(input_dim)))), OneDimFactor())
vector_default_kernel = SeparableKernel(LowRankFactor(Int(ceil(sqrt(output_dim)))), LowRankFactor(Int(ceil(sqrt(output_dim)))))</code></pre><div class="admonition is-info"><header class="admonition-header">Relating covariance structure and training</header><div class="admonition-body"><p>The parallels between random feature and Gaussian process also extends to the hyperparameter learning. For example,</p><ul><li>A <code>ScalarRandomFeatureInterface</code> with a <code>DiagonalFactor</code> input covariance structure approximates a Gaussian process with automatic relevance determination (ARD) kernel, where one learns a lengthscale in each dimension of the input space</li></ul></div></div><h2 id="The-optimizer_options-keyword-for-performance"><a class="docs-heading-anchor" href="#The-optimizer_options-keyword-for-performance">The <code>optimizer_options</code> keyword - for performance</a><a id="The-optimizer_options-keyword-for-performance-1"></a><a class="docs-heading-anchor-permalink" href="#The-optimizer_options-keyword-for-performance" title="Permalink"></a></h2><p>Passed as a dictionary, this keyword allows the user to configure many options from their defaults in the hyperparameter optimization. The optimizer itself relies on the <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl"><code>EnsembleKalmanProcesses</code></a> package.</p><p>We recommend users experiment with a subset of these flags. At first enable</p><pre><code class="language-julia hljs">Dict(&quot;verbose&quot; =&gt; true)</code></pre><p>If the covariance sampling takes too long, run with multithreading (e.g. <code>julia --project -t n_threads script.jl</code>). Sampling is embarassingly parallel so this acheives near linear scaling,</p><p>If sampling still takes too long, try setting</p><pre><code class="language-julia hljs">Dict(
    &quot;cov_sample_multiplier&quot; =&gt; csm,
    &quot;train_fraction&quot; =&gt; tf,
)</code></pre><ul><li>Decreasing <code>csm</code> (default <code>10.0</code>) towards <code>0.0</code> directly reduces the number of samples to estimate a covariance matrix in the optimizer, by using a shrinkage estimator to improve matrix conditioning. Guide: more samples implies less shrinkage for good conditioning and less approximation error. The amount of shrinkage is returned to user as a value between 0 (no shrinkage) and 1 (shrink to diagonal matrix), it is suggested that users choose <code>csm</code> to keep the shrinkage amount below <code>0.2</code>.</li><li>Increasing <code>tf</code> towards <code>1</code> changes the train-validate split, reducing samples but increasing cost-per-sample and reducing the available validation data (default <code>0.8</code>, suggested range <code>(0.5,0.95)</code>).</li></ul><p>If optimizer convergence stagnates or is too slow, or if it terminates before producing good results, try:</p><pre><code class="language-julia hljs">Dict(
    &quot;n_ensemble&quot; =&gt; n_e, 
    &quot;n_iteration&quot; =&gt; n_i,
    &quot;localization&quot; =&gt; loc,
    &quot;scheduler&quot; =&gt; sch,
)</code></pre><p>We suggest looking at the <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl"><code>EnsembleKalmanProcesses</code></a> documentation for more details; but to summarize</p><ul><li>Reducing optimizer samples <code>n_e</code> and iterations <code>n_i</code> reduces computation time but may limit convergence progress, see <a href="https://clima.github.io/EnsembleKalmanProcesses.jl/dev/ensemble_kalman_inversion/#Updating-the-Ensemble">here</a>.</li><li>If <code>n_e</code> becomes less than the number of hyperparameters, the updates may fail and a localizer must be specified in <code>loc</code>, see <a href="https://clima.github.io/EnsembleKalmanProcesses.jl/dev/localization/">here</a>. </li><li>If the algorithm terminates at <code>T=1</code> and resulting emulators looks unacceptable one can change or add arguments in <code>sch</code> e.g. <code>DataMisfitController(&quot;on_terminate&quot;=continue)</code>, see <a href="https://clima.github.io/EnsembleKalmanProcesses.jl/dev/learning_rate_scheduler/">here</a></li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Widely robust defaults here are a work in progress</p></div></div><h2 id="Key-methods"><a class="docs-heading-anchor" href="#Key-methods">Key methods</a><a id="Key-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Key-methods" title="Permalink"></a></h2><p>To interact with the kernel/covariance structures we have standard <code>get_*</code> methods along with some useful functions</p><ul><li><code>cov_structure_from_string(string,dim)</code> creates a basic covariance structure from a predefined string: <code>onedim</code>, <code>diagonal</code>, <code>cholesky</code>, <code>lowrank</code> etc. and a dimension</li><li><code>calculate_n_hyperparameters(in_dim, out_dim, kernel_structure)</code> calculates the number of hyperparameters created by using the given kernel structure (can be applied to the covariance structure individually too)</li><li><code>build_default_priors(in_dim, out_dim, kernel_structure)</code> creates a <code>ParameterDistribution</code> for the hyperparameters based on the kernel structure. This serves as the initialization of the training procedure.</li></ul><h2 id="Example-families-and-their-hyperparameters"><a class="docs-heading-anchor" href="#Example-families-and-their-hyperparameters">Example families and their hyperparameters</a><a id="Example-families-and-their-hyperparameters-1"></a><a class="docs-heading-anchor-permalink" href="#Example-families-and-their-hyperparameters" title="Permalink"></a></h2><h3 id="Scalar:-\\mathbb{R}5-\\to-\\mathbb{R}-at-defaults"><a class="docs-heading-anchor" href="#Scalar:-\\mathbb{R}5-\\to-\\mathbb{R}-at-defaults">Scalar: <span>$\mathbb{R}^5 \to \mathbb{R}$</span> at defaults</a><a id="Scalar:-\\mathbb{R}5-\\to-\\mathbb{R}-at-defaults-1"></a><a class="docs-heading-anchor-permalink" href="#Scalar:-\\mathbb{R}5-\\to-\\mathbb{R}-at-defaults" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
input_dim = 5
# build the default scalar kernel directly (here it will be a rank-3 perturbation from the identity)
scalar_default_kernel = SeparableKernel(
    cov_structure_from_string(&quot;lowrank&quot;, input_dim),
    cov_structure_from_string(&quot;onedim&quot;, 1)
) 

calculate_n_hyperparameters(input_dim, scalar_default_kernel) 
# answer = 19, 18 for the covariance structure, and one scaling parameter

build_default_prior(input_dim, scalar_default_kernel)
# builds a 3-entry distribution
# 3-dim positive distribution &#39;input_lowrank_diagonal&#39;
# 15-dim unbounded distribution &#39;input_lowrank_U&#39;
# 1-dim positive distribution `sigma`</code></pre><h3 id="Vector,-separable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-at-defaults"><a class="docs-heading-anchor" href="#Vector,-separable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-at-defaults">Vector, separable: <span>$\mathbb{R}^{25} \to \mathbb{R}^{50}$</span> at defaults</a><a id="Vector,-separable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-at-defaults-1"></a><a class="docs-heading-anchor-permalink" href="#Vector,-separable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-at-defaults" title="Permalink"></a></h3><p>Or take a diagonalized 8-dimensional input, and assume full 6-dimensional output</p><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
input_dim = 25
output_dim = 50
# build the default vector kernel directly (here it will be a rank-5 input and rank-8 output)
vector_default_kernel = SeparableKernel(
    cov_structure_from_string(&quot;lowrank&quot;, input_dim),
    cov_structure_from_string(&quot;lowrank&quot;, output_dim)
)

calculate_n_hyperparameters(input_dim, output_dim, vector_default_kernel) 
# answer = 539; 130 for input, 408 for the output, and 1 scaling

build_default_prior(input_dim, output_dim, vector_default_kernel)
# builds a 5-entry distribution
# 5-dim positive distribution &#39;input_lowrank_diagonal&#39;
# 125-dim unbounded distribution &#39;input_lowrank_U&#39;
# 8-dim positive distribution &#39;output_lowrank_diagonal&#39;
# 400-dim unbounded distribution &#39;output_lowrank_U&#39;
# 1-dim postive distribution `sigma`</code></pre><h3 id="Vector,-nonseparable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}"><a class="docs-heading-anchor" href="#Vector,-nonseparable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}">Vector, nonseparable: <span>$\mathbb{R}^{25} \to \mathbb{R}^{50}$</span></a><a id="Vector,-nonseparable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}-1"></a><a class="docs-heading-anchor-permalink" href="#Vector,-nonseparable:-\\mathbb{R}{25}-\\to-\\mathbb{R}{50}" title="Permalink"></a></h3><p>The following represents the most general kernel case.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Use low-rank/diagonls representations where possible to control the number of hyperparameters.</p></div></div><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
input_dim = 25
output_dim = 50
eps = 1e-8
rank=5
# build a full-rank nonseparable vector kernel
vector_lowrank_kernel = NonseparableKernel(LowRankFactor(rank, eps))

calculate_n_hyperparameters(input_dim, output_dim, vector_lowrank_kernel)
# answer = 6256; 6255 for the joint input-output space, and 1 scaling

build_default_prior(input_dim, output_dim, vector_lowrank_kernel)
# builds a 2-entry distribution
# 5-dim positive distribution &#39;full_lowrank_diagonal&#39;
# 6250-dim unbounded distribution &#39;full_lowrank_U&#39;
# 1-dim positive distribution `sigma`</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Naive representations lead to very large numbers of hyperparameters.</p></div></div><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
input_dim = 25
output_dim = 50
eps = 1e-8
# build a full-rank nonseparable vector kernel
vector_general_kernel = NonseparableKernel(CholeskyFactor(eps))

calculate_n_hyperparameters(input_dim, output_dim, vector_general_kernel)
# answer = 781876; 781875 for the joint input-output space, and 1 scaling

build_default_prior(input_dim, output_dim, vector_general_kernel)
# builds a 2-entry distribution
# 781875-dim unbounded distribution &#39;full_cholesky&#39;
# 1-dim positive distribution `sigma`</code></pre><p>See the API for more details.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../GaussianProcessEmulator/">« Gaussian Process</a><a class="docs-footer-nextpage" href="../sample/">Sample »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 1 August 2024 23:56">Thursday 1 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
