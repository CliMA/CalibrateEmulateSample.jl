<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sinusoid Example · CalibrateEmulateSample.jl</title><meta name="title" content="Sinusoid Example · CalibrateEmulateSample.jl"/><meta property="og:title" content="Sinusoid Example · CalibrateEmulateSample.jl"/><meta property="twitter:title" content="Sinusoid Example · CalibrateEmulateSample.jl"/><meta name="description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="og:description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="twitter:description" content="Documentation for CalibrateEmulateSample.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="CalibrateEmulateSample.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CalibrateEmulateSample.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation_instructions/">Installation instructions</a></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><a class="tocitem" href="../../calibrate/">Calibrate</a></li><li><a class="tocitem" href="../../emulate/">Emulate</a></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../lorenz_example/">Lorenz example</a></li><li><a class="tocitem" href="../edmf_example/">Turbulence example</a></li><li><a class="tocitem" href="../Cloudy_example/">Cloudy example</a></li><li><input class="collapse-toggle" id="menuitem-6-4" type="checkbox"/><label class="tocitem" for="menuitem-6-4"><span class="docs-label">Emulator testing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../emulators/regression_2d_2d/">Regression of <span>$\mathbb{R}^2 \to \mathbb{R}^2$</span> smooth function</a></li><li><a class="tocitem" href="../emulators/lorenz_integrator_3d_3d/">Integrating Lorenz 63 with an emulated integrator</a></li><li><a class="tocitem" href="../emulators/ishigami_3d_1d/">Global Sensitiviy Analysis for an emulated Ishigami function</a></li></ul></li></ul></li><li><a class="tocitem" href="../../GaussianProcessEmulator/">Gaussian Process</a></li><li><a class="tocitem" href="../../random_feature_emulator/">Random Features</a></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">Package Design</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../API/AbstractMCMC/">AbstractMCMC sampling API</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">API</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-10-1" type="checkbox"/><label class="tocitem" for="menuitem-10-1"><span class="docs-label">CalibrateEmulateSample</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-10-1-1" type="checkbox"/><label class="tocitem" for="menuitem-10-1-1"><span class="docs-label">Emulators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../API/Emulators/">General Interface</a></li><li><a class="tocitem" href="../../API/GaussianProcess/">Gaussian Process</a></li><li><a class="tocitem" href="../../API/RandomFeatures/">Random Features</a></li></ul></li><li><a class="tocitem" href="../../API/MarkovChainMonteCarlo/">MarkovChainMonteCarlo</a></li><li><a class="tocitem" href="../../API/Utilities/">Utilities</a></li></ul></li></ul></li><li><a class="tocitem" href="../../glossary/">Glossary</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Sinusoid Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sinusoid Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl/blob/main/docs/src/examples/sinusoid_example.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Sinusoid-Example"><a class="docs-heading-anchor" href="#Sinusoid-Example">Sinusoid Example</a><a id="Sinusoid-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Sinusoid-Example" title="Permalink"></a></h1><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>This example demonstrates how to use CalibrateEmulateSample.jl for a simple model that generates noisy observables of a signal. The sinusoid signal is defined by two parameters: its shift along an axis and its amplitude. We make noisy observations of the signal and we can calculate the mean of the signal, which is informative about its shift along the axis, and the range of the signal, which is informative  about the amplitude. We show how we can carry out uncertainty quantification on our parameter values, given  some observations and a model of this problem.</p><h3 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h3><p>We have a model of a sinusoidal signal that is a function of parameters <span>$\theta=(A,v)$</span>, where <span>$A$</span> is the amplitude of the signal and <span>$v$</span> is vertical shift of the signal:  <span>$f(A, v) = A \sin(\phi + t) + v, \forall t \in [0,2\pi]$</span> Here, <span>$\phi$</span> is the random phase of each signal.  The goal is to estimate the not just point estimates of the parameters <span>$\theta=(A,v)$</span>, but entire probability distributions of them, given some noisy observations. We will use the range and mean of a signal as our observable:  <span>$G(\theta) = \big[ \text{range}\big(f(\theta)\big), \text{mean}\big(f(\theta)\big) \big] $$ Then, our noisy observations, $y_{obs}$, can be written as: $$y_{obs} = G(\theta) + \mathcal{N}(0, \Gamma)$</span>  where <span>$\Gamma$</span> is the observational covariance matrix. We will assume the noise to be independent for each observable, giving us a diagonal covariance matrix.</p><h1 id="Walkthrough-of-code"><a class="docs-heading-anchor" href="#Walkthrough-of-code">Walkthrough of code</a><a id="Walkthrough-of-code-1"></a><a class="docs-heading-anchor-permalink" href="#Walkthrough-of-code" title="Permalink"></a></h1><p>The code is split into four sections:</p><ol><li>Model set up in <code>sinusoid_setup.jl</code></li><li>Calibrate in <code>calibrate.jl</code></li><li>Emulate in <code>emulate.jl</code></li><li>Sample in <code>sample.jl</code></li></ol><p>You do not need to explicitly run <code>sinusoid_setup.jl</code> as it is called from <code>calibrate.jl</code>. However, this file contains the functions for the model and for generating pseudo-observations.  You will need to run steps 1-3 in order as each one relies on output saved from the previous steps.</p><h2 id="Set-up"><a class="docs-heading-anchor" href="#Set-up">Set up</a><a id="Set-up-1"></a><a class="docs-heading-anchor-permalink" href="#Set-up" title="Permalink"></a></h2><p>First, we load the packages we need:</p><pre><code class="nohighlight hljs">using LinearAlgebra, Random
using Plots
using JLD2</code></pre><p>Define a model which generates a sinusoid given parameters <span>$\theta=(A,v)$</span>  (amplitude and vertical shift). We will estimate these parameters from data. The model adds a random phase shift upon evaluation.</p><pre><code class="nohighlight hljs"># Seed for pseudo-random number generator.
rng_seed = 41
rng = Random.MersenneTwister(rng_seed)
# Define x-axis
dt = 0.01
trange = 0:dt:(2 * pi + dt)

function model(amplitude, vert_shift)
    phi = 2 * pi * rand(rng)
    return amplitude * sin.(trange .+ phi) .+ vert_shift
end
</code></pre><p>We will define a &quot;true&quot; amplitude and vertical shift, to generate some pseudo-observations.  Let <span>$\theta=(3.0, 7.0)$</span>.</p><pre><code class="nohighlight hljs">amplitude_true = 3.0
vert_shift_true = 7.0
# Our input parameters are 2d and our outputs are 2d
theta_true = [amplitude_true, vert_shift_true]
dim_params = 2
# Generate the &quot;true&quot; signal for these parameters
signal_true = model(amplitude_true, vert_shift_true)</code></pre><p>We will observe properties of the signal that inform us about the amplitude and vertical  position. These properties will be the range (the difference between the maximum and the minimum), which is informative about the amplitude of the sinusoid, and the mean, which is informative  about the vertical shift. </p><pre><code class="nohighlight hljs">y1_true = maximum(signal_true) - minimum(signal_true)
y2_true = mean(signal_true)</code></pre><p>However, our observations are typically not noise-free, so we add some white noise to our  observables. We call this <span>$y_{obs}$</span>. The user can choose the observational covariance matrix, <span>$\Gamma$</span>. We will assume the noise is independent (a diagonal covariance matrix <span>$\Gamma=0.2 * I$</span>). </p><pre><code class="nohighlight hljs">dim_output = 2
Γ = 0.2 * I
white_noise = MvNormal(zeros(dim_output), Γ)
y_obs = [y1_true, y2_true] .+ rand(white_noise)
y1_obs = y[1]
y2_obs = y[2]
println(&quot;Observations:&quot;, y_obs)</code></pre><p>This gives <span>$y_{obs}=(6.15, 6.42)$</span>. We plot the true signal in black, the true observables in red and the noisy observables in blue.</p><pre><code class="nohighlight hljs">p = plot(signal_true, color = :black, linewidth = 3, label = &quot;True signal&quot;)
hline!([y2_true],color=:red, style=:dash, linewidth=2, label=&quot;True mean: &quot; * string(round(y2_true, digits=2)) )
plot!([argmax(signal_true),argmax(signal_true)], [minimum(signal_true), maximum(signal_true)],
 arrows=:both, color=:red, linewidth=2, label=&quot;True range: &quot; * string(round(y1_true, digits=2)) )
hline!([y2_obs],color=:blue, style=:dash, linewidth=1, label=&quot;Observed mean: &quot; * string(round(y2_obs, digits=2)) )
plot!([argmax(signal_true)+10, argmax(signal_true)+10], [y2_true - y1_obs/2, y2_true + y1_obs/2],
 arrows=:both, color=:blue, linewidth=1, label=&quot;Observed range: &quot;  * string(round(y1_obs, digits=2)) )</code></pre><p><img src="../../assets/sinusoid_true_vs_observed_signal.png" alt="signal"/></p><p>It will be helpful for us to define a function <span>$G(\theta)$</span>, which returns these observables  (the range and the mean) of the sinusoid given a parameter vector. </p><pre><code class="nohighlight hljs">function G(theta)
    amplitude, vert_shift = theta
    sincurve = model(amplitude, vert_shift)
    return [maximum(sincurve) - minimum(sincurve), mean(sincurve)]
end</code></pre><h2 id="Calibrate"><a class="docs-heading-anchor" href="#Calibrate">Calibrate</a><a id="Calibrate-1"></a><a class="docs-heading-anchor-permalink" href="#Calibrate" title="Permalink"></a></h2><p>For the calibration step, we need to solve the inverse problem <span>$y=G(\theta)+\mathcal{N}(0,\Gamma)$</span>. In other words, we need to find the optimal values for <span>$\theta$</span>. First, load the packages we need from CES.</p><pre><code class="nohighlight hljs"># CES
using CalibrateEmulateSample
const EKP = CalibrateEmulateSample.EnsembleKalmanProcesses
const PD = EKP.ParameterDistributions</code></pre><p>We define prior distributions on the two parameters. For the amplitude, we define a prior with mean 2 and standard deviation 1. It is additionally constrained to be nonnegative. For the vertical shift we define a Gaussian prior with mean 0 and standard deviation 5.</p><pre><code class="nohighlight hljs">prior_u1 = PD.constrained_gaussian(&quot;amplitude&quot;, 2, 1, 0, Inf)
prior_u2 = PD.constrained_gaussian(&quot;vert_shift&quot;, 0, 5, -Inf, Inf)
prior = PD.combine_distributions([prior_u1, prior_u2])
# Plot priors
p = plot(prior)</code></pre><p><img src="../../assets/sinusoid_prior.png" alt="prior"/></p><p>We now generate the initial ensemble and set up the ensemble Kalman inversion.</p><pre><code class="nohighlight hljs">N_ensemble = 10
N_iterations = 5

initial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)

ensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, y_obs, Γ, EKP.Inversion(); rng = rng)</code></pre><p>We are now ready to carry out the inversion. At each iteration, we get the ensemble from the last iteration, apply <span>$G(\theta)$</span> to each ensemble member, and apply the Kalman update to the ensemble.</p><pre><code class="nohighlight hljs">for i in 1:N_iterations
    params_i = EKP.get_ϕ_final(prior, ensemble_kalman_process)

    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)

    EKP.update_ensemble!(ensemble_kalman_process, G_ens)
end</code></pre><p>Finally, we get the ensemble after the last iteration. This provides our estimate of the parameters.</p><pre><code class="nohighlight hljs">final_ensemble = EKP.get_ϕ_final(prior, ensemble_kalman_process)

# Check that the ensemble mean is close to the theta_true
println(&quot;Ensemble mean: &quot;, mean(final_ensemble, dims=2))   # [3.08, 6.37]
println(&quot;True parameters: &quot;, theta_true)   # [3.0, 7.0]</code></pre><table><tr><th style="text-align: left">Parameter</th><th style="text-align: center">Truth</th><th style="text-align: center">EKI mean</th></tr><tr><td style="text-align: left">Amplitude</td><td style="text-align: center">3.0</td><td style="text-align: center">3.08</td></tr><tr><td style="text-align: left">Vertical shift</td><td style="text-align: center">7.0</td><td style="text-align: center">6.37</td></tr></table><p>The EKI ensemble mean at the final iteration is close to the true parameters, which is good. We can also see how the ensembles evolve at each iteration in the plot below.</p><p><img src="../../assets/sinusoid_eki_pairs.png" alt="eki"/></p><p>The ensembles are initially spread out but move closer to the true parameter values with each iteration, indicating the EKI algorithm is converging towards the minimum. Taking the mean of the ensemble gives a point estimate of the optimal parameters. However, EKI does not give us an estimate of the uncertainty, as the ensemble collapses. To carry out uncertainty quantification, we can sample from the posterior distribution, which requires a &quot;cheap&quot; method to evaluate our model, i.e., an emulator.  In the next step of CES, we will build an emulator using the dataset generated in EKI.</p><h2 id="Emulate"><a class="docs-heading-anchor" href="#Emulate">Emulate</a><a id="Emulate-1"></a><a class="docs-heading-anchor-permalink" href="#Emulate" title="Permalink"></a></h2><p>In the previous calibrate step, we learned point estimates for the optimal parameters <span>$\theta$</span>, but  for uncertainty quantification, we want to learn posterior distributions on our parameters. We can sample from posterior distributions with Markov chain Monte Carlo (MCMC) methods, but these  typically require many model evaluations. In many scientific problems, model evaluations are highly  costly, making this infeasible. To get around this, we build an emulator of our model,  which allows us to approximate the expensive model almost instantaneously. An emulator can also be  helpful for noisy problems as they provide a smoother approximation, leading to better MCMC  convergence properties.  In this section, we show how the codebase can be used to build emulators of our sinusoid model.</p><p>We ran Ensemble Kalman Inversion with an ensemble size of 10 for 5  iterations. This generated a total of 50 input output pairs from our model. We will use these samples to train an emulator. The EKI samples make a suitable  dataset for training an emulator because in the first iteration, the ensemble parameters  are spread out according to the prior, meaning they cover the full support of the parameter space. This is important for building an emulator that can be evaluated anywhere  in this space. In later iterations, the ensemble parameters are focused around the truth.  This means the emulator that will be more accurate around this region.</p><p>Load additional packages we need for this section.</p><pre><code class="nohighlight hljs">using CalibrateEmulateSample.Emulators</code></pre><p>We will build two types of emulator here for comparison: Gaussian processes and Random  Features. First, set up the data in the correct format. CalibrateEmulateSample.jl uses a paired data container that matches the inputs (in the unconstrained space) to the outputs:</p><pre><code class="nohighlight hljs">input_output_pairs = CES.Utilities.get_training_points(ensemble_kalman_process, N_iterations)
unconstrained_inputs = CES.Utilities.get_inputs(input_output_pairs)
inputs = Emulators.transform_unconstrained_to_constrained(prior, unconstrained_inputs)
outputs = CES.Utilities.get_outputs(input_output_pairs)</code></pre><h3 id="Gaussian-process"><a class="docs-heading-anchor" href="#Gaussian-process">Gaussian process</a><a id="Gaussian-process-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-process" title="Permalink"></a></h3><p>We will set up a basic Gaussian process (GP) emulator using the <a href="https://scikitlearnjl.readthedocs.io/en/latest/models/#scikitlearn-models"><code>ScikitLearn.jl</code></a> package or <a href="https://stor-i.github.io/GaussianProcesses.jl/latest/"><code>GaussianProcesses.jl</code></a>.  See the <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/GaussianProcessEmulator/">Gaussian process page</a> for more information and options, including choice of package and kernels.</p><pre><code class="nohighlight hljs">gppackage = Emulators.GPJL()
gauss_proc = Emulators.GaussianProcess(gppackage, noise_learn = false)

# Build emulator with data
emulator_gp = Emulator(gauss_proc, input_output_pairs, normalize_inputs = true,  obs_noise_cov = Γ)
optimize_hyperparameters!(emulator_gp)</code></pre><p>For this simple example, we already know the observational noise <code>Γ=0.2*I</code>, so we set <code>noise_learn = false</code>.  However, for more complicated problems we may want to learn the noise as an additional hyperparameter.</p><p>We will check performance of the GP by testing on unseen data. We do this by generating some additional data from our true model for a set of 10 parameters.</p><pre><code class="nohighlight hljs">amplitude_points = range(0.4, 4, length = 10)
shift_points = range(-1, 8, length = 10)
x_test = hcat(shuffle(amplitude_points), shuffle(shift_points))&#39;

# benchmark against true model
y_true = zeros(2, length(amplitude_points))
for i in 1:length(amplitude_points)
    y_true[:, i] = G(x_test[:, i])
end
# emulator is built in unconstrained space, we need to first transform our variables
x_test_unconstrained = Emulators.transform_constrained_to_unconstrained(prior, x_test)
y_gp, y_gp_covs = Emulators.predict(emulator_gp, x_test_unconstrained, transform_to_real=true);

# This predicts an output and a covariance matrix.
# Estimate root mean squared error
rmse_gp = sqrt.(mean((y_gp - y_true).^2, dims=2))
println(&quot;RMSE GP:&quot;, rmse_gp)     # [1.0188362036675438; 0.10148451670974754;;]</code></pre><p>One metric for assessing emulator performance is to calculate the root mean squared error  of our test set, in this case (1.02, 0.10). These errors are relatively small compared against  the outputs, around 10%. You could also consider other metrics, such as R^2, mean absolute error or  log likelihood. It is also helpful to check emulator performance by plotting the true output  against the predicted output. </p><p><img src="../../assets/sinusoid_GP_emulator.png" alt="GP_emulator"/></p><p>For a perfect GP emulator, our predictions should line up with the true output, along the y=x dashed line. We find that this is mostly the case, except for large values of y_1. This is not too surprising, because this region is more extrapolation rather than interpolation, as EKI focuses our training data towards the truth (see figure about). We should also check that GP uncertainties are reasonable, and ideally they should be larger  for points further away from training data. The error bars in the figure above shows 1 standard deviation, so  we expect that at least 68% of the points should be in agreement with the y=x line when considering these error bars. </p><h3 id="Random-Features"><a class="docs-heading-anchor" href="#Random-Features">Random Features</a><a id="Random-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Features" title="Permalink"></a></h3><p>An alternative emulator can be created with random features (RF). Random features can approximate a Gaussian process with improved scaling properties, making them more suitable for for higher dimensional problems.  More information can be found (here)[https://clima.github.io/CalibrateEmulateSample.jl/dev/random<em>feature</em>emulator/].</p><pre><code class="nohighlight hljs"># Select number of features
n_features = 60
nugget = 1e-12
kernel_structure = SeparableKernel(LowRankFactor(2, nugget), OneDimFactor())

# Create random features
optimizer_options =
    Dict(&quot;n_ensemble&quot; =&gt; 20, &quot;scheduler&quot; =&gt; EKP.DataMisfitController(on_terminate = &quot;continue&quot;), &quot;n_iteration&quot; =&gt; 50)
random_features = ScalarRandomFeatureInterface(
    n_features,
    input_dim,
    kernel_structure = kernel_structure,
    optimizer_options = optimizer_options,
)
emulator_random_features = Emulator(random_features, input_output_pairs, normalize_inputs = true)
optimize_hyperparameters!(emulator_random_features)</code></pre><p>We will also test this on the same data we created for the validation of the Gaussian process. </p><pre><code class="nohighlight hljs">y_rf, y_rf_covs = Emulators.predict(emulator_random_features, x_test_unconstrained);

# Estimate root mean squared error
rmse_rf = sqrt.(mean((y_rf - y_true).^2, dims=2))
println(&quot;RMSE RF:&quot;, rmse_rf)     # [2.3993665360796514; 1.4247135862280327;;]</code></pre><p><img src="../../assets/sinusoid_GP_vs_RF_emulator.png" alt="GP_vs_RF_emulator"/></p><p>This comparison shows the GP emulator is more accurate on the test data compared to the RF emulator. Both methods are more accurate around <span>$y_{obs}=(6.15,6.42)$</span> with worse performance at the edges, especially for large y1.  This is as expected, because our training dataset from EKI has more points focused around theta_true. RF gives slightly larger uncertainty estimates, but these are reasonably similar.</p><h2 id="Sample"><a class="docs-heading-anchor" href="#Sample">Sample</a><a id="Sample-1"></a><a class="docs-heading-anchor-permalink" href="#Sample" title="Permalink"></a></h2><p>Now that we have a cheap emulator for our model, we can carry out uncertainty quantification to learn the posterier distribution of the parameters, <span>$\theta$</span>. We use Markov chain Monte Carlo (MCMC) to sample the posterior distribtion. In MCMC, we start with a sample from a prior distribution and propose a new sample from a proposal distribution, which is accepted with a probability relating the the ratio of the posterior distribution to the proposal distributions. If accepted, this proposed sample is  added to the chain, or otherwise, the original sample is added to the chain. This is repeated over many iterations and eventually creates a sequence of samples from the posterior distribution. The CES code uses <a href="https://turing.ml/dev/docs/for-developers/interface">AbstractMCMC.jl</a>, full details can be found <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/API/AbstractMCMC/">here</a>. For this example, we will use a random walk Metropolis-Hastings sampler (<code>RWMHSampling</code>), which assumes that  the proposal distribution is a random walk, with a step-size <code>\delta</code>. Usually, we have little knowledge of  what this step size should be, but we can optimize this as shown below.</p><p>First, load additional packages we need</p><pre><code class="nohighlight hljs">using CalibrateEmulateSample.MarkovChainMonteCarlo</code></pre><p>We will provide the API with the observations, priors and our cheap emulator from the previous section. In this  example we use the GP emulator. </p><pre><code class="nohighlight hljs">mcmc = MCMCWrapper(RWMHSampling(), y_obs, prior, emulator; init_params = init_sample)
# First let&#39;s run a short chain to determine a good step size
new_step = optimize_stepsize(mcmc; init_stepsize = 0.1, N = 2000, discard_initial = 0)

# Now begin the actual MCMC
println(&quot;Begin MCMC - with step size &quot;, new_step)
chain = MarkovChainMonteCarlo.sample(mcmc, 100_000; stepsize = new_step, discard_initial = 2_000)

# We can print summary statistics of the MCMC chain
display(chain)</code></pre><table><tr><th style="text-align: left">parameters</th><th style="text-align: center">mean</th><th style="text-align: center">std</th></tr><tr><td style="text-align: left">amplitude</td><td style="text-align: center">1.1040</td><td style="text-align: center">0.0868</td></tr><tr><td style="text-align: left">vert_shift</td><td style="text-align: center">6.3696</td><td style="text-align: center">0.4645</td></tr></table><p>Note that these values are provided in the unconstrained space. The vertical shift seems reasonable, but the amplitude is not. This is because the amplitude is constrained to be positive, but the MCMC is run in the unconstrained space.  We can transform to the real  constrained space and re-calculate these values.</p><pre><code class="nohighlight hljs"># Extract posterior samples
posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)
# Back to constrained coordinates
constrained_posterior = Emulators.transform_unconstrained_to_constrained(
    prior, MarkovChainMonteCarlo.get_distribution(posterior)
)</code></pre><p>This gives: | parameters         | mean    | std | | :–––––––– | :–––: | :––: | | amplitude         |   3.0277 | 0.2620  | | vert_shift        |   6.3696 | 0.4645  |</p><p>This is in agreement with the true <span>$\theta=(3.0, 7.0)$</span> and with the observational covariance matrix we provided <span>$\Gamma=0.2 * I$</span> (i.e., a standard deviation of approx. <span>$0.45$</span>).</p><pre><code class="nohighlight hljs">
# We can quickly plot priors and posterior using built-in capabilities
p = plot(prior)
plot!(posterior)
</code></pre><p><img src="../../assets/sinusoid_posterior_GP.png" alt="GP_posterior"/></p><p>For multi-dimensional problems, we may be interested in how the posterior distributions for each parameter co-vary. We plot a 2D histogram of <span>$\theta_1$</span> vs <span>$\theta_2$</span> below.</p><p><img src="../../assets/sinusoid_MCMC_hist_GP.png" alt="GP_2d_posterior"/></p><h3 id="Sample-with-Random-Features"><a class="docs-heading-anchor" href="#Sample-with-Random-Features">Sample with Random Features</a><a id="Sample-with-Random-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-with-Random-Features" title="Permalink"></a></h3><p>We can do the same sampling method using the random features emulator instead of the Gaussian process. The random features emulator showed weaker performance and larger uncertainty estimates,  especially for predicting y1. As a result, the posterior distributions are wider. It appears that y1 is not so well constrained and looks more similar to the prior distribution. This highlights the need for careful emulator validation, to understand where and how the emulator performance could affect posterior samples.</p><p><img src="../../assets/sinusoid_posterior_RF.png" alt="RF_posterior"/></p><p>We also see this in the 2d histogram:</p><p><img src="../../assets/sinusoid_MCMC_hist_RF.png" alt="RF_2d_posterior"/></p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Tuesday 19 December 2023 22:48">Tuesday 19 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
