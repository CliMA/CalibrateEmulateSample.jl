var documenterSearchIndex = {"docs":
[{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to CalibrateEmulateSample! We encourage opening issues and pull requests (PRs).","category":"page"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The easiest way to contribute is by using CalibrateEmulateSample, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of CalibrateEmulateSample insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition.","category":"page"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you are unfamiliar with git and version control, the following guides will be helpful:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Atlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"page"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Create your own fork of CalibrateEmulateSample on GitHub and check out your copy:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git clone https://github.com/<your-username>/CalibrateEmulateSample.jl.git\n$ cd CalibrateEmulateSample.jl","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Now you have access to your fork of CalibrateEmulateSample through origin. Create a branch for your feature; this will hold your contribution:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout -b <branchname>","category":"page"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"page"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"julia --project=.dev .dev/climaformat.jl .","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"from the CalibrateEmulateSample.jl directory.","category":"page"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with CalibrateEmulateSample.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To make sure you are up to date with main, you can use the following workflow:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To squash your commits, you can use the following command:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~n","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"where n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~4","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"will open the following file in (typically) vim:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Then in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git push -u origin <name_of_local_branch> --force","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can find more information about squashing here.","category":"page"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Currently a number of checks are run per commit for a given PR.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"JuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Unit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"page"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use bors to manage merging PR's in the the CalibrateEmulateSample repo. If you're a collaborator and have the necessary permissions, you can type bors try in a comment on a PR to have integration test suite run on that PR, or bors r+ to try and merge the code.  Bors ensures that all integration tests for a given PR always pass before merging into main. The integration tests currently run example cases in examples/. Any breaking changes will need to also update the examples/, else bors will fail.","category":"page"},{"location":"API/AbstractMCMC/#AbstractMCMC-sampling-API","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The \"sample\" part of CES refers to exact sampling from the emulated posterior via Markov chain Monte Carlo (MCMC). Within this paradigm, we want to provide the flexibility to use multiple sampling algorithms; the approach we take is to use the general-purpose AbstractMCMC.jl API, provided by the Turing.jl probabilistic programming framework.","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"This page provides a summary of AbstractMCMC which augments the existing documentation ([1], [2]) and highlights how it's used by the CES package in MarkovChainMonteCarlo. It's not meant to be a complete description of the AbstractMCMC package.","category":"page"},{"location":"API/AbstractMCMC/#Use-in-MarkovChainMonteCarlo","page":"AbstractMCMC sampling API","title":"Use in MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"At present, Turing has limited support for derivative-free optimization, so we only use this abstract API and not Turing itself. We also use two related dependencies, AdvancedMH and MCMCChains. ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Julia's philosophy is to use small, composable packages rather than monoliths, but this can make it difficult to remember where methods are defined! Below we describe the relevant parts of ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The AbstractMCMC API,\nExtended to the case of Metropolis-Hastings (MH) sampling by AdvancedMH,\nFurther extended for the needs of CES in Markov chain Monte Carlo.","category":"page"},{"location":"API/AbstractMCMC/#Classes-and-methods","page":"AbstractMCMC sampling API","title":"Classes and methods","text":"","category":"section"},{"location":"API/AbstractMCMC/#Sampler","page":"AbstractMCMC sampling API","title":"Sampler","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"A Sampler is AbstractMCMC's term for an implementation of a MCMC sampling algorithm, along with all its configuration parameters. All samplers must inherit from AbstractMCMC.AbstractSampler. ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Currently CES only implements the Metropolis-Hastings (MH) algorithm. Because it's so straightforward, much of AbstractMCMC isn't needed. We implement two variants of MH with two different Samplers: RWMetropolisHastings and pCNMetropolisHastings, both of which inherit from the AdvancedMH.MHSampler base class. The public constructor for both Samplers is MetropolisHastingsSampler; the different Samplers are specified by passing a MCMCProtocol object to this constructor.","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The MH Sampler classes have only one field, proposal, which is the distribution used to generate new MH proposals via stochastic offsets to the current parameter values. This is done by AdvancedMH.propose(), which gets called for each MCMC step() (below). The difference between our two Samplers is in how this proposal is generated:","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"RWMHSampling does vanilla random-walk proposal generation with a constant, user-specified step size (this differs from the AdvancedMH implementation, which doesn't provide for a step size.)\npCNMHSampling for preconditioned Crank-Nicholson proposals. Vanilla random walk sampling doesn't have a well-defined limit for high-dimensional parameter spaces; pCN replaces the random walk with an Ornstein–Uhlenbeck [AR(1)] process so that the Metropolis acceptance probability remains non-zero in this limit. See Beskos et. al. (2008) and Cotter et. al. (2013).","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"This is the only difference: generated proposals are then either accepted or rejected according to the same MH criterion (in step(), below.)","category":"page"},{"location":"API/AbstractMCMC/#Models","page":"AbstractMCMC sampling API","title":"Models","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"In Turing, the Model is the distribution one performs inference on, which may involve observed and hidden variables and parameters. For CES, we simply want to sample from the posterior, so our Model distribution is simply the emulated likelihood (see Emulators) together with the prior. This is constructed by EmulatorPosteriorModel.","category":"page"},{"location":"API/AbstractMCMC/#Sampling-with-the-MCMC-Wrapper-object","page":"AbstractMCMC sampling API","title":"Sampling with the MCMC Wrapper object","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"At a high level, a Sampler and Model is all that's needed to do MCMC sampling. This is done by the sample method provided by AbstractMCMC (extending the method from BaseStats). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"To be more user-friendly, in CES we wrap the Sampler, Model and other necessary configuration into a MCMCWrapper object. The constructor for this object ensures that all its components are created consistently, and performs necessary bookkeeping, such as converting coordinates to the decorrelated basis. We extend sample with methods to use this object (that simply unpack its fields and call the appropriate method from AbstractMCMC.)","category":"page"},{"location":"API/AbstractMCMC/#Chain","page":"AbstractMCMC sampling API","title":"Chain","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"The MCMCChain class is used to store the results of the MCMC sampling; the package provides simple diagnostics for visualization and diagnosing chain convergence.","category":"page"},{"location":"API/AbstractMCMC/#Internals:-Transitions","page":"AbstractMCMC sampling API","title":"Internals: Transitions","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Implementing MCMC involves defining states and transitions of a Markov process (whose stationary distribution is what we seek to sample from). AbstractMCMC's terminology is a bit confusing for the MH case; states of the chain are described by Transition objects, which contain the current sample (and other information like its log-probability). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"AdvancedMH defines an AbstractTransition base class for use with its methods; we implement our own child class, MCMCState, in order to record statistics on the MH acceptance ratio.","category":"page"},{"location":"API/AbstractMCMC/#Internals:-Markov-steps","page":"AbstractMCMC sampling API","title":"Internals: Markov steps","text":"","category":"section"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"Markov transitions of the chain are defined by overloading AbstractMCMC's step method, which takes the Sampler and current Transition and implements the Sampler's logic to returns an updated Transition representing the chain's new state (actually, a pair of Transitions, for cases where the Sampler doesn't obey detailed balance; this isn't relevant for us). ","category":"page"},{"location":"API/AbstractMCMC/","page":"AbstractMCMC sampling API","title":"AbstractMCMC sampling API","text":"For example, in Metropolis-Hastings sampling this is where we draw a proposal sample and accept or reject it according to the MH criterion. AdvancedMH implements this here; we re-implement this method because 1) we need to record whether a proposal was accepted or rejected, and 2) our calls to propose() are stepsize-dependent.","category":"page"},{"location":"installation_instructions/#Installation-Instructions","page":"Installation instructions","title":"Installation Instructions","text":"","category":"section"},{"location":"installation_instructions/#Installing-CalibrateEmulateSample.jl","page":"Installation instructions","title":"Installing CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Currently CalibrateEmulateSample (CES) depends on some external python dependencies  including scikit-learn wrapped by ScikitLearn.jl, which requires a couple extra  installation steps:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"First clone the project into a new local repository","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> git clone git@github.com:Clima/CalibrateEmulateSample.jl\n> cd CalibrateEmulateSample.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Install and build the project dependencies. Given that CES depends on python packages  it is easiest to set the project to use its own  Conda environment variable (set by exporting the ENV variable PYTHON=\"\").","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The scikit-learn package (along with scipy) then has to be installed if using a Julia project-specific Conda environment:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scipy=1.8.1\", channel=\"conda-forge\")'\n> PYTHON=\"\" julia --project -e 'using Conda; Conda.add(\"scikit-learn=1.1.1\")'\n","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"See the PyCall.jl documentation  for more information about how to configure the local Julia / Conda / Python environment. Typically it will require building in the  REPL via","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project\njulia> using Pkg\njulia> Pkg.build(\"PyCall\")","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To test that the package is working:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You need to first build the top-level project before building the documentation:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"cd CalibrateEmulateSample.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Then you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia --project=docs/ -e 'using Pkg; Pkg.instantiate()'\njulia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html.","category":"page"},{"location":"API/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"API/Utilities/","page":"Utilities","title":"Utilities","text":"Modules = [CalibrateEmulateSample.Utilities]\nOrder   = [:module, :type, :function]","category":"page"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_obs_sample-Union{Tuple{IT}, Tuple{Random.AbstractRNG, EnsembleKalmanProcesses.Observations.Observation}} where IT<:Int64","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_obs_sample","text":"get_obs_sample(rng::Random.AbstractRNG, obs::EnsembleKalmanProcesses.Observations.Observation; rng_seed) -> Any\n\n\nReturn a random sample from the observations, for use in the MCMC.\n\nrng - optional RNG object used to pick random sample; defaults to Random.GLOBAL_RNG.\nobs - Observation struct with the observations (extract will pick one         of the sample observations to train).\nrng_seed - optional kwarg; if provided, used to re-seed rng before sampling.\n\n\n\n\n\n","category":"method"},{"location":"API/Utilities/#CalibrateEmulateSample.Utilities.get_training_points-Union{Tuple{P}, Tuple{IT}, Tuple{FT}, Tuple{EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, IT}} where {FT, IT, P}","page":"Utilities","title":"CalibrateEmulateSample.Utilities.get_training_points","text":"get_training_points(ekp::EnsembleKalmanProcesses.EnsembleKalmanProcess{FT, IT, P}, N_train_iterations) -> EnsembleKalmanProcesses.DataContainers.PairedDataContainer\n\n\nExtract the training points needed to train the Gaussian process regression.\n\nekp - EnsembleKalmanProcess holding the parameters and the data that were produced during the Ensemble Kalman (EK) process.\nN_train_iterations - Number of EK layers/iterations to train on.\n\n\n\n\n\n","category":"method"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in CalibrateEmulateSample.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.","category":"page"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"Name Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta θ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-example","page":"Lorenz example","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"We provide the following template for how the tools may be applied.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"For small examples typically have 2 files.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"GModel.jl Contains the forward map. The inputs should be the so-called free parameters we are interested in learning, and the output should be the measured data\nThe example script which contains the inverse problem setup and solve","category":"page"},{"location":"examples/lorenz_example/#The-structure-of-the-example-script","page":"Lorenz example","title":"The structure of the example script","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"First we create the data and the setting for the model","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Set up the forward model.\nConstruct/load the truth data. Store this data conveniently in the Observations.Observation object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we set up the inverse problem","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Define the prior distributions. Use the ParameterDistribution object\nDecide on which process tool you would like to use (we recommend you begin with Invesion()). Then initialize this with the relevant constructor\ninitialize the EnsembleKalmanProcess object","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Then we solve the inverse problem, in a loop perform the following for as many iterations as required:","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"Obtain the current parameter ensemble\nTransform them from the unbounded computational space to the physical space\ncall the forward map on the ensemble of parameters, producing an ensemble of measured data\ncall the update_ensemble! function to generate a new parameter ensemble based on the new data","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz example","title":"Lorenz example","text":"One can then obtain the solution, dependent on the process type.","category":"page"},{"location":"GaussianProcessEmulator/#Gaussian-Process-Emulator","page":"Gaussian Process","title":"Gaussian Process Emulator","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"One type of MachineLearningTool we provide for emulation is a Gaussian process. Gaussian processes are a generalization of the Gaussian probability distribution, extended to functions rather than random variables. They can be used for statistical emulation, as they provide both mean and covariances. To build a Gaussian process, we first define a prior over all possible functions, by choosing the covariance function or kernel. The kernel describes how similar two outputs (y_i, y_j) are, given the similarities between their input values (x_i, x_j). Kernels encode the functional form of these relationships and are defined by hyperparameters, which are usually initially unknown to the user. To learn the posterior Gaussian process, we condition on data using Bayes theorem and optimize the hyperparameters of the kernel.  Then, we can make predictions to predict a mean function and covariance for new data points.","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"A useful resource to learn about Gaussian processes is Rasmussen and Williams (2006).","category":"page"},{"location":"GaussianProcessEmulator/#User-Interface","page":"Gaussian Process","title":"User Interface","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"CalibrateEmulateSample.jl allows the Gaussian process emulator to be built using either GaussianProcesses.jl  or ScikitLearn.jl. To use GaussianProcesses.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.GPJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"To use ScikitLearn.jl, define the package type as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gppackage = Emulators.SKLJL()","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Initialize a basic Gaussian Process with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(gppackage)","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"This initializes the prior Gaussian process.  We train the Gaussian process by feeding the gauss_proc alongside the data into the Emulator struct and optimizing the hyperparameters, described here.","category":"page"},{"location":"GaussianProcessEmulator/#Prediction-Type","page":"Gaussian Process","title":"Prediction Type","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can specify the type of prediction when initializing the Gaussian Process emulator. The default type of prediction is to predict data, YType().  You can create a latent function type prediction with","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage,\n    prediction_type = FType())\n","category":"page"},{"location":"GaussianProcessEmulator/#Kernels","page":"Gaussian Process","title":"Kernels","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"The Gaussian process above assumes the default kernel: the Squared Exponential kernel, also called  the Radial Basis Function (RBF).  A different type of kernel can be specified when the Gaussian process is initialized.  Read more about kernel options here.","category":"page"},{"location":"GaussianProcessEmulator/#GPJL","page":"Gaussian Process","title":"GPJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"For the GaussianProcess.jl package, there are a range of kernels to choose from.  For example, ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nmy_kernel = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You do not need to provide useful hyperparameter values when you define the kernel, as these are learned in  optimize_hyperparameters!(emulator).","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine kernels together through linear operations, for example,","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using GaussianProcesses\nkernel_1 = GaussianProcesses.Mat32Iso(0., 0.)      # Create a Matern 3/2 kernel with lengthscale=0 and sd=0\nkernel_2 = GaussianProcesses.Lin(0.)               # Create a linear kernel with lengthscale=0\nmy_kernel = kernel_1 + kernel_2                    # Create a new additive kernel\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/#SKLJL","page":"Gaussian Process","title":"SKLJL","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Alternatively if you are using the ScikitLearn.jl package, you can find the list of kernels here.  These need this preamble:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"using PyCall\nusing ScikitLearn\nconst pykernels = PyNULL()\nfunction __init__()\n    copy!(pykernels, pyimport(\"sklearn.gaussian_process.kernels\"))\nend","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Then they are accessible, for example, as","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"my_kernel = pykernels.RBF(length_scale = 1)\ngauss_proc = GaussianProcess(\n    gppackage;\n    kernel = my_kernel )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You can also combine multiple ScikitLearn kernels via linear operations in the same way as above.","category":"page"},{"location":"GaussianProcessEmulator/#Learning-the-noise","page":"Gaussian Process","title":"Learning the noise","text":"","category":"section"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"Often it is useful to learn the noise of the data by adding a white noise kernel. This is added with the  Boolean keyword noise_learn when initializing the Gaussian process. The default is true. ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = true )","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"When noise_learn is true, an additional white noise kernel is added to the kernel. This white noise is present across all parameter values, including the training data.  The scale parameters of the white noise kernel are learned in optimize_hyperparameters!(emulator). ","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"You may not need to learn the noise if you already have a good estimate of the noise from your training data.  When noise_learn is false, additional regularization is added for stability. The default value is 1e-3 but this can be chosen through the optional argument alg_reg_noise:","category":"page"},{"location":"GaussianProcessEmulator/","page":"Gaussian Process","title":"Gaussian Process","text":"gauss_proc = GaussianProcess(\n    gppackage;\n    noise_learn = false,\n    alg_reg_noise = 1e-3 )","category":"page"},{"location":"API/GaussianProcess/#GaussianProcess","page":"Gaussian Process","title":"GaussianProcess","text":"","category":"section"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/GaussianProcess/","page":"Gaussian Process","title":"Gaussian Process","text":"GaussianProcessesPackage\nPredictionType\nGaussianProcess\nbuild_models!\noptimize_hyperparameters!(::GaussianProcess{GPJL})","category":"page"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcessesPackage","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcessesPackage","text":"abstract type GaussianProcessesPackage\n\nType to dispatch which GP package to use:\n\nGPJL for GaussianProcesses.jl,\nSKLJL for the ScikitLearn GaussianProcessRegressor.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.PredictionType","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.PredictionType","text":"abstract type PredictionType\n\nPredict type for GPJL in GaussianProcesses.jl:\n\nYType\nFType latent function.\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.GaussianProcess","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.GaussianProcess","text":"struct GaussianProcess{GPPackage, FT} <: CalibrateEmulateSample.Emulators.MachineLearningTool\n\nStructure holding training input and the fitted Gaussian process regression models.\n\nFields\n\nmodels::Vector{Union{Nothing, PyCall.PyObject, GaussianProcesses.GPE}}\nThe Gaussian Process (GP) Regression model(s) that are fitted to the given input-data pairs.\nkernel::Union{Nothing, var\"#s11\", var\"#s12\"} where {var\"#s11\"<:GaussianProcesses.Kernel, var\"#s12\"<:PyCall.PyObject}\nKernel object.\nnoise_learn::Bool\nLearn the noise with the White Noise kernel explicitly?\nalg_reg_noise::Any\nAdditional observational or regularization noise in used in GP algorithms\nprediction_type::CalibrateEmulateSample.Emulators.PredictionType\nPrediction type (y to predict the data, f to predict the latent function).\n\n\n\n\n\n","category":"type"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.build_models!","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.build_models!","text":"build_models!(gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, input_output_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT<:AbstractFloat})\n\n\nMethod to build Gaussian process models based on the package.\n\n\n\n\n\n","category":"function"},{"location":"API/GaussianProcess/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}}","page":"Gaussian Process","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, args...; kwargs...)\n\n\nOptimize Gaussian process hyperparameters using in-build package method.\n\nWarning: if one uses GPJL() and wishes to modify positional arguments. The first positional argument must be the Optim method (default LBGFS()).\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#MarkovChainMonteCarlo","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"CurrentModule = CalibrateEmulateSample.MarkovChainMonteCarlo","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Top-level-class-and-methods","page":"MarkovChainMonteCarlo","title":"Top-level class and methods","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCWrapper\nMCMCWrapper(mcmc_alg::MCMCProtocol, obs_sample::AbstractVector{FT}, prior::ParameterDistribution, em::Emulator;init_params::AbstractVector{FT}, burnin::IT, kwargs...) where {FT<:AbstractFloat, IT<:Integer}\nsample\nget_posterior\noptimize_stepsize","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"struct MCMCWrapper\n\nTop-level class holding all configuration information needed for MCMC sampling: the prior,  emulated likelihood and sampling algorithm (DensityModel and Sampler, respectively, in  AbstractMCMC's terminology).\n\nFields\n\nprior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\nParameterDistribution object describing the prior distribution on parameter values.\nlog_posterior_map::AbstractMCMC.AbstractModel\nAdvancedMH.DensityModel object, used to evaluate the posterior density being sampled from.\nmh_proposal_sampler::AbstractMCMC.AbstractSampler\nObject describing a MCMC sampling algorithm and its settings.\nsample_kwargs::NamedTuple\nNamedTuple of other arguments to be passed to AbstractMCMC.sample().\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper-Union{Tuple{IT}, Tuple{FT}, Tuple{CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol, AbstractVector{FT}, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, CalibrateEmulateSample.Emulators.Emulator}} where {FT<:AbstractFloat, IT<:Integer}","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper","text":"MCMCWrapper(mcmc_alg::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol, obs_sample::AbstractArray{FT<:AbstractFloat, 1}, prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, em::CalibrateEmulateSample.Emulators.Emulator; init_params, burnin, kwargs...)\n\n\nConstructor for MCMCWrapper which performs the same standardization (SVD  decorrelation) that was applied in the Emulator. It creates and wraps an instance of  EmulatorPosteriorModel, for sampling from the Emulator, and  MetropolisHastingsSampler, for generating the MC proposals.\n\nmcmc_alg: MCMCProtocol describing the MCMC sampling algorithm to use. Currently implemented algorithms are:\nRWMHSampling: Metropolis-Hastings sampling from a vanilla random walk with fixed stepsize.\npCNMHSampling: Metropolis-Hastings sampling using the preconditioned  Crank-Nicholson algorithm, which has a well-behaved small-stepsize limit.\nobs_sample: A single sample from the observations. Can, e.g., be picked from an  Observation struct using get_obs_sample.\nprior: ParameterDistribution  object containing the parameters' prior distributions.\nem: Emulator to sample from. \nstepsize: MCMC step size, applied as a scaling to the prior covariance.\ninit_params: Starting parameter values for MCMC sampling.\nburnin: Initial number of MCMC steps to discard from output (pre-convergence).\n\n\n\n\n\n","category":"method"},{"location":"API/MarkovChainMonteCarlo/#StatsBase.sample","page":"MarkovChainMonteCarlo","title":"StatsBase.sample","text":"sample([rng,] mcmc::MCMCWrapper, args...; kwargs...)\n\nExtends the sample methods of AbstractMCMC (which extends StatsBase) to sample from the  emulated posterior, using the MCMC sampling algorithm and Emulator configured in  MCMCWrapper. Returns a MCMCChains.Chains  object containing the samples. \n\nSupported methods are:\n\nsample([rng, ]mcmc, N; kwargs...)\nReturn a MCMCChains.Chains object containing N samples from the emulated posterior.\nsample([rng, ]mcmc, isdone; kwargs...)\nSample from the model with the Markov chain Monte Carlo sampler until a convergence  criterion isdone returns true, and return the samples. The function isdone has the  signature\n    isdone(rng, model, sampler, samples, state, iteration; kwargs...)\nwhere state and iteration are the current state and iteration of the sampler,  respectively. It should return true when sampling should end, and false otherwise.\nsample([rng, ]mcmc, parallel_type, N, nchains; kwargs...)\nSample nchains Monte Carlo Markov chains in parallel according to parallel_type, which may be MCMCThreads() or MCMCDistributed() for thread and parallel sampling,  respectively.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.get_posterior","text":"get_posterior(mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper, chain::MCMCChains.Chains) -> Any\n\n\nReturns a ParameterDistribution object corresponding to the empirical distribution of the  samples in chain.\n\nnote: Note\nThis method does not currently support combining samples from multiple Chains.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.optimize_stepsize","text":"optimize_stepsize(rng::Random.AbstractRNG, mcmc::CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCWrapper; init_stepsize, N, max_iter, sample_kwargs...) -> Float64\n\n\nUses a heuristic to return a stepsize for the mh_proposal_sampler element of  MCMCWrapper which yields fast convergence of the Markov chain.\n\nThe criterion used is that Metropolis-Hastings proposals should be accepted between 15% and  35% of the time.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"See AbstractMCMC sampling API for background on our use of Turing.jl's  AbstractMCMC API for  MCMC sampling.","category":"page"},{"location":"API/MarkovChainMonteCarlo/#Sampler-algorithms","page":"MarkovChainMonteCarlo","title":"Sampler algorithms","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCProtocol\nRWMHSampling\npCNMHSampling\nMetropolisHastingsSampler","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol","text":"abstract type MCMCProtocol\n\nType used to dispatch different methods of the MetropolisHastingsSampler  constructor, corresponding to different sampling algorithms.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling","text":"struct RWMHSampling <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters as as vanilla random walk, based on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.pCNMHSampling","text":"struct pCNMHSampling <: CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCProtocol\n\nMCMCProtocol which uses Metropolis-Hastings sampling that generates proposals for  new parameters according to the preconditioned Crank-Nicholson (pCN) algorithm, which is  usable for MCMC in the stepsize → 0 limit, unlike the vanilla random walk. Steps are based  on the covariance of prior.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MetropolisHastingsSampler","text":"MetropolisHastingsSampler(_::CalibrateEmulateSample.MarkovChainMonteCarlo.RWMHSampling, prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution) -> CalibrateEmulateSample.MarkovChainMonteCarlo.RWMetropolisHastings\n\n\nConstructor for all Sampler objects, with one method for each supported MCMC algorithm.\n\nwarning: Warning\nBoth currently implemented Samplers use a Gaussian approximation to the prior:  specifically, new Metropolis-Hastings proposals are a scaled combination of the old  parameter values and a random shift distributed as a Gaussian with the same covariance as the prior. This suffices for our current use case, in which our priors are Gaussian (possibly in a transformed space) and we assume enough fidelity in the Emulator that  inference isn't prior-dominated.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Emulated-posterior-(Model)","page":"MarkovChainMonteCarlo","title":"Emulated posterior (Model)","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"EmulatorPosteriorModel","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.EmulatorPosteriorModel","text":"EmulatorPosteriorModel(prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, obs_sample::AbstractArray{FT<:AbstractFloat, 1}) -> AdvancedMH.DensityModel\n\n\nFactory which constructs AdvancedMH.DensityModel objects given a prior on the model  parameters (prior) and an Emulator of the log-likelihood of the data given  parameters. Together these yield the log posterior density we're attempting to sample from  with the MCMC, which is the role of the DensityModel class in the AbstractMCMC interface.\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-MCMC-State","page":"MarkovChainMonteCarlo","title":"Internals - MCMC State","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"MCMCState\naccept_ratio","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.MCMCState","text":"struct MCMCState{T, L<:Real} <: AdvancedMH.AbstractTransition\n\nExtends the AdvancedMH.Transition (which encodes the current state of the MC during sampling) with a boolean flag to record whether this state is new (arising from accepting a Metropolis-Hastings proposal) or old (from rejecting a proposal).\n\nFields\n\nparams::Any\nSampled value of the parameters at the current state of the MCMC chain.\nlog_density::Real\nLog probability of params, as computed by the model using the prior.\naccepted::Bool\nWhether this state resulted from accepting a new MH proposal.\n\n\n\n\n\n","category":"type"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.accept_ratio","text":"accept_ratio(chain::MCMCChains.Chains) -> Any\n\n\nFraction of MC proposals in chain which were accepted (according to Metropolis-Hastings.)\n\n\n\n\n\n","category":"function"},{"location":"API/MarkovChainMonteCarlo/#Internals-Other","page":"MarkovChainMonteCarlo","title":"Internals - Other","text":"","category":"section"},{"location":"API/MarkovChainMonteCarlo/","page":"MarkovChainMonteCarlo","title":"MarkovChainMonteCarlo","text":"to_decorrelated","category":"page"},{"location":"API/MarkovChainMonteCarlo/#CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","page":"MarkovChainMonteCarlo","title":"CalibrateEmulateSample.MarkovChainMonteCarlo.to_decorrelated","text":"to_decorrelated(data::AbstractArray{FT<:AbstractFloat, 2}, em::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}) -> Any\n\n\nTransform samples from the original (correlated) coordinate system to the SVD-decorrelated coordinate system used by Emulator. Used in the constructor for MCMCWrapper.\n\n\n\n\n\n","category":"function"},{"location":"calibrate/#The-Calibrate-stage","page":"Calibrate","title":"The Calibrate stage","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration of the computer model entails finding an optimal parameter theta^* that maximizes the posterior probability","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"rho(thetavert y Gamma_y) = dfrace^-mathcalL(theta y)Z(yvertGamma_y)rho_mathrmprior(theta) qquad mathcalL(theta y) = langle mathcalG(theta) - y    Gamma_y^-1 left ( mathcalG(theta) - y right ) rangle","category":"page"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"where mathcalL is the loss or negative log-likelihood, Z(yvertGamma) is a normalizing constant, y represents the data, Gamma_y is the noise covariance matrix and rho_mathrmprior(theta) is the prior density. Calibration is performed using ensemble Kalman processes, which generate input-output pairs theta mathcalG(theta) in high density from the prior initial guess to the found optimal parameter theta^*. These input-output pairs are then used as the data to train an emulator of the forward model mathcalG.","category":"page"},{"location":"calibrate/#Ensemble-Kalman-Processes","page":"Calibrate","title":"Ensemble Kalman Processes","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Calibration can be performed using different ensemble Kalman processes: ensemble Kalman inversion (Iglesias et al, 2013), ensemble Kalman sampler (Garbuno-Inigo et al, 2020), and unscented Kalman inversion (Huang et al, 2022). All algorithms are implemented in EnsembleKalmanProcesses.jl. Documentation of each algorithm is available in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"calibrate/#Typical-construction-of-the-EnsembleKalmanProcess","page":"Calibrate","title":"Typical construction of the EnsembleKalmanProcess","text":"","category":"section"},{"location":"calibrate/","page":"Calibrate","title":"Calibrate","text":"Documentation on how to construct an EnsembleKalmanProcess from the computer model and the data can be found in the EnsembleKalmanProcesses docs.","category":"page"},{"location":"API/Emulators/#Emulators","page":"General Emulator","title":"Emulators","text":"","category":"section"},{"location":"API/Emulators/","page":"General Emulator","title":"General Emulator","text":"CurrentModule = CalibrateEmulateSample.Emulators","category":"page"},{"location":"API/Emulators/","page":"General Emulator","title":"General Emulator","text":"Emulator\noptimize_hyperparameters!(::Emulator)\npredict\nnormalize\nstandardize\nreverse_standardize\nsvd_transform\nsvd_reverse_transform_mean_cov","category":"page"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.Emulator","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.Emulator","text":"struct Emulator{FT<:AbstractFloat}\n\nStructure used to represent a general emulator, independently of the algorithm used.\n\nFields\n\nmachine_learning_tool::CalibrateEmulateSample.Emulators.MachineLearningTool\nMachine learning tool, defined as a struct of type MachineLearningTool.\ntraining_pairs::EnsembleKalmanProcesses.DataContainers.PairedDataContainer{FT} where FT<:AbstractFloat\nNormalized, standardized, transformed pairs given the Booleans normalize_inputs, standardize_outputs, retained_svd_frac.\ninput_mean::AbstractVector{FT} where FT<:AbstractFloat\nMean of input; length input_dim.\nnormalize_inputs::Bool\nSquare root of the inverse of the input covariance matrix; size input_dim × input_dim.\nsqrt_inv_input_cov::Union{Nothing, LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat\nWhether to fit models on normalized outputs: outputs / standardize_outputs_factor.\nstandardize_outputs::Bool\nIf normalizing: whether to fit models on normalized inputs ((inputs - input_mean) * sqrt_inv_input_cov).\nstandardize_outputs_factors::Union{Nothing, AbstractVector{FT}} where FT<:AbstractFloat\nIf standardizing: Standardization factors (characteristic values of the problem).\ndecomposition::Union{Nothing, LinearAlgebra.SVD}\nThe singular value decomposition of obs_noise_cov, such that obs_noise_cov = decomposition.U * Diagonal(decomposition.S) * decomposition.Vt. NB: the SVD may be reduced in dimensions.\nretained_svd_frac::AbstractFloat\nFraction of singular values kept in decomposition. A value of 1 implies full SVD spectrum information.\n\n\n\n\n\n","category":"type"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.optimize_hyperparameters!-Tuple{CalibrateEmulateSample.Emulators.Emulator}","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.optimize_hyperparameters!","text":"optimize_hyperparameters!(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, args...; kwargs...)\n\n\nOptimizes the hyperparameters in the machine learning tool.\n\n\n\n\n\n","category":"method"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.predict","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.predict","text":"predict(gp::CalibrateEmulateSample.Emulators.GaussianProcess{CalibrateEmulateSample.Emulators.GPJL}, new_inputs::AbstractArray{FT<:AbstractFloat, 2}) -> Tuple{Any, Any}\n\n\nPredict means and covariances in decorrelated output space using Gaussian process models.\n\n\n\n\n\npredict(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, new_inputs::AbstractArray{FT<:AbstractFloat, 2}; transform_to_real) -> Tuple{Any, Any}\n\n\nMakes a prediction using the emulator on new inputs (each new inputs given as data columns). Default is to predict in the decorrelated space.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.normalize","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.normalize","text":"normalize(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, inputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}) -> Any\n\n\nNormalize the input data, with a normalizing function.\n\n\n\n\n\nnormalize(inputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}, input_mean::AbstractArray{FT<:AbstractFloat, 1}, sqrt_inv_input_cov::Union{AbstractArray{FT<:AbstractFloat, 2}, LinearAlgebra.UniformScaling{FT<:AbstractFloat}}) -> Any\n\n\nNormalize with the empirical Gaussian distribution of points.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.standardize","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.standardize","text":"standardize(outputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}, output_covs::Array{<:Union{AbstractArray{FT<:AbstractFloat, 2}, LinearAlgebra.UniformScaling{FT<:AbstractFloat}}, 1}, factors::AbstractArray{FT<:AbstractFloat, 1}) -> Tuple{Any, Vector{<:Union{LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}}} where FT<:AbstractFloat}\n\n\nStandardize with a vector of factors (size equal to output dimension).\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.reverse_standardize","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.reverse_standardize","text":"reverse_standardize(emulator::CalibrateEmulateSample.Emulators.Emulator{FT<:AbstractFloat}, outputs::Union{AbstractArray{FT<:AbstractFloat, 1}, AbstractArray{FT<:AbstractFloat, 2}}, output_covs::Union{AbstractArray{FT<:AbstractFloat, 2}, Array{<:AbstractArray{FT<:AbstractFloat, 2}, 1}}) -> Tuple{Any, Any}\n\n\nReverse a previous standardization with the stored vector of factors (size equal to output  dimension). output_cov is a Vector of covariance matrices, such as is returned by svd_reverse_transform_mean_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_transform","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.svd_transform","text":"svd_transform(data::AbstractArray{FT<:AbstractFloat, 2}, obs_noise_cov::Union{Nothing, AbstractArray{FT<:AbstractFloat, 2}}; retained_svd_frac) -> Tuple{Any, Any}\n\n\nApply a singular value decomposition (SVD) to the data\n\ndata - GP training data/targets; size output_dim × N_samples\nobs_noise_cov - covariance of observational noise\ntruncate_svd - Project onto this fraction of the largest principal components. Defaults to 1.0 (no truncation).\n\nReturns the transformed data and the decomposition, which is a matrix  factorization of type LinearAlgebra.SVD. \n\nNote: If F::SVD is the factorization object, U, S, V and Vt can be obtained via  F.U, F.S, F.V and F.Vt, such that A = U * Diagonal(S) * Vt. The singular values  in S are sorted in descending order.\n\n\n\n\n\n","category":"function"},{"location":"API/Emulators/#CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","page":"General Emulator","title":"CalibrateEmulateSample.Emulators.svd_reverse_transform_mean_cov","text":"svd_reverse_transform_mean_cov(μ::AbstractArray{FT<:AbstractFloat, 2}, σ2::AbstractArray{FT<:AbstractFloat, 2}, decomposition::LinearAlgebra.SVD) -> Tuple{Any, Any}\n\n\nTransform the mean and covariance back to the original (correlated) coordinate system\n\nμ - predicted mean; size output_dim × N_predicted_points.\nσ2 - predicted variance; size output_dim × N_predicted_points.\ndecomposition - SVD decomposition of obs_noise_cov.\n\nReturns the transformed mean (size output_dim × N_predicted_points) and variance.  Note that transforming the variance back to the original coordinate system results in non-zero off-diagonal elements, so instead of just returning the  elements on the main diagonal (i.e., the variances), we return the full  covariance at each point, as a vector of length N_predicted_points, where  each element is a matrix of size output_dim × output_dim.\n\n\n\n\n\n","category":"function"},{"location":"emulate/#The-Emulate-stage","page":"Emulate","title":"The Emulate stage","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Emulation is performed through the construction of an Emulator object, which has two components","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"A wrapper for any statistical emulator,\nData-processing and dimensionality reduction functionality.","category":"page"},{"location":"emulate/#Typical-construction-from-Lorenz_example.jl","page":"Emulate","title":"Typical construction from Lorenz_example.jl","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"First, obtain data in a PairedDataContainer, for example, get this from an EnsembleKalmanProcess ekpobj generated during the Calibrate stage, or see the constructor here","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"using CalibrateEmulateSample.Utilities\ninput_output_pairs = Utilities.get_training_points(ekpobj, 5) # use first 5 iterations as data","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Wrapping a predefined machine learning tool, e.g. a Gaussian process gauss_proc, the Emulator can then be built:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"emulator = Emulator(\n    gauss_proc, \n    input_output_pairs; # optional arguments after this\n    obs_noise_cov = Γy,\n    normalize_inputs = true,\n    standardize_outputs = true,\n    standardize_outputs_factors = factor_vector,\n    retained_svd_frac = 0.95,\n)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"The optional arguments above relate to the data processing.","category":"page"},{"location":"emulate/#Emulator-Training","page":"Emulate","title":"Emulator Training","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"The emulator is trained when we combine the machine learning tool and the data into the Emulator above.  For any machine learning tool, we must also optimize the hyperparameters:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"optimize_hyperparameters!(emulator)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"In the Lorenz example, this line learns the hyperparameters of the Gaussian process, which depend on the choice of kernel. Predictions at new inputs can then be made using","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"y, cov = Emulator.predict(emulator, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This returns both a mean value and a covariance.","category":"page"},{"location":"emulate/#Data-processing","page":"Emulate","title":"Data processing","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Some effects of the following are outlined in a practical setting in the results and appendices of Howland, Dunbar, Schneider, (2022).","category":"page"},{"location":"emulate/#Diagonalization-and-output-dimension-reduction","page":"Emulate","title":"Diagonalization and output dimension reduction","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"obs_noise_cov = Γy (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"We always use singular value decomposition to diagonalize the output space, requiring output covariance Γy. Why? If we need to train a mathbbR^10 to mathbbR^100 emulator, diagonalization allows us to instead train 100 mathbbR^10 to mathbbR^1 emulators (far cheaper).","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"retained_svd_frac = 0.95 (default 1.0)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Performance is increased further by throwing away less informative output dimensions, if 95% of the information (i.e., variance) is in the first 40 diagonalized output dimensions then setting retained_svd_frac=0.95 will train only 40 emulators.","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"note: Note\nDiagonalization is an approximation. It is however a good approximation when the observational covariance varies slowly in the parameter space.","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"warn: Warn\nSevere approximation errors can occur if obs_noise_cov is not provided.","category":"page"},{"location":"emulate/#Normalization-and-standardization","page":"Emulate","title":"Normalization and standardization","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"This arises from the optional arguments","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"normalize_inputs = true (default: true)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"We normalize the input data in a standard way by centering, and scaling with the empirical covariance","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"standardize_outputs = true (default: false)\nstandardize_outputs_factors = factor_vector (default: nothing)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"To help with poor conditioning of the covariance matrix, users can also standardize each output dimension with by a multiplicative factor given by the elements of factor_vector","category":"page"},{"location":"emulate/#Modular-interface","page":"Emulate","title":"Modular interface","text":"","category":"section"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Each statistical emulator has the following supertype and methods:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"abstract type MachineLearningTool end\nfunction build_models!(mlt, iopairs)\nfunction optimize_hyperparameters!(mlt)\nfunction predict(mlt, new_inputs)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Add a new tool as follows:","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"Create MyMLToolName.jl, and include \"MyMLToolName.jl\" in Emulators.jl\nCreate a struct MyMLTool <: MachineLearningTool \nCreate these three methods to build, train, and predict with your tool (use GaussianProcess.jl as a guide)","category":"page"},{"location":"emulate/","page":"Emulate","title":"Emulate","text":"note: Note\nThe predict method currently needs to return both a predicted mean and a predicted (co)variance at new inputs, which are used in the Sample stage.","category":"page"},{"location":"#CalibrateEmulateSample.jl","page":"Home","title":"CalibrateEmulateSample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl solves parameter estimation problems using accelerated (and approximate) Bayesian inversion.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The framework can be applied currently to learn:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the joint distribution for a moderate numbers of parameters (<40),\nit is not inherently restricted to unimodal distributions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It can be used with computer models that:","category":"page"},{"location":"","page":"Home","title":"Home","text":"can be noisy or chaotic,\nare non-differentiable,\ncan only be treated as black-box (interfaced only with parameter files).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The computer model is supplied by the user, as a parameter-to-data map mathcalG(theta) mathbbR^p rightarrow mathbbR^d. For example, mathcalG could be a map from any given parameter configuration theta to a collection of statistics of a dynamical system trajectory. mathcalG is referred to as the forward model in the Bayesian inverse problem setting.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The data produced by the forward model are compared to observations y, which are assumed to be corrupted by additive noise eta, such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"y = mathcalG(theta) + eta","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the noise eta is drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y).","category":"page"},{"location":"#The-inverse-problem","page":"Home","title":"The inverse problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Given an observation y, the computer model mathcalG, the observational noise Gamma_y, and some broad prior information on theta, we return the joint distribution of a data-informed distribution for \"theta given y\".","category":"page"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, CalibrateEmulateSample.jl breaks this problem into a sequence of three steps: calibration, emulation, and sampling. A comprehensive treatment of the calibrate-emulate-sample approach to Bayesian inverse problems can be found in Cleary et al. (2020).","category":"page"},{"location":"#The-three-steps-of-the-algorithm:","page":"Home","title":"The three steps of the algorithm:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The calibrate step of the algorithm consists of an application of Ensemble Kalman Processes, which generates input-output pairs theta mathcalG(theta) in high density around an optimal parameter theta^*. This theta^* will be near a mode of the posterior distribution (Note: This the only time we interface with the forward model mathcalG).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The emulate step takes these pairs theta mathcalG(theta) and trains a statistical surrogate model (e.g., a Gaussian process), emulating the forward map mathcalG.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The sample step uses this surrogate in place of mathcalG in a sampling method (Markov chain Monte Carlo) to sample the posterior distribution of theta.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl contains the following modules:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nCalibrateEmulateSample.jl Pulls in the Ensemble Kalman Processes package\nEmulator.jl Emulate: Modular template for emulators\nGaussianProcess.jl - A Gaussian process emulator\nMarkovChainMonteCarlo.jl Sample: Modular template for MCMC\nUtilities.jl Helper functions","category":"page"},{"location":"","page":"Home","title":"Home","text":"The best way to get started is to have a look at the examples!","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CalibrateEmulateSample.jl is being developed by the Climate Modeling Alliance.","category":"page"},{"location":"examples/edmf_example/#Eddy-Diffusivity-Mass-Flux-model-(EDMF)","page":"Turbulence example","title":"Eddy-Diffusivity Mass-Flux model (EDMF)","text":"","category":"section"},{"location":"examples/edmf_example/#Background","page":"Turbulence example","title":"Background","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"... What is EDMF","category":"page"},{"location":"examples/edmf_example/#What-is-being-solved-here","page":"Turbulence example","title":"What is being solved here","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"...Set up the inverse problem (prior, model, data and noise)","category":"page"},{"location":"examples/edmf_example/#Running-the-examples","page":"Turbulence example","title":"Running the examples","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"We have two example scenario data (output from a (C)alibration run) that must be simply unzipped before calibration:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"ent-det-calibration.zip # two-parameter calibration\nent-det-tked-tkee-stab-calibration.zip # five-parameter calibration","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To perform uncertainty quantification use the file uq_for_EDMF.jl. Set the experiment name, and date (for outputs), e.g.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"exp_name = \"ent-det-tked-tkee-stab-calibration\" \ndate_of_run = Date(year, month, day)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"and call,","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"> julia --project uq_for_EDMF.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Info\nThese runs take currently take ~1 hour to complete","category":"page"},{"location":"examples/edmf_example/#Solution-and-output","page":"Turbulence example","title":"Solution and output","text":"","category":"section"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The solution is the posterior distribution, stored in the file posterior.jld2.","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior is visualized by using plot_posterior.jl, which produces corner-type scatter plots of posterior distribution, which show pairwise correlations. Again, set the exp_name and date_of_run values, then call","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"julia --project plot_posterior.jl","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"The posterior samples can also be investigated directly. They are stored as a ParameterDistribution-type Samples object. One can load this and extract an array of parameters with:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"# input:\n# path to posterior.jld2: posterior_filepath (string)\n\nusing CalibrateEmulateSample.ParameterDistribution\nposterior = load(posterior_filepath)[\"posterior\"]\nposterior_samples = vcat([get_distribution(posterior)[name] for name in get_name(posterior)]...) # samples are columns","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"To transform these samples into physical parameter space use the following:","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"transformed_posterior_samples =\nmapslices(x -> transform_unconstrained_to_constrained(posterior, x), posterior_samples, dims = 1)","category":"page"},{"location":"examples/edmf_example/","page":"Turbulence example","title":"Turbulence example","text":"info: Computational vs Physical space\nThe computational theta-space are the parameters on which the algorithms act. Statistics (e.g. mean/covariance) are most meaningful when taken in this space. The physical phi-space is a (nonlinear) transformation of the computational space to apply parameter constraints. To pass parameter values back into the forward model, one must transform them. Full details and examples can be found here","category":"page"}]
}
