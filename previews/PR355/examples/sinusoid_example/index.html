<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Simple example walkthrough · CalibrateEmulateSample.jl</title><meta name="title" content="Simple example walkthrough · CalibrateEmulateSample.jl"/><meta property="og:title" content="Simple example walkthrough · CalibrateEmulateSample.jl"/><meta property="twitter:title" content="Simple example walkthrough · CalibrateEmulateSample.jl"/><meta name="description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="og:description" content="Documentation for CalibrateEmulateSample.jl."/><meta property="twitter:description" content="Documentation for CalibrateEmulateSample.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="CalibrateEmulateSample.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CalibrateEmulateSample.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation_instructions/">Installation instructions</a></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Simple example walkthrough</a><ul class="internal"><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li class="toplevel"><a class="tocitem" href="#Walkthrough-of-code"><span>Walkthrough of code</span></a></li><li><a class="tocitem" href="#Set-up"><span>Set up</span></a></li><li><a class="tocitem" href="#Calibrate"><span>Calibrate</span></a></li><li><a class="tocitem" href="#Emulate"><span>Emulate</span></a></li><li><a class="tocitem" href="#Sample"><span>Sample</span></a></li></ul></li><li><a class="tocitem" href="../lorenz_example/">Lorenz example</a></li><li><a class="tocitem" href="../edmf_example/">Turbulence example</a></li><li><a class="tocitem" href="../Cloudy_example/">Cloudy example</a></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Emulator testing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../emulators/regression_2d_2d/">Regression of <span>$\mathbb{R}^2 \to \mathbb{R}^2$</span> smooth function</a></li><li><a class="tocitem" href="../emulators/lorenz_integrator_3d_3d/">Integrating Lorenz 63 with an emulated integrator</a></li><li><a class="tocitem" href="../emulators/global_sens_analysis/">Global Sensitiviy Analysis (GSA) test functions</a></li></ul></li></ul></li><li><a class="tocitem" href="../../calibrate/">Calibrate</a></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Emulate</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../emulate/">Emulator</a></li><li><a class="tocitem" href="../../GaussianProcessEmulator/">Gaussian Process</a></li><li><a class="tocitem" href="../../random_feature_emulator/">Random Features</a></li></ul></li><li><a class="tocitem" href="../../sample/">Sample</a></li><li><a class="tocitem" href="../../glossary/">Glossary</a></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">API</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-9-1" type="checkbox"/><label class="tocitem" for="menuitem-9-1"><span class="docs-label">CalibrateEmulateSample</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-9-1-1" type="checkbox"/><label class="tocitem" for="menuitem-9-1-1"><span class="docs-label">Emulators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../API/Emulators/">General Interface</a></li><li><a class="tocitem" href="../../API/GaussianProcess/">Gaussian Process</a></li><li><a class="tocitem" href="../../API/RandomFeatures/">Random Features</a></li></ul></li><li><a class="tocitem" href="../../API/MarkovChainMonteCarlo/">MarkovChainMonteCarlo</a></li><li><a class="tocitem" href="../../API/Utilities/">Utilities</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Simple example walkthrough</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Simple example walkthrough</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/CliMA/CalibrateEmulateSample.jl/blob/main/docs/src/examples/sinusoid_example.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Sinusoid-Example"><a class="docs-heading-anchor" href="#Sinusoid-Example">Sinusoid Example</a><a id="Sinusoid-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Sinusoid-Example" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">How do I run this code?</header><div class="admonition-body"><p>The full code is found in the <a href="https://github.com/CliMA/CalibrateEmulateSample.jl/tree/main/examples"><code>examples/</code></a> directory of the github repository</p></div></div><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>This example demonstrates how to use <code>CalibrateEmulateSample.jl</code> for a simple model that generates noisy observables of a signal. The sinusoid signal is defined by two parameters: its shift along the vertical axis and its amplitude. We make noisy observations of the signal and we can calculate the mean of the signal, which is informative about its shift along the axis, and the range of the signal, which is informative  about the amplitude. Although our sinusoid function is simple and quick to evaluate, we shall pretend it is non-differentiable and expensive to evaluate, as a case study for carrying out uncertainty quantification on  more complex systems. Additionally, we will work in a &quot;perfect model&quot; setting for this example, meaning we will generate pseudo-observations for our model and pretend that these are noisy observations of our system.</p><h3 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h3><p>We have a model of a sinusoidal signal that is a function of parameters <span>$\theta=(A,v)$</span>, where <span>$A$</span> is the amplitude of the signal and <span>$v$</span> is vertical shift of the signal:</p><p class="math-container">\[f(A, v) = A \sin(\phi + t) + v, \forall t \in [0,2\pi]\]</p><p>Here, <span>$\phi$</span> is the random phase of each signal.  The goal is to estimate not just the point estimates of the parameters <span>$\theta=(A,v)$</span>, but entire probability distributions of them, given some noisy observations. We will use the range and mean of a signal as our observable: </p><p class="math-container">\[G(\theta) = \big[ \text{range}\big(f(\theta)\big), \text{mean}\big(f(\theta)\big) \big] \]</p><p>This highlights the role of choosing a good observable, in particular our choice of <span>$G$</span> is independent of the random phase shift <span>$\phi$</span> and is in fact deterministic. This allows us to write out an expression for the noisy observation, <span>$y_{obs}$</span>:</p><p class="math-container">\[y_{obs} = G(\theta) + \gamma, \qquad \gamma \sim \mathcal{N}(0, \Gamma)\]</p><p>where <span>$\Gamma$</span> is the observational covariance matrix. We will assume the noise to be independent for each observable, giving us a diagonal covariance matrix.</p><h1 id="Walkthrough-of-code"><a class="docs-heading-anchor" href="#Walkthrough-of-code">Walkthrough of code</a><a id="Walkthrough-of-code-1"></a><a class="docs-heading-anchor-permalink" href="#Walkthrough-of-code" title="Permalink"></a></h1><p>You can find the full scripts to reproduce this tutorial in <code>examples/Sinusoid/</code>. The code is split into four sections:</p><ol><li>Model set up in <code>sinusoid_setup.jl</code></li><li>Calibrate in <code>calibrate.jl</code></li><li>Emulate in <code>emulate.jl</code></li><li>Sample in <code>sample.jl</code></li></ol><p>You do not need to explicitly run <code>sinusoid_setup.jl</code> as it is called from <code>calibrate.jl</code>. However, this file contains the functions for the model and for generating pseudo-observations.  You will need to run steps 2-4 in order as each one relies on output saved from the previous steps.</p><h2 id="Set-up"><a class="docs-heading-anchor" href="#Set-up">Set up</a><a id="Set-up-1"></a><a class="docs-heading-anchor-permalink" href="#Set-up" title="Permalink"></a></h2><p>First, we load the packages we need for setting up the model:</p><pre><code class="language-julia hljs">using LinearAlgebra, Random
using Plots
using JLD2
using Statistics, Distributions
</code></pre><p>We define a model that generates a sinusoid given parameters <span>$\theta=(A,v)$</span>  (amplitude and vertical shift). We will estimate these parameters from data. The model adds a random phase shift upon evaluation.</p><pre><code class="language-julia hljs"># Define x-axis
dt = 0.01
trange = 0:dt:(2 * pi + dt)

function model(amplitude, vert_shift)
    # Set phi, the random phase
    phi = 2 * pi * rand()
    return amplitude * sin.(trange .+ phi) .+ vert_shift
end
</code></pre><p>We will define a &quot;true&quot; amplitude and vertical shift to generate some pseudo-observations.  Let <span>$\theta=(3.0, 7.0)$</span>.</p><pre><code class="language-julia hljs">amplitude_true = 3.0
vert_shift_true = 7.0
# Our input parameters are 2d and our outputs are 2d
theta_true = [amplitude_true, vert_shift_true]
dim_params = 2
# Generate the &quot;true&quot; signal for these parameters
signal_true = model(amplitude_true, vert_shift_true)</code></pre><p>We will observe properties of the signal that inform us about the amplitude and vertical  position. These properties will be the range (the difference between the maximum and the minimum), which is informative about the amplitude of the sinusoid, and the mean, which is informative  about the vertical shift. </p><pre><code class="language-julia hljs">y1_true = maximum(signal_true) - minimum(signal_true)
y2_true = mean(signal_true)</code></pre><p>However, our observations are typically not noise-free, so we add some white noise to our  observables. We call this <span>$y_{obs}$</span>. The user can choose the observational covariance matrix, <span>$\Gamma$</span>. We will assume the noise is independent (a diagonal covariance matrix <span>$\Gamma=0.2 * I$</span>). </p><pre><code class="language-julia hljs">dim_output = 2
Γ = 0.2 * I
white_noise = MvNormal(zeros(dim_output), Γ)
y_obs = [y1_true, y2_true] .+ rand(white_noise)
println(&quot;Observations:&quot;, y_obs)</code></pre><p>This gives <span>$y_{obs}=(6.15, 6.42)$</span>. We can plot the true signal in black, the true observables in red and the noisy observables in blue. <img src="../../assets/sinusoid_true_vs_observed_signal.png" alt="signal"/></p><p>It will be helpful for us to define a function <span>$G(\theta)$</span>, which returns these observables  (the range and the mean) of the sinusoid given a parameter vector. </p><pre><code class="language-julia hljs">function G(theta)
    amplitude, vert_shift = theta
    sincurve = model(amplitude, vert_shift)
    return [maximum(sincurve) - minimum(sincurve), mean(sincurve)]
end</code></pre><h2 id="Calibrate"><a class="docs-heading-anchor" href="#Calibrate">Calibrate</a><a id="Calibrate-1"></a><a class="docs-heading-anchor-permalink" href="#Calibrate" title="Permalink"></a></h2><p>We are interested in learning the posterior distribution of <span>$\theta$</span> for the inverse problem <span>$y_{obs}=G(\theta)+\mathcal{N}(0,\Gamma)$</span>. We first carry out calibration, which aims to solve the inverse problem for point estimates of the optimal values for <span>$\theta$</span>. Specifically, we use an ensemble based calibration method,  such as Ensemble Kalman Inversion, because it provides us with ensembles of <span>$G(\theta)$</span> evaluations that  are focused near to the optimal values for <span>$\theta$</span>. These ensembles provide us with a suitable dataset  for training an emulator to be used in sampling the posterior distribution. </p><p>We are using the <a href="https://clima.github.io/EnsembleKalmanProcesses.jl/dev/">EnsembleKalmanProcesses.jl</a> package for Ensemble Kalman Inversion (EKI). We start with user-defined prior distributions and sample an ensemble of parameters <span>$\theta$</span>, which we use to evaluate <span>$G(\theta)$</span>. Then, we iteratively update the ensemble until our parameters <span>$\theta$</span> are near to the optimal.</p><p>First, we will load the packages we need from CES:</p><pre><code class="language-julia hljs"># CES
using CalibrateEmulateSample
const EKP = CalibrateEmulateSample.EnsembleKalmanProcesses
const PD = EKP.ParameterDistributions</code></pre><p>We define prior distributions on the two parameters. For the amplitude, we define a prior with mean 2 and standard deviation 1. It is additionally constrained to be nonnegative. For the vertical shift we define a Gaussian prior with mean 0 and standard deviation 5.</p><pre><code class="language-julia hljs">prior_u1 = PD.constrained_gaussian(&quot;amplitude&quot;, 2, 1, 0, Inf)
prior_u2 = PD.constrained_gaussian(&quot;vert_shift&quot;, 0, 5, -Inf, Inf)
prior = PD.combine_distributions([prior_u1, prior_u2])
# Plot priors
p = plot(prior, fill = :lightgray)</code></pre><p><img src="../../assets/sinusoid_prior.png" alt="prior"/></p><p>We now generate the initial ensemble and set up the EKI.</p><pre><code class="language-julia hljs">N_ensemble = 10
N_iterations = 5

initial_ensemble = EKP.construct_initial_ensemble(prior, N_ensemble)

ensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, y_obs, Γ, EKP.Inversion())</code></pre><p>We are now ready to carry out the inversion. At each iteration, we get the ensemble from the last iteration, apply <span>$G(\theta)$</span> to each ensemble member, and apply the Kalman update to the ensemble.</p><pre><code class="language-julia hljs">for i in 1:N_iterations
    params_i = EKP.get_ϕ_final(prior, ensemble_kalman_process)

    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)

    EKP.update_ensemble!(ensemble_kalman_process, G_ens)
end</code></pre><p>Finally, we get the ensemble after the last iteration. This provides our estimate of the parameters.</p><pre><code class="language-julia hljs">final_ensemble = EKP.get_ϕ_final(prior, ensemble_kalman_process)

# Check that the ensemble mean is close to the theta_true
println(&quot;Ensemble mean: &quot;, mean(final_ensemble, dims=2))   # [3.05, 6.37]
println(&quot;True parameters: &quot;, theta_true)   # [3.0, 7.0]</code></pre><table><tr><th style="text-align: left">Parameter</th><th style="text-align: center">Truth</th><th style="text-align: center">EKI mean</th></tr><tr><td style="text-align: left">Amplitude</td><td style="text-align: center">3.0</td><td style="text-align: center">3.05</td></tr><tr><td style="text-align: left">Vertical shift</td><td style="text-align: center">7.0</td><td style="text-align: center">6.37</td></tr></table><p>The EKI ensemble mean at the final iteration is close to the true parameters, which is good. We can also see how the ensembles evolve at each iteration in the plot below.</p><p><img src="../../assets/sinusoid_eki_pairs.png" alt="eki"/></p><p>The ensembles are initially spread out but move closer to the true parameter values with each iteration, indicating the EKI algorithm is converging towards the minimum. Taking the mean of the ensemble gives a point estimate of the optimal parameters. However, EKI does not give us an estimate of the uncertainty, as the ensemble collapses. To carry out uncertainty quantification, we can sample from the posterior distribution, which requires a &quot;cheap&quot; method to evaluate our model, i.e., an emulator.  In the next step of CES, we will build an emulator using the dataset generated in EKI.</p><h2 id="Emulate"><a class="docs-heading-anchor" href="#Emulate">Emulate</a><a id="Emulate-1"></a><a class="docs-heading-anchor-permalink" href="#Emulate" title="Permalink"></a></h2><p>In the previous calibrate step, we learned point estimates for the optimal parameters <span>$\theta$</span>, but  for uncertainty quantification, we want to learn posterior distributions on our parameters. We can sample from posterior distributions with Markov chain Monte Carlo (MCMC) methods, but these  typically require many model evaluations. In many scientific problems, model evaluations are highly  costly, making this infeasible. To get around this, we build an emulator of our model,  which allows us to approximate the expensive model almost instantaneously. An emulator can also be  helpful for noisy problems as they provide a smoother approximation, leading to better MCMC  convergence properties.  In this section, we show how the codebase can be used to build emulators of our sinusoid model.</p><p>We ran Ensemble Kalman Inversion with an ensemble size of 10 for 5  iterations. This generated a total of 50 input output pairs from our model. We will use these samples to train an emulator. The EKI samples make a suitable  dataset for training an emulator because in the first iteration, the ensemble parameters  are spread out according to the prior, meaning they cover the full support of the parameter space. This is important for building an emulator that can be evaluated anywhere  in this space. In later iterations, the ensemble parameters are focused around the truth.  This means the emulator that will be more accurate around this region.</p><p>First, we load additional packages we need for this section:</p><pre><code class="language-julia hljs">using CalibrateEmulateSample.Emulators
const CES = CalibrateEmulateSample</code></pre><p>We will build two types of emulator here for comparison: Gaussian processes and Random  Features. First, set up the data in the correct format. CalibrateEmulateSample.jl uses a paired data container that matches the inputs (in the unconstrained space) to the outputs:</p><pre><code class="language-julia hljs">input_output_pairs = CES.Utilities.get_training_points(ensemble_kalman_process, N_iterations)
unconstrained_inputs = CES.Utilities.get_inputs(input_output_pairs)
inputs = Emulators.transform_unconstrained_to_constrained(prior, unconstrained_inputs)
outputs = CES.Utilities.get_outputs(input_output_pairs)</code></pre><h3 id="Gaussian-process"><a class="docs-heading-anchor" href="#Gaussian-process">Gaussian process</a><a id="Gaussian-process-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-process" title="Permalink"></a></h3><p>We will set up a basic Gaussian process (GP) emulator using the <a href="https://scikitlearnjl.readthedocs.io/en/latest/models/#scikitlearn-models"><code>ScikitLearn.jl</code></a> package or <a href="https://stor-i.github.io/GaussianProcesses.jl/latest/"><code>GaussianProcesses.jl</code></a>.  See the <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/GaussianProcessEmulator/">Gaussian process page</a> for more information and options, including choice of package and kernels.</p><pre><code class="language-julia hljs">gppackage = Emulators.GPJL()
gauss_proc = Emulators.GaussianProcess(gppackage, noise_learn = false)

# Build emulator with data
emulator_gp = Emulator(gauss_proc, input_output_pairs, normalize_inputs = true,  obs_noise_cov = Γ)
optimize_hyperparameters!(emulator_gp)</code></pre><p>For this simple example, we already know the observational noise <code>Γ=0.2*I</code>, so we set <code>noise_learn = false</code>.  However, for more complicated problems we may want to learn the noise as an additional hyperparameter.</p><p>We will check performance of the GP by testing on unseen data in a moment, but first, we will build a random features emulator for comparison.</p><h3 id="Random-Features"><a class="docs-heading-anchor" href="#Random-Features">Random Features</a><a id="Random-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Features" title="Permalink"></a></h3><p>An alternative emulator can be created with random features (RF). Random features can approximate a Gaussian process with improved scaling properties, making them more suitable for higher dimensional problems. We use a Vector Random Features emulator here, chosen because we find it is a reasonable approximation to the Gaussian process emulator above. For new problems, you may need to play around with these parameter choices. More information can be found <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/random_feature_emulator/">here</a>.</p><pre><code class="language-julia hljs"># We have two input dimensions and two output dimensions.
input_dim = 2
output_dim = 2
# Select number of features
n_features = 60
nugget = 1e-9
kernel_structure = NonseparableKernel(LowRankFactor(2, nugget))
optimizer_options = Dict(
    &quot;n_ensemble&quot; =&gt; 50,
    &quot;cov_sample_multiplier&quot; =&gt; 10,
    &quot;scheduler&quot; =&gt; EKP.DataMisfitController(on_terminate = &quot;continue&quot;),
    &quot;n_iteration&quot; =&gt; 50,
    &quot;verbose&quot; =&gt; true,
)
random_features = VectorRandomFeatureInterface(
    n_features,
    input_dim,
    output_dim,
    kernel_structure = kernel_structure,
    optimizer_options = optimizer_options,
)
emulator_random_features =
    Emulator(random_features, input_output_pairs, normalize_inputs = true, obs_noise_cov = Γ, decorrelate = false)
optimize_hyperparameters!(emulator_random_features)</code></pre><h3 id="Emulator-Validation"><a class="docs-heading-anchor" href="#Emulator-Validation">Emulator Validation</a><a id="Emulator-Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Emulator-Validation" title="Permalink"></a></h3><p>Now we will validate both GP and RF emulators and compare them against the ground truth, <span>$G(\theta)$</span>. Note this is only possible in our example because our true model, <span>$G(\theta)$</span>, is cheap to evaluate. In more complex systems, we would have limited data to validate emulator performance with. </p><p>Here, we will compare emulator performance across a wide range of parameters, but we will pay close attention to  performance near the final ensemble mean <span>$\theta=(3, 6)$</span>. This is because we need high accuracy in this region in the next step, when we sample the posterior distribution.</p><p>First, we check the ground truth model <span>$G(\theta)$</span> over our parameter space. We can plot how the two outputs (range, mean), vary with the two input parameters (amplitude, vertical shift).</p><p><img src="../../assets/sinusoid_groundtruth_contours.png" alt="groundtruth"/></p><p>The first panel shows how the range varies with respect to the two parameters in the true forward map. The contours show the range is mostly dependent on the amplitude, with little variation with respect to the vertical shift. The second panel shows how the mean varies with the respect to the two parameters and is mostly dependent on the vertical shift. This result makes sense for our model setup.</p><p>Below, we recreate the same contour plot with the emulators. We will also overlay the training data points  from the EKI, where the colors show the output from <span>$G(\theta)$</span> evaluated at the training points.  The emulator contours should agree with the training data.</p><p>First, for the Gaussian process emulator:</p><p><img src="../../assets/sinusoid_GP_emulator_contours.png" alt="GP_emulator"/></p><p>This looks similar to the output from <span>$G(\theta)$</span>. Next, let&#39;s check the random features emulator:</p><p><img src="../../assets/sinusoid_RF_emulator_contours.png" alt="RF_emulator"/></p><p>Both the GP and RF emulator give similar results to the ground truth <span>$G(\theta)$</span>, indicating they are correctly learning the relationships between the parameters and the outputs. We also see the contours agree with the  colors of the training data points. </p><p>We should also validate how accurate the emulators are by looking at the absolute difference between emulator predictions and the ground truth. </p><p>The Gaussian process absolute errors are plotted here:</p><p><img src="../../assets/sinusoid_GP_errors_contours.png" alt="GP_errors"/></p><p>and the random features absolute errors are here:</p><p><img src="../../assets/sinusoid_RF_errors_contours.png" alt="RF_errors"/></p><p>Both these error maps look similar. Importantly, we want the emulator to show the low errors in the region around the true parameter values near <span>$\theta=(3, 6)$</span> (i.e, near where the EKI points converge, shown by the scatter points in the previous plot). This the region that we will be sampling in the next step.  We see low errors near here for both outputs and for both emulators. Now we have validated these emulators,  we will proceed the last step of CES: Sampling of the posterior distribution. </p><h2 id="Sample"><a class="docs-heading-anchor" href="#Sample">Sample</a><a id="Sample-1"></a><a class="docs-heading-anchor-permalink" href="#Sample" title="Permalink"></a></h2><p>Now that we have a cheap emulator for our model, we can carry out uncertainty quantification to learn the posterier distribution of the parameters, <span>$\theta$</span>. We use Markov chain Monte Carlo (MCMC) to sample the posterior distribtion. In MCMC, we start with a sample from a prior distribution and propose a new sample from a proposal distribution, which is accepted with a probability relating the the ratio of the posterior distribution to the proposal distributions. If accepted, this proposed sample is  added to the chain, or otherwise, the original sample is added to the chain. This is repeated over many iterations and eventually creates a sequence of samples from the posterior distribution. The CES code uses <a href="https://turing.ml/dev/docs/for-developers/interface">AbstractMCMC.jl</a>, full details can be found <a href="https://clima.github.io/CalibrateEmulateSample.jl/dev/API/AbstractMCMC/">here</a>. For this example, we will use a random walk Metropolis-Hastings sampler (<code>RWMHSampling</code>), which assumes that  the proposal distribution is a random walk, with a step-size <span>$\delta$</span>. Usually, we have little knowledge of  what this step size should be, but we can optimize this as shown below.</p><p>First, we will load the additional packages we need:</p><pre><code class="language-julia hljs">using CalibrateEmulateSample.MarkovChainMonteCarlo</code></pre><p>We will provide the API with the observations, priors and our cheap emulator from the previous section. In this  example we use the GP emulator. First, we need to find a suitable starting point, ideally one that is near the posterior distribution. We will use the final ensemble mean from EKI as this will increase the chance of acceptance near the start of the chain, and reduce burn-in time.</p><pre><code class="language-julia hljs">init_sample = EKP.get_u_mean_final(ensemble_kalman_process)
println(&quot;initial parameters: &quot;, init_sample)    # (1.11, 6.37)</code></pre><p>Now, we can set up and carry out the MCMC starting from this point. </p><pre><code class="language-julia hljs">mcmc = MCMCWrapper(RWMHSampling(), y_obs, prior, emulator_gp; init_params = init_sample)
# First let&#39;s run a short chain to determine a good step size
new_step = optimize_stepsize(mcmc; init_stepsize = 0.1, N = 2000, discard_initial = 0)

# Now begin the actual MCMC
println(&quot;Begin MCMC - with step size &quot;, new_step)
chain = MarkovChainMonteCarlo.sample(mcmc, 100_000; stepsize = new_step, discard_initial = 2_000)

# We can print summary statistics of the MCMC chain
display(chain)</code></pre><table><tr><th style="text-align: left">parameters</th><th style="text-align: center">mean</th><th style="text-align: center">std</th></tr><tr><td style="text-align: left">amplitude</td><td style="text-align: center">1.1068</td><td style="text-align: center">0.0943</td></tr><tr><td style="text-align: left">vert_shift</td><td style="text-align: center">6.3897</td><td style="text-align: center">0.4601</td></tr></table><p>Note that these values are provided in the unconstrained space. The vertical shift seems reasonable, but the amplitude is not. This is because the amplitude is constrained to be positive, but the MCMC is run in the unconstrained space.  We can transform to the real  constrained space and re-calculate these values.</p><pre><code class="language-julia hljs"># Extract posterior samples
posterior = MarkovChainMonteCarlo.get_posterior(mcmc, chain)
# Back to constrained coordinates
constrained_posterior = Emulators.transform_unconstrained_to_constrained(
    prior, MarkovChainMonteCarlo.get_distribution(posterior)
)
println(&quot;Amplitude mean: &quot;, mean(constrained_posterior[&quot;amplitude&quot;]), &quot;, std: &quot;, std(constrained_posterior[&quot;amplitude&quot;]))
println(&quot;Vertical shift mean: &quot;, mean(constrained_posterior[&quot;vert_shift&quot;]), &quot;, std: &quot;, std(constrained_posterior[&quot;vert_shift&quot;]))</code></pre><p>This gives:</p><table><tr><th style="text-align: left">parameters</th><th style="text-align: center">mean</th><th style="text-align: center">std</th></tr><tr><td style="text-align: left">amplitude</td><td style="text-align: center">3.0382</td><td style="text-align: center">0.2880</td></tr><tr><td style="text-align: left">vert_shift</td><td style="text-align: center">6.3774</td><td style="text-align: center">0.4586</td></tr></table><p>This is in agreement with the true <span>$\theta=(3.0, 7.0)$</span> and with the observational covariance matrix we provided <span>$\Gamma=0.2 * I$</span> (i.e., a standard deviation of approx. <span>$0.45$</span>). <code>CalibrateEmulateSample.jl</code> has built-in plotting recipes to help us visualize the prior and posterior distributions.  Note that these are the marginal distributions.</p><pre><code class="language-julia hljs"># We can quickly plot priors and posterior using built-in capabilities
p = plot(prior, fill = :lightgray)
plot!(posterior, fill = :darkblue, alpha = 0.5)
</code></pre><p><img src="../../assets/sinusoid_posterior_GP.png" alt="GP_posterior"/></p><p>The MCMC has learned the posterior distribution which is much narrower than the prior.  For multidimensional problems, the posterior is typically multidimensional, and marginal  distribution plots do not show how parameters co-vary. We plot a 2D histogram of <span>$\theta_1$</span> vs. <span>$\theta_2$</span> below, with the marginal distributions on each axis. </p><p><img src="../../assets/sinusoid_MCMC_hist_GP.png" alt="GP_2d_posterior"/></p><h3 id="Sample-with-Random-Features"><a class="docs-heading-anchor" href="#Sample-with-Random-Features">Sample with Random Features</a><a id="Sample-with-Random-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-with-Random-Features" title="Permalink"></a></h3><p>We can repeat the sampling method using the random features emulator instead of the Gaussian process and we find similar results: </p><table><tr><th style="text-align: left">parameters</th><th style="text-align: center">mean</th><th style="text-align: center">std</th></tr><tr><td style="text-align: left">amplitude</td><td style="text-align: center">3.3210</td><td style="text-align: center">0.7216</td></tr><tr><td style="text-align: left">vert_shift</td><td style="text-align: center">6.3986</td><td style="text-align: center">0.5098</td></tr></table><p><img src="../../assets/sinusoid_MCMC_hist_RF.png" alt="RF_2d_posterior"/></p><p>It is reassuring to see that our uncertainty quantification methods are robust to the different emulator choices here. This is because our particular GP and RF emulators showed similar accuracy during validation.  However, this result is highly sensitive to the choices of GP kernel and RF kernel structure. If you find very  different posterior distributions for different emulators, it is likely that the kernel choices need be refined. The kernel choices must be flexible enough to accurately capture the relationships between the inputs and outputs.  We recommend trying a variety of different emulator configurations and carefully considering emulator validation  on samples that the emulator has not been trained on. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../contributing/">« Contributing</a><a class="docs-footer-nextpage" href="../lorenz_example/">Lorenz example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Tuesday 8 April 2025 21:05">Tuesday 8 April 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
